<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Example I: Multiple linear regression · ProfileLikelihood.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link rel="canonical" href="https://DanielVandH.github.io/ProfileLikelihood.jl/regression/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">ProfileLikelihood.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../interface/">Interface</a></li><li><a class="tocitem" href="../docstrings/">Docstrings</a></li><li class="is-active"><a class="tocitem" href>Example I: Multiple linear regression</a><ul class="internal"><li><a class="tocitem" href="#Setting-up-the-problem"><span>Setting up the problem</span></a></li><li><a class="tocitem" href="#Profiling"><span>Profiling</span></a></li><li><a class="tocitem" href="#Visualisation"><span>Visualisation</span></a></li><li><a class="tocitem" href="#Just-the-code"><span>Just the code</span></a></li></ul></li><li><a class="tocitem" href="../logistic/">Example II: Logistic ordinary differential equation</a></li><li><a class="tocitem" href="../exponential/">Example III: Linear exponential ODE and grid searching</a></li><li><a class="tocitem" href="../heat/">Example IV: Diffusion equation on a square plate</a></li><li><a class="tocitem" href="../lotka/">Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods</a></li><li><a class="tocitem" href="../math/">Mathematical and Implementation Details</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Example I: Multiple linear regression</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Example I: Multiple linear regression</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/DanielVandH/ProfileLikelihood.jl/blob/main/docs/src/regression.md#" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Example-I:-Multiple-linear-regression"><a class="docs-heading-anchor" href="#Example-I:-Multiple-linear-regression">Example I: Multiple linear regression</a><a id="Example-I:-Multiple-linear-regression-1"></a><a class="docs-heading-anchor-permalink" href="#Example-I:-Multiple-linear-regression" title="Permalink"></a></h1><p>Let us start with a linear regression example. First, load the packages needed:</p><pre><code class="language-julia hljs">using ProfileLikelihood
using Random 
using PreallocationTools 
using Distributions 
using CairoMakie 
using LaTeXStrings 
using LinearAlgebra
using Optimization 
using OptimizationOptimJL</code></pre><p>We perform a simulation study where we try and estimate the parameters in a regression of the form </p><p class="math-container">\[y_i = \beta_0 + \beta_1x_{1i} + \beta_2x_{2i} + \beta_3x_{1i}x_{3i} + \varepsilon_i, \quad \varepsilon_i \sim \mathcal N(0, \sigma^2), \quad i=1,2,\ldots, n. \]</p><p>We also try and estimate <span>$\sigma$</span>. </p><h2 id="Setting-up-the-problem"><a class="docs-heading-anchor" href="#Setting-up-the-problem">Setting up the problem</a><a id="Setting-up-the-problem-1"></a><a class="docs-heading-anchor-permalink" href="#Setting-up-the-problem" title="Permalink"></a></h2><p>Let us start by simulating the data:</p><pre><code class="language-julia hljs">using Random, Distributions 
Random.seed!(98871) 
n = 600
β = [-1.0, 1.0, 0.5, 3.0]
σ = 0.05
x₁ = rand(Uniform(-1, 1), n)
x₂ = rand(Normal(1.0, 0.5), n)
X = hcat(ones(n), x₁, x₂, x₁ .* x₂)
ε = rand(Normal(0.0, σ), n)
y = X * β + ε</code></pre><p>The data <code>y</code> is now our noisy data. The likelihood function in this example is </p><p class="math-container">\[\ell(\sigma, \boldsymbol \beta \mid \boldsymbol y) = -(n/2)\log(2\mathrm{\pi}\sigma^2) - (1/2\sigma^2)\sum_i (y_i - \beta_0 - \beta_1x_{1i} - \beta_2x_{2i} - \beta_3x_{1i}x_{2i})^2. \]</p><p>We now define our likelihood function. To allow for automatic differentiation, we use <code>PreallocationTools.DiffCache</code> to define our cache vectors.</p><pre><code class="language-julia hljs">sse = DiffCache(zeros(n))
β_cache = DiffCache(similar(β), 10)
dat = (y, X, sse, n, β_cache)
@inline function loglik_fnc(θ, data)
    σ, β₀, β₁, β₂, β₃ = θ
    y, X, sse, n, β = data
    _sse = get_tmp(sse, θ)
    _β = get_tmp(β, θ)
    _β[1] = β₀
    _β[2] = β₁
    _β[3] = β₂
    _β[4] = β₃
    ℓℓ = -0.5n * log(2π * σ^2)
    mul!(_sse, X, _β)
    for i in eachindex(y)
        ℓℓ = ℓℓ - 0.5 / σ^2 * (y[i] - _sse[i])^2
    end
    return ℓℓ
end</code></pre><p>Now having defined our likelihood, we can define the likelihood problem. We let the problem be unconstrained, except for <span>$\sigma &gt; 0$</span>. We start at the value <span>$1$</span> for each parameter. To use automatic differentiation, we use <code>Optimization.AutoForwardDiff</code> for the <code>adtype</code>.</p><pre><code class="language-julia hljs">using Optimization
θ₀ = ones(5)
prob = LikelihoodProblem(loglik_fnc, θ₀;
    data=dat,
    f_kwargs=(adtype=Optimization.AutoForwardDiff(),),
    prob_kwargs=(
        lb=[0.0, -Inf, -Inf, -Inf, -Inf],
        ub=Inf * ones(5)
    ),
    syms=[:σ, :β₀, :β₁, :β₂, :β₃]
)
LikelihoodProblem. In-place: true
θ₀: 5-element Vector{Float64}
     σ: 1.0
     β₀: 1.0
     β₁: 1.0
     β₂: 1.0
     β₃: 1.0</code></pre><h3 id="Finding-the-MLEs"><a class="docs-heading-anchor" href="#Finding-the-MLEs">Finding the MLEs</a><a id="Finding-the-MLEs-1"></a><a class="docs-heading-anchor-permalink" href="#Finding-the-MLEs" title="Permalink"></a></h3><p>Now we can compute the MLEs.</p><pre><code class="language-julia hljs">using OptimizationOptimJL
sol = mle(prob, Optim.LBFGS())
LikelihoodSolution. retcode: Success
Maximum likelihood: 957.6376683220673
Maximum likelihood estimates: 5-element Vector{Float64}
     σ: 0.049045771053511954
     β₀: -1.0041730424101303
     β₁: 1.006051999753723
     β₂: 0.5041343138021581
     β₃: 2.9922041467801934</code></pre><p>We can compare these MLEs to the true MLES <span>$\hat{\beta} = (\boldsymbol X^{\mathsf T}\boldsymbol X)^{-1}\boldsymbol X^{\mathsf T}\boldsymbol y$</span> and <span>$\hat\sigma^2 = (1/n_d)(\boldsymbol y - \boldsymbol X\boldsymbol \beta)^{\mathsf T}(\boldsymbol y - \boldsymbol X\boldsymbol \beta)$</span>, where <span>$n_d$</span> is the degrees of freedom, as follows (note the indexing):</p><pre><code class="language-julia hljs">using Test, LinearAlgebra
df = n - (length(β) + 1)
resids = y .- X * sol[2:5]
@test sol[2:5] ≈ inv(X&#39; * X) * X&#39; * y # sol[i] = sol.mle[i] 
@test sol[:σ]^2 ≈ 1 / df * sum(resids .^ 2) atol = 1e-4 # symbol indexing</code></pre><h2 id="Profiling"><a class="docs-heading-anchor" href="#Profiling">Profiling</a><a id="Profiling-1"></a><a class="docs-heading-anchor-permalink" href="#Profiling" title="Permalink"></a></h2><p>We can now profile the results. In this case, since the problem has no bounds for some parameters we need to manually define the parameter bounds used for profiling. The function <code>construct_profile_ranges</code> is used for this. Note that we use <code>parallel = true</code> below to allow for multithreading, allowing multiple parameters to be profiled at the same time.</p><pre><code class="language-julia hljs">lb = [1e-12, -5.0, -5.0, -5.0, -5.0]
ub = [15.0, 15.0, 15.0, 15.0, 15.0]
resolutions = [600, 200, 200, 200, 200] # use many points for σ
param_ranges = construct_profile_ranges(sol, lb, ub, resolutions)
prof = profile(prob, sol; param_ranges, parallel=true)
ProfileLikelihoodSolution. MLE retcode: Success
Confidence intervals: 
     95.0% CI for σ: (0.04639652142575396, 0.05196200098682017)
     95.0% CI for β₀: (-1.013328678265197, -0.9950163004240635)
     95.0% CI for β₁: (0.9906172772152076, 1.0214865014037124)
     95.0% CI for β₂: (0.4960199617761395, 0.5122490969333844)
     95.0% CI for β₃: (2.978618197988093, 3.0057902255444136)</code></pre><p>These confidence intervals can be compared to the true confidence intervals as follows, noting that the variance-covariance matrix for the <span>$\beta_i$</span> coefficients is <span>$\boldsymbol\Sigma = \sigma^2(\boldsymbol X^{\mathsf T}\boldsymbol X)^{-1}$</span> so that their confidence interval is <span>$\hat\beta_i \pm 1.96\sqrt{\boldsymbol\Sigma_{ii}}$</span>. Additionally, a confidence interval for <span>$\sigma$</span> is <span>$\sqrt{(\boldsymbol y - \boldsymbol X\boldsymbol \beta)^{\mathsf T}(\boldsymbol y - \boldsymbol X\boldsymbol \beta)}(1/\sqrt{\chi_{0.975, n_d}}, 1/\sqrt{\chi_{0.025, n_d}})$</span>.</p><pre><code class="language-julia hljs">vcov_mat = sol[:σ]^2 * inv(X&#39; * X)
for i in 1:4
    @test prof.confidence_intervals[i+1][1] ≈ sol.mle[i+1] - 1.96sqrt(vcov_mat[i, i]) atol = 1e-3
    @test prof.confidence_intervals[i+1][2] ≈ sol.mle[i+1] + 1.96sqrt(vcov_mat[i, i]) atol = 1e-3
end
rss = sum(resids .^ 2)
χ²_up = quantile(Chisq(df), 0.975)
χ²_lo = quantile(Chisq(df), 0.025)
σ_CI_exact = sqrt.(rss ./ (χ²_up, χ²_lo))
@test get_confidence_intervals(prof, :σ).lower ≈ σ_CI_exact[1] atol = 1e-3
@test ProfileLikelihood.get_upper(get_confidence_intervals(prof, :σ)) ≈ σ_CI_exact[2] atol = 1e-3</code></pre><p>You can use <code>prof</code> to view a single parameter&#39;s results, e.g.</p><pre><code class="language-julia hljs">prof[:β₂]
Profile likelihood for parameter β₂. MLE retcode: Success
MLE: 0.5041343138021581
95.0% CI for β₂: (0.4960199617761438, 0.5122490969333844)</code></pre><p>You can also evaluate the profile at a point inside its confidence interval. (If you want to evaluate outside the confidence interval, you need to use a non-<code>Throw</code> <code>extrap</code> in the <code>profile</code> function&#39;s keyword argument [see also Interpolations.jl].) The following are all the same, evaluating the profile for <span>$\beta_2$</span> at <span>$\beta_2=0.5$</span>:</p><pre><code class="language-julia hljs">prof[:β₂](0.50)
prof(0.50, :β₂)
prof(0.50, 4)</code></pre><h2 id="Visualisation"><a class="docs-heading-anchor" href="#Visualisation">Visualisation</a><a id="Visualisation-1"></a><a class="docs-heading-anchor-permalink" href="#Visualisation" title="Permalink"></a></h2><p>We can now also visualise the results. In the plot below, the red line is at the threshold for the confidence region, so that the parameters between these values define the confidence interval. The red lines are at the MLEs, and the black lines are at the true values. </p><pre><code class="language-julia hljs">using CairoMakie, LaTeXStrings
fig = plot_profiles(prof;
    latex_names=[L&quot;\sigma&quot;, L&quot;\beta_0&quot;, L&quot;\beta_1&quot;, L&quot;\beta_2&quot;, L&quot;\beta_3&quot;], # default names would be of the form θᵢ
    show_mles=true,
    shade_ci=true,
    true_vals=[σ, β...],
    fig_kwargs=(fontsize=30, resolution=(2134.0f0, 906.0f0)),
    axis_kwargs=(width=600, height=300))
xlims!(fig.content[1], 0.045, 0.055) # fix the ranges
xlims!(fig.content[2], -1.025, -0.975)
xlims!(fig.content[4], 0.475, 0.525)</code></pre><p><img src="https://github.com/DanielVandH/ProfileLikelihood.jl/blob/main/test/figures/regression_profiles.png?raw=true" alt="Regression profiles"/></p><p>You could also plot individual or specific parameters:</p><pre><code class="language-julia hljs">plot_profiles(prof, [1, 3]) # plot σ and β₁
plot_profiles(prof, [:σ, :β₁, :β₃]) # can use symbols 
plot_profiles(prof, 1) # can just provide an integer 
plot_profiles(prof, :β₂) # symbols work</code></pre><h2 id="Just-the-code"><a class="docs-heading-anchor" href="#Just-the-code">Just the code</a><a id="Just-the-code-1"></a><a class="docs-heading-anchor-permalink" href="#Just-the-code" title="Permalink"></a></h2><p>Here is all the code used for obtaining the results in this example, should you want a version that you can directly copy and paste.</p><pre><code class="language-julia hljs">using Random, Distributions, PreallocationTools, LinearAlgebra
## Step 1: Generate some data for the problem and define the likelihood
Random.seed!(98871)
n = 600
β = [-1.0, 1.0, 0.5, 3.0]
σ = 0.05
x₁ = rand(Uniform(-1, 1), n)
x₂ = rand(Normal(1.0, 0.5), n)
X = hcat(ones(n), x₁, x₂, x₁ .* x₂)
ε = rand(Normal(0.0, σ), n)
y = X * β + ε
sse = DiffCache(zeros(n))
β_cache = DiffCache(similar(β), 10)
dat = (y, X, sse, n, β_cache)
@inline function loglik_fnc(θ, data)
    σ, β₀, β₁, β₂, β₃ = θ
    y, X, sse, n, β = data
    _sse = get_tmp(sse, θ)
    _β = get_tmp(β, θ)
    _β[1] = β₀
    _β[2] = β₁
    _β[3] = β₂
    _β[4] = β₃
    ℓℓ = -0.5n * log(2π * σ^2)
    mul!(_sse, X, _β)
    for i in eachindex(y)
        ℓℓ = ℓℓ - 0.5 / σ^2 * (y[i] - _sse[i])^2
    end
    return ℓℓ
end

## Step 2: Define the problem 
using Optimization
θ₀ = ones(5)
prob = LikelihoodProblem(loglik_fnc, θ₀;
    data=dat,
    f_kwargs=(adtype=Optimization.AutoForwardDiff(),),
    prob_kwargs=(
        lb=[0.0, -Inf, -Inf, -Inf, -Inf],
        ub=Inf * ones(5)
    ),
    syms=[:σ, :β₀, :β₁, :β₂, :β₃]
)

## Step 3: Compute the MLE
using OptimizationOptimJL
sol = mle(prob, Optim.LBFGS())

## Step 4: Profile 
lb = [1e-12, -5.0, -5.0, -5.0, -5.0]
ub = [15.0, 15.0, 15.0, 15.0, 15.0]
resolutions = [600, 200, 200, 200, 200] # use many points for σ
param_ranges = construct_profile_ranges(sol, lb, ub, resolutions)
prof = profile(prob, sol; param_ranges, parallel=true)

## Step 5: Visualise 
using CairoMakie, LaTeXStrings
fig = plot_profiles(prof;
    latex_names=[L&quot;\sigma&quot;, L&quot;\beta_0&quot;, L&quot;\beta_1&quot;, L&quot;\beta_2&quot;, L&quot;\beta_3&quot;], # default names would be of the form θᵢ
    show_mles=true,
    shade_ci=true,
    true_vals=[σ, β...],
    fig_kwargs=(fontsize=30, resolution=(2134.0f0, 906.0f0)),
    axis_kwargs=(width=600, height=300))
xlims!(fig.content[1], 0.045, 0.055) # fix the ranges
xlims!(fig.content[2], -1.025, -0.975)
xlims!(fig.content[4], 0.475, 0.525)</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../docstrings/">« Docstrings</a><a class="docs-footer-nextpage" href="../logistic/">Example II: Logistic ordinary differential equation »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.24 on <span class="colophon-date" title="Tuesday 28 February 2023 21:24">Tuesday 28 February 2023</span>. Using Julia version 1.8.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
