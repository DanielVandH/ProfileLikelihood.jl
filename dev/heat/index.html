<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Example IV: Diffusion equation on a square plate · ProfileLikelihood.jl</title><meta name="title" content="Example IV: Diffusion equation on a square plate · ProfileLikelihood.jl"/><meta property="og:title" content="Example IV: Diffusion equation on a square plate · ProfileLikelihood.jl"/><meta property="twitter:title" content="Example IV: Diffusion equation on a square plate · ProfileLikelihood.jl"/><meta name="description" content="Documentation for ProfileLikelihood.jl."/><meta property="og:description" content="Documentation for ProfileLikelihood.jl."/><meta property="twitter:description" content="Documentation for ProfileLikelihood.jl."/><meta property="og:url" content="https://DanielVandH.github.io/ProfileLikelihood.jl/heat/"/><meta property="twitter:url" content="https://DanielVandH.github.io/ProfileLikelihood.jl/heat/"/><link rel="canonical" href="https://DanielVandH.github.io/ProfileLikelihood.jl/heat/"/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">ProfileLikelihood.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../interface/">Interface</a></li><li><a class="tocitem" href="../docstrings/">Docstrings</a></li><li><a class="tocitem" href="../regression/">Example I: Multiple linear regression</a></li><li><a class="tocitem" href="../logistic/">Example II: Logistic ordinary differential equation</a></li><li><a class="tocitem" href="../exponential/">Example III: Linear exponential ODE and grid searching</a></li><li><a class="tocitem" href="../lotka/">Example IV: Lotka-Volterra ODE and computing bivarate profile likelihoods</a></li><li><a class="tocitem" href="../stefan/">Example V: Fisher-Stefan PDE</a></li><li><a class="tocitem" href="../math/">Mathematical and Implementation Details</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Example IV: Diffusion equation on a square plate</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Example IV: Diffusion equation on a square plate</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/DanielVandH/ProfileLikelihood.jl/blob/main/docs/src/heat.md#" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Example-IV:-Diffusion-equation-on-a-square-plate"><a class="docs-heading-anchor" href="#Example-IV:-Diffusion-equation-on-a-square-plate">Example IV: Diffusion equation on a square plate</a><a id="Example-IV:-Diffusion-equation-on-a-square-plate-1"></a><a class="docs-heading-anchor-permalink" href="#Example-IV:-Diffusion-equation-on-a-square-plate" title="Permalink"></a></h1><p><em>Warning</em>: Much of the code in this example takes a very long time, e.g. the MLEs take just under an hour. The total runtime is around six hours on my machine (mostly coming from the mesh for the PDE being very dense). </p><p>The packages we use in this example are:</p><pre><code class="language-julia hljs">using FiniteVolumeMethod 
using ProfileLikelihood 
using DelaunayTriangulation
using Random 
using LinearSolve 
using OrdinaryDiffEq
using CairoMakie 
using StaticArraysCore
using Optimization 
using OptimizationNLopt</code></pre><p>Let us now consider the problem of estimating parameters defining a diffusion equation on a square plate. In particular, consider </p><p class="math-container">\[\begin{equation*}
\begin{array}{rcll}
\displaystyle
\frac{\partial u(x, y, t)}{\partial t} &amp;=&amp; \dfrac{1}{k}\boldsymbol{\nabla}^2 u(x, y, t) &amp; (x, y) \in \Omega,t&gt;0, \\
u(x, y, t) &amp;= &amp; 0 &amp; (x, y) \in \partial \Omega,t&gt;0, \\
u(x, y, 0) &amp;= &amp; u_0\mathbb{I}(y \leq c) &amp;(x,y)\in\Omega,
\end{array}
\end{equation*}\]</p><p>where <span>$\Omega = [0, 2]^2$</span>. This problem extends the corresponding example given in FiniteVolumeMethod.jl, namely <a href="https://github.com/DanielVandH/FiniteVolumeMethod.jl#diffusion-equation-on-a-square-plate">this example</a>, and so not all the code used in defining this PDE will be explained here; refer to the FiniteVolumeMethod.jl documentation. We will take the true values <span>$k = 9$</span>, <span>$c = 1$</span>, <span>$u_0 = 50$</span>, and let the standard deviation of the noise, <span>$\sigma$</span>, in the data be <span>$0.1$</span>. We are interested in recovering <span>$(k, c, u_0)$</span>; we do not consider estimating <span>$\sigma$</span> here, estimating it leads to identifiability issues that distract from the main point of our example here, i.e. to just show how to setup a problem.</p><h2 id="Building-the-FVMProblem"><a class="docs-heading-anchor" href="#Building-the-FVMProblem">Building the FVMProblem</a><a id="Building-the-FVMProblem-1"></a><a class="docs-heading-anchor-permalink" href="#Building-the-FVMProblem" title="Permalink"></a></h2><p>Let us start by defining the PDE problem, and then we will discuss profiling.</p><pre><code class="language-julia hljs">using FiniteVolumeMethod, DelaunayTriangulation, LinearSolve
a, b, c, d = 0.0, 2.0, 0.0, 2.0
r = 0.022
GMSH_PATH = &quot;./gmsh-4.9.4-Windows64/gmsh.exe&quot; # set this to whatever your path is
tri = generate_mesh(a, b, c, d, r; gmsh_path=GMSH_PATH)
mesh = FVMGeometry(tri)
bc = ((x, y, t, u::T, p) where {T}) -&gt; zero(T)
type = :D
BCs = BoundaryConditions(mesh, bc, type)
c = 1.0
u₀ = 50.0
f = (x, y) -&gt; y ≤ c ? u₀ : 0.0
D = (x, y, t, u, p) -&gt; p[1]
flux = (q, x, y, t, α, β, γ, p) -&gt; (q[1] = -α / p[1]; q[2] = -β / p[1])
R = ((x, y, t, u::T, p) where {T}) -&gt; zero(T)
points = get_points(tri)
initc = @views f.(points[1, :], points[2, :])
iip_flux = true
final_time = 0.1
k = [9.0]
prob = FVMProblem(mesh, BCs; iip_flux,
    flux_function=flux, reaction_function=R,
    initial_condition=initc, final_time,
    flux_parameters=k)</code></pre><p>Our problem has now been defined. Notice that we wrap <code>k</code> in a vector so that we can easily mutate the <code>flux_parameters</code> field of <code>prob</code>; if <code>k</code> were a scalar, we could not mutate it.</p><p>Now let&#39;s generate some data. We start by solving the PDE.</p><pre><code class="language-julia hljs">alg = TRBDF2(linsolve=KLUFactorization(; reuse_symbolic=false))
sol = solve(prob, alg; specialization=SciMLBase.FullSpecialize, saveat=0.01)</code></pre><p>(We use <code>reuse_symbolic=false</code> due to https://github.com/JuliaSparse/KLU.jl/issues/12 causing issues with multithreading later.) </p><h2 id="Defining-a-summary-statistic"><a class="docs-heading-anchor" href="#Defining-a-summary-statistic">Defining a summary statistic</a><a id="Defining-a-summary-statistic-1"></a><a class="docs-heading-anchor-permalink" href="#Defining-a-summary-statistic" title="Permalink"></a></h2><p>Now, one complication with a PDE compared to the scalar ODE cases that we considered previously is that we have data at <span>$(x_i, y_j, t_k)$</span> for many indices <span>$(i, j, k)$</span>. Rather than defining our objective function in terms of these data points, we will instead use a summary statistic. The summary statistic we use in this example is the average density,</p><p class="math-container">\[\tilde M(t) = \frac{1}{\mathrm{Area}(\Omega)}\iint_\Omega u(x, y, t)\mathrm{dA}. \]</p><p>We need to be able to compute this integral efficiently and accurately. For this, recall that the finite volume method discretises the domain into triangles. If <span>$\mathcal T$</span> is this set of triangles, then </p><p class="math-container">\[\tilde M(t) = \frac{1}{\mathrm{Area}(\Omega)}\sum_{T_k \in \mathcal T} \iint_{T_k} u(x, y, t)\mathrm{dA}. \]</p><p>Then, recall that <span>$u$</span> is represented as a linear function <span>$\alpha_k x + \beta_k y + \gamma_k$</span> inside the triangle <span>$T_k$</span>, thus </p><p class="math-container">\[\tilde M(t) \approx \frac{1}{\mathrm{Area}(\Omega)}\sum_{T_k \in \mathcal T} \left[\alpha_k \iint_{T_k} x\mathrm{dA} + \beta_k \iint_{T_k} y\mathrm{dA} + \gamma_k\iint_{T_k}\mathrm{dA}\right] \]</p><p>Now factoring out an <span>$\mathrm{Area}(T_k) = \iint_{T_k}\mathrm{dA}$</span>, </p><p class="math-container">\[\tilde M(t) \approx \sum_{T_k \in \mathcal T} \frac{\mathrm{Area}(T_k)}{\mathrm{Area}(\Omega)}\left[\alpha_k \dfrac{\iint_{T_k} x\mathrm{dA}}{\iint_{T_k} \mathrm{dA}} + \beta_k \dfrac{\iint_{T_k} y\mathrm{dA}}{\iint_{T_k} \mathrm{dA}} + \gamma_k\right]. \]</p><p>Notice that the two ratios of integrals shown are simply <span>$\hat x_k$</span> and <span>$\hat y_k$</span>, where <span>$(\hat x_k, \hat y_k)$</span> is the centroid of <span>$T_k$</span>. Thus, the term in brackets is <span>$\alpha_k \hat x_k + \beta_k \hat y_k + \gamma_k$</span>, which is the approximation to <span>$u$</span> at the centroid, <span>$\tilde u(\hat x_k, \hat y_k, t)$</span>. Thus, our approximation to the average density is </p><p class="math-container">\[\tilde M(t) \approx \sum_{T_k \in \mathcal T} w_k \tilde u(\hat x_k, \hat y_k, t), \qquad w_k = \frac{\mathrm{Area}(T_k)}{\mathrm{Area}(\Omega)}. \]</p><p>The following function provides a method for computing this mass. </p><pre><code class="language-julia hljs">function compute_mass!(M::AbstractVector{T}, αβγ, sol, prob) where {T}
    mesh_area = prob.mesh.mesh_information.total_area
    fill!(M, zero(T))
    for i in eachindex(M)
        for V in FiniteVolumeMethod.get_elements(prob)
            element = FiniteVolumeMethod.get_element_information(prob.mesh, V)
            cx, cy = FiniteVolumeMethod.get_centroid(element)
            element_area = FiniteVolumeMethod.get_area(element)
            interpolant_val = eval_interpolant!(αβγ, prob, cx, cy, V, sol.u[i])
            M[i] += (element_area / mesh_area) * interpolant_val
        end
    end
    return nothing
end </code></pre><p>Let&#39;s now compute this mass and add some noise onto it. </p><pre><code class="language-julia hljs">using Random 
M = zeros(length(sol.t))
αβγ = zeros(3)
compute_mass!(M, αβγ, sol, prob)
true_M = deepcopy(M)
Random.seed!(29922881)
σ = 0.1
true_M .+= σ * randn(length(M))</code></pre><h2 id="Defining-the-LikelihoodProblem"><a class="docs-heading-anchor" href="#Defining-the-LikelihoodProblem">Defining the LikelihoodProblem</a><a id="Defining-the-LikelihoodProblem-1"></a><a class="docs-heading-anchor-permalink" href="#Defining-the-LikelihoodProblem" title="Permalink"></a></h2><p>We now need to define the likelihood problem. We need to use the method for <code>LikelihoodProblem</code> that takes the <code>integrator</code> as an argument explicitly, so we must somehow construct an integrator from an <code>FVMProblem</code>. Here is one way that this can be done. Notice that we use <code>parallel=true</code> so that the PDE is solved with multithreading. For an isolated solution, this seems to solve the PDE twice as fast on my machine (eight threads).</p><pre><code class="language-julia hljs">function ProfileLikelihood.construct_integrator(prob::FVMProblem, alg; ode_problem_kwargs, kwargs...)
    ode_problem = ODEProblem(prob; no_saveat=false, ode_problem_kwargs...)
    return ProfileLikelihood.construct_integrator(ode_problem, alg; kwargs...)
end
jac = float.(FiniteVolumeMethod.jacobian_sparsity(prob))
fvm_integrator = construct_integrator(prob, alg; ode_problem_kwargs=(jac_prototype=jac, saveat=0.01, parallel=true))</code></pre><p>Now we define the likelihood function. </p><pre><code class="language-julia hljs">function loglik_fvm(θ::AbstractVector{T}, param, integrator) where {T}
    _k, _c, _u₀ = θ
    ## Update and solve
    (; prob) = param
    prob.flux_parameters[1] = _k
    pts = FiniteVolumeMethod.get_points(prob)
    for i in axes(pts, 2)
        pt = get_point(pts, i)
        prob.initial_condition[i] = gety(pt) ≤ _c ? _u₀ : zero(T)
    end
    reinit!(integrator, prob.initial_condition)
    solve!(integrator)
    if !SciMLBase.successful_retcode(integrator.sol)
        return typemin(T)
    end
    ## Compute the mass
    (; mass_data, mass_cache, shape_cache, sigma) = param
    compute_mass!(mass_cache, shape_cache, integrator.sol, prob)
    if any(isnan, mass_cache)
        return typemin(T)
    end
    ## Done 
    ℓ = @views gaussian_loglikelihood(mass_data, mass_cache, sigma, length(mass_data))
    return ℓ
end</code></pre><p>Finally, here is the <code>LikelihoodProblem</code>.</p><pre><code class="language-julia hljs">likprob = LikelihoodProblem(
    loglik_fvm,
    [8.54, 0.98, 29.83],
    fvm_integrator;
    syms=[:k, :c, :u₀],
    data=(prob=prob, mass_data=true_M, mass_cache=zeros(length(true_M)), shape_cache=zeros(3), sigma=σ),
    f_kwargs=(adtype=Optimization.AutoFiniteDiff(),),
    prob_kwargs=(lb=[3.0, 0.0, 0.0],
        ub=[15.0, 2.0, 250.0])
)</code></pre><h2 id="Parameter-estimation"><a class="docs-heading-anchor" href="#Parameter-estimation">Parameter estimation</a><a id="Parameter-estimation-1"></a><a class="docs-heading-anchor-permalink" href="#Parameter-estimation" title="Permalink"></a></h2><p>Now that we have the problem completely setup, we are in a position for maximum likelihood estimation and profiling. For the maximum likelihood estimates, we first use a global optimiser and then we refine the solution with a local optimiser.</p><pre><code class="language-julia hljs">mle_sol = mle(likprob, (NLopt.GN_DIRECT_L_RAND(), NLopt.LN_BOBYQA); ftol_abs=1e-8, ftol_rel=1e-8, xtol_abs=1e-8, xtol_rel=1e-8) # global, and then refine with a local algorithm
LikelihoodSolution. retcode: Failure
Maximum likelihood: 11.046014040624534
Maximum likelihood estimates: 3-element Vector{Float64}
     k: 7.847020395441574
     c: 1.1944331289720689
     u₀: 41.667309553688305</code></pre><p>Next, let us profile. For interest, we show the difference in runtime when we use multithreading for profiling vs. when we do not use multithreading. I am using eight threads.</p><pre><code class="language-julia hljs">@time prof = profile(likprob, mle_sol; alg=NLopt.LN_BOBYQA,
    ftol_abs=1e-4, ftol_rel=1e-4, xtol_abs=1e-4, xtol_rel=1e-4,
    resolution=60)
5131.960778 seconds (133.61 M allocations: 948.495 GiB, 0.13% gc time, 0.04% compilation time)</code></pre><pre><code class="language-julia hljs">@time _prof = profile(likprob, mle_sol; alg=NLopt.LN_BOBYQA,
    ftol_abs=1e-4, ftol_rel=1e-4, xtol_abs=1e-4, xtol_rel=1e-4,
    resolution=60, parallel=true)
3324.605865 seconds (131.24 M allocations: 948.598 GiB, 0.40% gc time, 0.01% compilation time)</code></pre><p>The results are about twice as fast in this example. The reason it&#39;s not even faster is because we are also using multithreading in solving the PDE. If we had no used multithreading in solving the PDE, these results would take a significantly longer time. Here are the results from <code>prof</code> (same for <code>_prof</code>):</p><pre><code class="language-julia hljs">ProfileLikelihoodSolution. MLE retcode: Failure
Confidence intervals: 
     95.0% CI for k: (7.4088716591304715, 8.574442050142432)
     95.0% CI for c: (0.6478281377475628, 2.0)
     95.0% CI for u₀: (33.78499567791489, 79.47955668442242)</code></pre><p>See that all the true parameter intervals are inside these confidence intervals except for <span>$k$</span>, although <span>$c$</span>&#39;s upper bound is right at the bounds we gave it in the problem. Let&#39;s now view the profile curves.</p><pre><code class="language-julia hljs">using CairoMakie
fig = plot_profiles(prof; nrow=1, ncol=3,
    latex_names=[L&quot;k&quot;, L&quot;c&quot;, L&quot;u_0&quot;],
    true_vals=[k[1], c, u₀],
    fig_kwargs=(fontsize=38, size=(2109.644f0, 444.242f0)),
    axis_kwargs=(width=600, height=300))
scatter!(fig.content[1], get_parameter_values(prof, :k), get_profile_values(prof, :k), color=:black, markersize=9)
scatter!(fig.content[2], get_parameter_values(prof, :c), get_profile_values(prof, :c), color=:black, markersize=9)
scatter!(fig.content[3], get_parameter_values(prof, :u₀), get_profile_values(prof, :u₀), color=:black, markersize=9)
xlims!(fig.content[1], 7.0, 9.5)</code></pre><p><img src="https://github.com/DanielVandH/ProfileLikelihood.jl/blob/main/test/figures/heat_pde_example.png?raw=true" alt="PDE profiles"/></p><p>See that the profile curves for <span>$c$</span> and <span>$u_0$</span> are very flat, and we have not recovered <span>$k$</span>. This means that the parameters <span>$c$</span> and <span>$u_0$</span> are not <em>identifiable</em>, essentially meaning the data is not enough to recover these parameters. This is most likely because the mass <span>$\tilde M(t)$</span> alone is not enough to uniquely define the solution. We could consider a summary statistic like </p><p class="math-container">\[\mathcal S(t) = w\tilde M(t) + (1-w)\tilde A(t),\]</p><p>for some <span>$0 \leq w \leq 1$</span>, where <span>$\tilde A(t)$</span> is the area of the region below the leading edge of the solution, i.e. the area of the non-zero part of the solution. We do not consider this here. What we do consider is fixing <span>$c$</span>, keeping the summary statistic <span>$\tilde M(t)$</span>, and seeing what we can do with only two parameters <span>$k$</span> and <span>$u_0$</span>.</p><h2 id="Reducing-to-two-parameters-and-grid-searching"><a class="docs-heading-anchor" href="#Reducing-to-two-parameters-and-grid-searching">Reducing to two parameters and grid searching</a><a id="Reducing-to-two-parameters-and-grid-searching-1"></a><a class="docs-heading-anchor-permalink" href="#Reducing-to-two-parameters-and-grid-searching" title="Permalink"></a></h2><p>Let us now fix <span>$c$</span> at its true value, <span>$c = 1$</span>, and consider estimating only <span>$k$</span> and <span>$u_0$</span>. Since we have only <span>$k$</span> and <span>$u_0$</span> to estimate, it may be worthwhile to perform a grid search over our likelihood function so that we can (1) visualise the likelihood surface and (2) see reasonable estimates for <span>$k$</span> and <span>$u_0$</span>. </p><p>First, we redefine the problem.</p><pre><code class="language-julia hljs">using StaticArraysCore
@inline function loglik_fvm_2(θ::AbstractVector{T}, param, integrator) where {T}
    _k, _u₀, = θ
    (; c) = param
    new_θ = SVector{3,T}((_k, c, _u₀))
    return loglik_fvm(new_θ, param, integrator)

end
likprob_2 = LikelihoodProblem(
    loglik_fvm_2,
    [8.54, 29.83],
    fvm_integrator;
    syms=[:k, :u₀],
    data=(prob=prob, mass_data=true_M, mass_cache=zeros(length(true_M)), shape_cache=zeros(3), sigma=σ, c=c),
    f_kwargs=(adtype=Optimization.AutoFiniteDiff(),),
    prob_kwargs=(lb=[3.0, 0.0],
        ub=[15.0, 250.0])
)</code></pre><p>Now let&#39;s do our grid search. We show the timing when we use a multithreaded grid search vs. a serial grid search. </p><pre><code class="language-julia hljs">grid = RegularGrid(get_lower_bounds(likprob_2), get_upper_bounds(likprob_2), 50)
@time gs, lik_vals = grid_search(likprob_2, grid; save_vals = Val(true), parallel=Val(true))
1529.393520 seconds (91.55 M allocations: 606.223 GiB, 2.10% gc time)</code></pre><pre><code class="language-julia hljs">@time _gs, _lik_vals = grid_search(likprob_2, grid; save_vals = Val(true), parallel=Val(false))
3454.357503 seconds (86.48 M allocations: 605.468 GiB, 0.14% gc time)</code></pre><p>Here are the results from the grid search.</p><pre><code class="language-julia hljs">LikelihoodSolution. retcode: Success
Maximum likelihood: -24.399451875029165
Maximum likelihood estimates: 2-element Vector{Float64}
     k: 7.408163265306122
     u₀: 51.0204081632653</code></pre><p>Let us now visualise the likelihood function.</p><pre><code class="language-julia hljs">fig = Figure(fontsize=38)
k_grid = get_range(grid, 1)
u₀_grid = get_range(grid, 2)
ax = Axis(fig[1, 1],
    xlabel=L&quot;k&quot;, ylabel=L&quot;u_0&quot;,
    xticks=0:3:15,
    yticks=0:50:250)
co = heatmap!(ax, k_grid, u₀_grid, lik_vals, colormap=Reverse(:matter))
contour!(ax, k_grid, u₀_grid, lik_vals, levels=40, color=:black, linewidth=1 / 4)
scatter!(ax, [k[1]], [u₀], color=:white, markersize=14)
scatter!(ax, [gs[:k]], [gs[:u₀]], color=:blue, markersize=14)
clb = Colorbar(fig[1, 2], co, label=L&quot;\ell(k, u_0)&quot;, vertical=true)</code></pre><p><img src="https://github.com/DanielVandH/ProfileLikelihood.jl/blob/main/test/figures/heat_pde_contour_example.png?raw=true" alt="Likelihood function for the PDE"/></p><p>The true parameter values are shown at the white marker, while the results from the grid search are shown in blue, and these two markers are reasonably close. We see that the likelihood function is quite flat around these values, so this might be an indicator of further identifiability issues. Let us now use the grid search results to update our initial guess and compute the MLEs, and then we profile.</p><pre><code class="language-julia hljs">likprob_2 = update_initial_estimate(likprob_2, gs)
mle_sol = mle(likprob_2, NLopt.LN_BOBYQA; ftol_abs=1e-8, ftol_rel=1e-8, xtol_abs=1e-8, xtol_rel=1e-8)
LikelihoodSolution. retcode: Failure
Maximum likelihood: 11.016184577792082
Maximum likelihood estimates: 2-element Vector{Float64}
     k: 9.40527352240195
     u₀: 49.741093700294336</code></pre><pre><code class="language-julia hljs">@time prof = profile(likprob_2, mle_sol; ftol_abs=1e-4, ftol_rel=1e-4, xtol_abs=1e-4, xtol_rel=1e-4, parallel=true)
612.723061 seconds (25.45 M allocations: 155.874 GiB, 0.41% gc time)
ProfileLikelihoodSolution. MLE retcode: Failure
Confidence intervals: 
     95.0% CI for k: (8.788003299163778, 10.094019297587579)
     95.0% CI for u₀: (49.44377511158833, 50.03883730450469)</code></pre><p>The confidence intervals contain the true values. We can now visualise.</p><pre><code class="language-julia hljs">fig = plot_profiles(prof; nrow=1, ncol=3,
    latex_names=[L&quot;k&quot;, L&quot;u_0&quot;],
    true_vals=[k[1], u₀],
    fig_kwargs=(fontsize=38, size=(1441.9216f0, 470.17322f0)),
    axis_kwargs=(width=600, height=300))
scatter!(fig.content[1], get_parameter_values(prof, :k), get_profile_values(prof, :k), color=:black, markersize=9)
scatter!(fig.content[2], get_parameter_values(prof, :u₀), get_profile_values(prof, :u₀), color=:black, markersize=9)</code></pre><p><img src="https://github.com/DanielVandH/ProfileLikelihood.jl/blob/main/test/figures/heat_pde_example_2.png?raw=true" alt="Second set of profiles"/></p><p>See that we&#39;ve recovered the parameters in the confidence intervals, and the profiles are smooth – the identifiability issues are gone. So, it seems like <span>$c$</span> was the problematic parameter, since our summary statistic does not really give us any information about it. Our idea of using the summary statistic <span>$\mathcal S(t)$</span> from above would likely ameliorate this issue, since it will give information directly relating to <span>$c$</span>.</p><h2 id="Comparing-methods-for-constructing-initial-estimates-when-profiling"><a class="docs-heading-anchor" href="#Comparing-methods-for-constructing-initial-estimates-when-profiling">Comparing methods for constructing initial estimates when profiling</a><a id="Comparing-methods-for-constructing-initial-estimates-when-profiling-1"></a><a class="docs-heading-anchor-permalink" href="#Comparing-methods-for-constructing-initial-estimates-when-profiling" title="Permalink"></a></h2><p>In the mathematical details section at the end of this README, it is mentioned that initial values for <span>$\boldsymbol\omega_j$</span> (the parameters to be optimised while an interest parameter is held fixed) can currently be set in two ways:</p><ul><li>Method 1: Simply starting <span>$\boldsymbol\omega_j$</span> at <span>$\boldsymbol\omega_{j-1}$</span>. This is the <code>next_initial_estimate_method = :prev</code> option in <code>profile</code>, and is the default.</li><li>Method 2: Using linear interpolation, we can use the previous two values and set <span>$\boldsymbol\omega_j = [\boldsymbol\omega_{j-2}(\psi_{j-1} - \psi_j) + \boldsymbol\omega_{j-1}(\psi_j - \psi_{j-2})] / (\psi_{j-1} - \psi_{j-2})$</span> (if <span>$\boldsymbol\omega_j$</span> then starts outside of the parameter bounds, we fall back to the first method). This is the <code>next_initial_estimate_method = :interp</code> option in <code>profile</code>.</li></ul><p>Is there a big difference in these methods? Let&#39;s demonstrate if there is any difference by doing some benchmarking. We will also compare multithreading versus no multithreading.</p><pre><code class="language-julia hljs">bnch_prev_serial = @benchmark profile($likprob_2, $mle_sol; ftol_abs=$1e-4, ftol_rel=$1e-4, xtol_abs=$1e-4, xtol_rel=$1e-4, parallel=$false, next_initial_estimate_method=$:prev)
bnch_interp_serial = @benchmark profile($likprob_2, $mle_sol; ftol_abs=$1e-4, ftol_rel=$1e-4, xtol_abs=$1e-4, xtol_rel=$1e-4, parallel=$false, next_initial_estimate_method=$:interp)
bnch_prev_parallel = @benchmark profile($likprob_2, $mle_sol; ftol_abs=$1e-4, ftol_rel=$1e-4, xtol_abs=$1e-4, xtol_rel=$1e-4, parallel=$true, next_initial_estimate_method=$:prev)
bnch_interp_parallel = @benchmark profile($likprob_2, $mle_sol; ftol_abs=$1e-4, ftol_rel=$1e-4, xtol_abs=$1e-4, xtol_rel=$1e-4, parallel=$true, next_initial_estimate_method=$:interp)</code></pre><p>Here are the results:</p><pre><code class="language-julia hljs">julia&gt; bnch_prev_serial
BenchmarkTools.Trial: 1 sample with 1 evaluation.
 Single result which took 855.578 s (0.23% GC) to evaluate,
 with a memory estimate of 155.70 GiB, over 24670284 allocations.</code></pre><pre><code class="language-julia hljs">julia&gt; bnch_interp_serial
BenchmarkTools.Trial: 1 sample with 1 evaluation.
 Single result which took 757.444 s (0.24% GC) to evaluate,
 with a memory estimate of 144.34 GiB, over 22976564 allocations.</code></pre><pre><code class="language-julia hljs">julia&gt; bnch_prev_parallel
BenchmarkTools.Trial: 1 sample with 1 evaluation.
 Single result which took 548.814 s (0.34% GC) to evaluate,
 with a memory estimate of 155.87 GiB, over 25443078 allocations.</code></pre><pre><code class="language-julia hljs">julia&gt; bnch_interp_parallel
BenchmarkTools.Trial: 1 sample with 1 evaluation.
 Single result which took 498.408 s (0.36% GC) to evaluate,
 with a memory estimate of 144.52 GiB, over 23809418 allocations.</code></pre><p>We see that linear interpolation is a significant help to the algorithm, saving 100 seconds when we profile without multithreading, and 50 seconds when we profile with multithreading. In summary, profiling with the <code>:interp</code> method was about 12% faster than <code>:prev</code> without multithreading, and about 10% faster with multithreading –- interpolation is certainly a big help. For problems where the likelihood function is much faster to compute, these results may be opposite – it is worth thinking about this for your applications.</p><h2 id="Prediction-intervals-for-the-mass"><a class="docs-heading-anchor" href="#Prediction-intervals-for-the-mass">Prediction intervals for the mass</a><a id="Prediction-intervals-for-the-mass-1"></a><a class="docs-heading-anchor-permalink" href="#Prediction-intervals-for-the-mass" title="Permalink"></a></h2><p>Let us now consider propagating the uncertainty in <span>$k$</span> and <span>$u_0$</span> into computing prediction intervals for <span>$\tilde M(t)$</span> at each <span>$t$</span>. This is done using the <code>get_prediction_intervals</code> function introduced in the second example. First, we must define our prediction function.</p><pre><code class="language-julia hljs">@inline function compute_mass_function(θ::AbstractVector{T}, data) where {T}
    k, u₀ = θ
    (; c, prob, t, alg, jac) = data
    prob.flux_parameters[1] = k
    pts = FiniteVolumeMethod.get_points(prob)
    for i in axes(pts, 2)
        pt = get_point(pts, i)
        prob.initial_condition[i] = gety(pt) ≤ c ? u₀ : zero(T)
    end
    sol = solve(prob, alg; saveat=t, parallel=true, jac_prototype=jac)
    shape_cache = zeros(T, 3)
    mass_cache = zeros(T, length(sol.u))
    compute_mass!(mass_cache, shape_cache, sol, prob)
    return mass_cache
end</code></pre><p>Now let&#39;s get the intervals.</p><pre><code class="language-julia hljs">t_many_pts = LinRange(prob.initial_time, prob.final_time, 250)
jac = FiniteVolumeMethod.jacobian_sparsity(prob)
prediction_data = (c=c, prob=prob, t=t_many_pts, alg=alg, jac=jac)
parameter_wise, union_intervals, all_curves, param_range =
    get_prediction_intervals(compute_mass_function, prof, prediction_data; parallel=true)</code></pre><p>Now we can visualise the curves. We will also show the mass curve from the exact parameter values, as well as from the MLE. </p><pre><code class="language-julia hljs">exact_soln = compute_mass_function([k[1], u₀], prediction_data)
mle_soln = compute_mass_function(get_mle(mle_sol), prediction_data)
fig = Figure(fontsize=38, size=(1360.512f0, 848.64404f0))
alp = join(&#39;a&#39;:&#39;z&#39;)
latex_names = [L&quot;k&quot;, L&quot;u_0&quot;]
for i in 1:2
    ax = Axis(fig[1, i], title=L&quot;(%$(alp[i])): Profile-wise PI for %$(latex_names[i])&quot;,
        titlealign=:left, width=600, height=300)
    [lines!(ax, t_many_pts, all_curves[i][:, j], color=:grey) for j in eachindex(param_range[1])]
    lines!(ax, t_many_pts, exact_soln, color=:red)
    lines!(ax, t_many_pts, mle_soln, color=:blue, linestyle=:dash)
    lines!(ax, t_many_pts, getindex.(parameter_wise[i], 1), color=:black)
    lines!(ax, t_many_pts, getindex.(parameter_wise[i], 2), color=:black)
end
ax = Axis(fig[2, 1:2], title=L&quot;(c):$ $ Union of all intervals&quot;,
    titlealign=:left, width=1200, height=300)
band!(ax, t_many_pts, getindex.(union_intervals, 1), getindex.(union_intervals, 2), color=:grey)
lines!(ax, t_many_pts, getindex.(union_intervals, 1), color=:black)
lines!(ax, t_many_pts, getindex.(union_intervals, 2), color=:black)
lines!(ax, t_many_pts, exact_soln, color=:red)
lines!(ax, t_many_pts, mle_soln, color=:blue, linestyle=:dash)</code></pre><p>Let us also add onto these plots the intervals coming from the full likelihood. (The reason to just not do this everytime in applications is because the code below takes a <em>very</em> long time to compute - a lifetime compared to the profile-wise intervals above.)</p><pre><code class="language-julia hljs">lb = [8.0, 45.0]
ub = [11.0, 50.0]
N = 1e4
grid = [[lb[i] + (ub[i] - lb[i]) * rand() for _ in 1:N] for i in 1:2]
grid = permutedims(reduce(hcat, grid), (2, 1))
ig = IrregularGrid(lb, ub, grid)
gs, lik_vals = grid_search(likprob_2, ig; parallel=Val(true), save_vals=Val(true))
lik_vals .-= get_maximum(mle_sol) # normalised 
feasible_idx = findall(lik_vals .&gt; ProfileLikelihood.get_chisq_threshold(0.95)) # values in the confidence region 
parameter_evals = grid[:, feasible_idx]
q = [compute_mass_function(θ, prediction_data) for θ in eachcol(parameter_evals)]
q_mat = reduce(hcat, q)
q_lwr = minimum(q_mat; dims=2) |&gt; vec
q_upr = maximum(q_mat; dims=2) |&gt; vec
lines!(ax, t_many_pts, q_lwr, color=:magenta)
lines!(ax, t_many_pts, q_upr, color=:magenta)</code></pre><p><img src="https://github.com/DanielVandH/ProfileLikelihood.jl/blob/main/test/figures/heat_pde_example_mass.png?raw=true" alt="Prediction intervals for the mass"/></p><p>The exact curve has been recovered by our profile likelihood results, and the uncertainty is extremely small. Moreover, the intervals are indeed close to the interval obtained the full profile likelihood as we would hope.</p><h2 id="Just-the-code"><a class="docs-heading-anchor" href="#Just-the-code">Just the code</a><a id="Just-the-code-1"></a><a class="docs-heading-anchor-permalink" href="#Just-the-code" title="Permalink"></a></h2><p>Here is all the code used for obtaining the results in this example, should you want a version that you can directly copy and paste.</p><pre><code class="language-julia hljs">## Step 1: Define the problem. See FiniteVolumeMethod.jl
using DelaunayTriangulation, FiniteVolumeMethod
a, b, c, d = 0.0, 2.0, 0.0, 2.0
r = 0.022
GMSH_PATH = &quot;./gmsh-4.9.4-Windows64/gmsh.exe&quot;
tri = generate_mesh(a, b, c, d, r; gmsh_path=GMSH_PATH)
mesh = FVMGeometry(tri)
bc = ((x, y, t, u::T, p) where {T}) -&gt; zero(T)
type = :D
BCs = BoundaryConditions(mesh, bc, type)
c = 1.0
u₀ = 50.0
f = (x, y) -&gt; y ≤ c ? u₀ : 0.0
D = (x, y, t, u, p) -&gt; p[1]
flux = (q, x, y, t, α, β, γ, p) -&gt; (q[1] = -α / p[1]; q[2] = -β / p[1])
R = ((x, y, t, u::T, p) where {T}) -&gt; zero(T)
points = get_points(tri)
initc = @views f.(points[1, :], points[2, :])
iip_flux = true
final_time = 0.1
k = [9.0]
prob = FVMProblem(mesh, BCs; iip_flux,
    flux_function=flux, reaction_function=R,
    initial_condition=initc, final_time,
    flux_parameters=deepcopy(k))

## Step 2: Generate some data.
using LinearSolve, OrdinaryDiffEq
alg = TRBDF2(linsolve=KLUFactorization(; reuse_symbolic=false))
sol = solve(prob, alg; specialization=SciMLBase.FullSpecialize, saveat=0.01)    

## Step 3: Let us compute the mass at each time and then add some noise to it
using Random
function compute_mass!(M::AbstractVector{T}, αβγ, sol, prob) where {T}
    mesh_area = prob.mesh.mesh_information.total_area
    fill!(M, zero(T))
    for i in eachindex(M)
        for V in FiniteVolumeMethod.get_elements(prob)
            element = FiniteVolumeMethod.get_element_information(prob.mesh, V)
            cx, cy = FiniteVolumeMethod.get_centroid(element)
            element_area = FiniteVolumeMethod.get_area(element)
            interpolant_val = eval_interpolant!(αβγ, prob, cx, cy, V, sol.u[i])
            M[i] += (element_area / mesh_area) * interpolant_val
        end
    end
    return nothing
end
M = zeros(length(sol.t))
αβγ = zeros(3)
compute_mass!(M, αβγ, sol, prob)
true_M = deepcopy(M)
Random.seed!(29922881)
σ = 0.1
true_M .+= σ * randn(length(M))

## Step 4: We need to now construct the integrator. Here&#39;s a method for converting an FVMProblem into an integrator. 
function ProfileLikelihood.construct_integrator(prob::FVMProblem, alg; ode_problem_kwargs, kwargs...)
    ode_problem = ODEProblem(prob; no_saveat=false, ode_problem_kwargs...)
    return ProfileLikelihood.construct_integrator(ode_problem, alg; kwargs...)
end
jac = float.(FiniteVolumeMethod.jacobian_sparsity(prob))
fvm_integrator = construct_integrator(prob, alg; ode_problem_kwargs=(jac_prototype=jac, saveat=0.01, parallel=true))

## Step 5: Now define the likelihood problem 
using Optimization
@inline function loglik_fvm(θ::AbstractVector{T}, param, integrator) where {T}
    _k, _c, _u₀ = θ
    ## Update and solve
    (; prob) = param
    prob.flux_parameters[1] = _k
    pts = FiniteVolumeMethod.get_points(prob)
    for i in axes(pts, 2)
        pt = get_point(pts, i)
        prob.initial_condition[i] = gety(pt) ≤ _c ? _u₀ : zero(T)
    end
    reinit!(integrator, prob.initial_condition)
    solve!(integrator)
    if !SciMLBase.successful_retcode(integrator.sol)
        return typemin(T)
    end
    ## Compute the mass
    (; mass_data, mass_cache, shape_cache, sigma) = param
    compute_mass!(mass_cache, shape_cache, integrator.sol, prob)
    if any(isnan, mass_cache)
        return typemin(T)
    end
    ## Done 
    ℓ = @views gaussian_loglikelihood(mass_data, mass_cache, sigma, length(mass_data))
    @show ℓ
    return ℓ
end
likprob = LikelihoodProblem(
    loglik_fvm,
    [8.54, 0.98, 29.83],
    fvm_integrator;
    syms=[:k, :c, :u₀],
    data=(prob=prob, mass_data=true_M, mass_cache=zeros(length(true_M)), shape_cache=zeros(3), sigma=σ),
    f_kwargs=(adtype=Optimization.AutoFiniteDiff(),),
    prob_kwargs=(lb=[3.0, 0.0, 0.0],
        ub=[15.0, 2.0, 250.0])
)

## Step 6: Find the MLEs 
using OptimizationNLopt 
mle_sol = mle(likprob, (NLopt.GN_DIRECT_L_RAND(), NLopt.LN_BOBYQA); ftol_abs=1e-8, ftol_rel=1e-8, xtol_abs=1e-8, xtol_rel=1e-8) # global, and then refine with a local algorithm

## Step 7: Profile 
prof = profile(likprob, mle_sol; alg=NLopt.LN_BOBYQA,
    ftol_abs=1e-4, ftol_rel=1e-4, xtol_abs=1e-4, xtol_rel=1e-4,
    resolution=60)

## Step 8: Visualise 
using CairoMakie
fig = plot_profiles(prof; nrow=1, ncol=3,
    latex_names=[L&quot;k&quot;, L&quot;c&quot;, L&quot;u_0&quot;],
    true_vals=[k[1], c, u₀],
    fig_kwargs=(fontsize=38, size=(2109.644f0, 444.242f0)),
    axis_kwargs=(width=600, height=300))
scatter!(fig.content[1], get_parameter_values(prof, :k), get_profile_values(prof, :k), color=:black, markersize=9)
scatter!(fig.content[2], get_parameter_values(prof, :c), get_profile_values(prof, :c), color=:black, markersize=9)
scatter!(fig.content[3], get_parameter_values(prof, :u₀), get_profile_values(prof, :u₀), color=:black, markersize=9)
xlims!(fig.content[1], 7.0, 9.5)

### Now consider profiling only two parameters
## Step 9: Define the problem 
using StaticArraysCore
@inline function loglik_fvm_2(θ::AbstractVector{T}, param, integrator) where {T}
    _k, _u₀, = θ
    (; c) = param
    new_θ = SVector{3,T}((_k, c, _u₀))
    return loglik_fvm(new_θ, param, integrator)

end
likprob_2 = LikelihoodProblem(
    loglik_fvm_2,
    [8.54, 29.83],
    fvm_integrator;
    syms=[:k, :u₀],
    data=(prob=prob, mass_data=true_M, mass_cache=zeros(length(true_M)), shape_cache=zeros(3), sigma=σ, c=c),
    f_kwargs=(adtype=Optimization.AutoFiniteDiff(),),
    prob_kwargs=(lb=[3.0, 0.0],
        ub=[15.0, 250.0])
)

## Step 10: Grid search 
grid = RegularGrid(get_lower_bounds(likprob_2), get_upper_bounds(likprob_2), 50)
gs, lik_vals = grid_search(likprob_2, grid; save_vals=Val(true), parallel=Val(true));

## Step 11: Visualise 
fig = Figure(fontsize=38)
k_grid = get_range(grid, 1)
u₀_grid = get_range(grid, 2)
ax = Axis(fig[1, 1],
    xlabel=L&quot;k&quot;, ylabel=L&quot;u_0&quot;,
    xticks=0:3:15,
    yticks=0:50:250)
co = heatmap!(ax, k_grid, u₀_grid, lik_vals, colormap=Reverse(:matter))
contour!(ax, k_grid, u₀_grid, lik_vals, levels=40, color=:black, linewidth=1 / 4)
scatter!(ax, [k[1]], [u₀], color=:white, markersize=14)
scatter!(ax, [gs[:k]], [gs[:u₀]], color=:blue, markersize=14)
clb = Colorbar(fig[1, 2], co, label=L&quot;\ell(k, u_0)&quot;, vertical=true)

## Step 12: Find the MLEs and profile 
likprob_2 = update_initial_estimate(likprob_2, gs)
mle_sol = mle(likprob_2, NLopt.LN_BOBYQA; ftol_abs=1e-8, ftol_rel=1e-8, xtol_abs=1e-8, xtol_rel=1e-8)
prof = profile(likprob_2, mle_sol; ftol_abs=1e-4, ftol_rel=1e-4, xtol_abs=1e-4, xtol_rel=1e-4, parallel=true)

## Step 13: Visualise the profile 
fig = plot_profiles(prof; nrow=1, ncol=3,
    latex_names=[L&quot;k&quot;, L&quot;u_0&quot;],
    true_vals=[k[1], u₀],
    fig_kwargs=(fontsize=38, size=(1441.9216f0, 470.17322f0)),
    axis_kwargs=(width=600, height=300))
scatter!(fig.content[1], get_parameter_values(prof, :k), get_profile_values(prof, :k), color=:black, markersize=9)
scatter!(fig.content[2], get_parameter_values(prof, :u₀), get_profile_values(prof, :u₀), color=:black, markersize=9)

## Step 14: Compute prediction intervals for the mass
@inline function compute_mass_function(θ::AbstractVector{T}, data) where {T}
    k, u₀ = θ
    (; c, prob, t, alg, jac) = data
    prob.flux_parameters[1] = k
    pts = FiniteVolumeMethod.get_points(prob)
    for i in axes(pts, 2)
        pt = get_point(pts, i)
        prob.initial_condition[i] = gety(pt) ≤ c ? u₀ : zero(T)
    end
    sol = solve(prob, alg; saveat=t, parallel=true, jac_prototype=jac)
    shape_cache = zeros(T, 3)
    mass_cache = zeros(T, length(sol.u))
    compute_mass!(mass_cache, shape_cache, sol, prob)
    return mass_cache
end
t_many_pts = LinRange(prob.initial_time, prob.final_time, 250)
jac = FiniteVolumeMethod.jacobian_sparsity(prob)
prediction_data = (c=c, prob=prob, t=t_many_pts, alg=alg, jac=jac)
parameter_wise, union_intervals, all_curves, param_range =
    get_prediction_intervals(compute_mass_function, prof, prediction_data; parallel=true)

## Step 15: Visualise the prediction intervals
exact_soln = compute_mass_function([k[1], u₀], prediction_data)
mle_soln = compute_mass_function(get_mle(mle_sol), prediction_data)

fig = Figure(fontsize=38, size=(1360.512f0, 848.64404f0))
alp = join(&#39;a&#39;:&#39;z&#39;)
latex_names = [L&quot;k&quot;, L&quot;u_0&quot;]
for i in 1:2
    ax = Axis(fig[1, i], title=L&quot;(%$(alp[i])): Profile-wise PI for %$(latex_names[i])&quot;,
        titlealign=:left, width=600, height=300)
    [lines!(ax, t_many_pts, all_curves[i][:, j], color=:grey) for j in eachindex(param_range[1])]
    lines!(ax, t_many_pts, exact_soln, color=:red)
    lines!(ax, t_many_pts, mle_soln, color=:blue, linestyle=:dash)
    lines!(ax, t_many_pts, getindex.(parameter_wise[i], 1), color=:black)
    lines!(ax, t_many_pts, getindex.(parameter_wise[i], 2), color=:black)
end
ax = Axis(fig[2, 1:2], title=L&quot;(c):$ $ Union of all intervals&quot;,
    titlealign=:left, width=1200, height=300)
band!(ax, t_many_pts, getindex.(union_intervals, 1), getindex.(union_intervals, 2), color=:grey)
lines!(ax, t_many_pts, getindex.(union_intervals, 1), color=:black)
lines!(ax, t_many_pts, getindex.(union_intervals, 2), color=:black)
lines!(ax, t_many_pts, exact_soln, color=:red)
lines!(ax, t_many_pts, mle_soln, color=:blue, linestyle=:dash)

lb = [8.0, 45.0]
ub = [11.0, 50.0]
N = 1e4
grid = [[lb[i] + (ub[i] - lb[i]) * rand() for _ in 1:N] for i in 1:2]
grid = permutedims(reduce(hcat, grid), (2, 1))
ig = IrregularGrid(lb, ub, grid)
gs, lik_vals = grid_search(likprob_2, ig; parallel=Val(true), save_vals=Val(true))
lik_vals .-= get_maximum(mle_sol) # normalised 
feasible_idx = findall(lik_vals .&gt; ProfileLikelihood.get_chisq_threshold(0.95)) # values in the confidence region 
parameter_evals = grid[:, feasible_idx]
q = [compute_mass_function(θ, prediction_data) for θ in eachcol(parameter_evals)]
q_mat = reduce(hcat, q)
q_lwr = minimum(q_mat; dims=2) |&gt; vec
q_upr = maximum(q_mat; dims=2) |&gt; vec
lines!(ax, t_many_pts, q_lwr, color=:magenta)
lines!(ax, t_many_pts, q_upr, color=:magenta)</code></pre></article><nav class="docs-footer"><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.6.0 on <span class="colophon-date" title="Saturday 31 August 2024 08:43">Saturday 31 August 2024</span>. Using Julia version 1.10.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
