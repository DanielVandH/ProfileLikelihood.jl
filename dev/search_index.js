var documenterSearchIndex = {"docs":
[{"location":"lotka/#Example-V:-Lotka-Volterra-ODE,-GeneralLazyBufferCache,-and-computing-bivarate-profile-likelihoods","page":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","title":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","text":"","category":"section"},{"location":"lotka/","page":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","title":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","text":"This example comes from the second case study of Simpson and Maclaren (2022). First, load the packages we'll be using:","category":"page"},{"location":"lotka/","page":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","title":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","text":"using Random\nusing Optimization\nusing OrdinaryDiffEq\nusing CairoMakie\nusing LaTeXStrings\nusing ProfileLikelihood\nusing OptimizationNLopt\nusing PreallocationTools\nusing OptimizationOptimJL\nusing LoopVectorization\nusing AbbreviatedStackTraces","category":"page"},{"location":"lotka/","page":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","title":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","text":"In this example, we will be considering the Lotka-Volterra ODE, and we will also demonstrate how the GeneralLazyBufferCache from PreallocationTools.jl can be used for supporting automatic differentiation for similar problems. In addition, we now also show how bivariate profiles can be computed, along with prediction intervals from a bivariate profile.  The Lotka-Volterra ODE is given by ","category":"page"},{"location":"lotka/","page":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","title":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","text":"beginalign*\nfracmathrm da(t)mathrm dt = alpha a(t) - a(t)b(t) \nfracmathrm db(t)mathrm dt = beta a(t)b(t)-b(t)\nendalign*","category":"page"},{"location":"lotka/","page":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","title":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","text":"and we suppose that a(0) = a_0 and b(0) = b_0. For this problem, we are interested in estimating boldsymbol = (alphabetaa_0b_0). We suppose that we have measures of the prey and predicator populations, given respectively by a(t) and b(t), at times t_i, i=1ldotsm. Letting a_i^ o = a(t_i) and b_i^o = b(t_i), i=1ldotsm, this means that we have the time series (a_i^o b_i^o)_i=1^m. Moreover, just as we did in the logistic ODE example, we suppose that the data (a_i^o b_i^o) are normally distributed about the solution curve boldsymbol z(t boldsymboltheta) = (a(t boldsymbol theta) b(t boldsymbol theta)). In particular, letting boldsymbol z_i(boldsymbol theta) denote the value of (a(t_i boldsymboltheta) b(t_i boldsymbol theta)) at t=t_i, we are supposing that ","category":"page"},{"location":"lotka/","page":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","title":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","text":"(a_i^o b_i^o) sim mathcal Nleft(boldsymbol z_i(boldsymbol theta) sigma^2 boldsymbol Iright) quad i=12ldotsm","category":"page"},{"location":"lotka/","page":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","title":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","text":"and this is what defines our likelihood (boldsymbol I is the 2-square identity matrix). We use values 0 leq t leq 7 for estimation, and predict on 0 leq t leq 10.","category":"page"},{"location":"lotka/#Data-generation-and-setting-up-the-problem","page":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","title":"Data generation and setting up the problem","text":"","category":"section"},{"location":"lotka/","page":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","title":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","text":"As usual, the first step in this example is generating the data.","category":"page"},{"location":"lotka/","page":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","title":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","text":"using OrdinaryDiffEq, Random, Random \n\n## Step 1: Generate the data and define the likelihood\nα = 0.9\nβ = 1.1\na₀ = 0.8\nb₀ = 0.3\nσ = 0.2\nt = LinRange(0, 10, 21)\n@inline function ode_fnc!(du, u, p, t) \n    α, β = p\n    a, b = u\n    du[1] = α * a - a * b\n    du[2] = β * a * b - b\n    return nothing\nend\n# Initial data is obtained by solving the ODE \ntspan = extrema(t)\np = [α, β]\nu₀ = [a₀, b₀]\nprob = ODEProblem(ode_fnc!, u₀, tspan, p)\nsol = solve(prob, Rosenbrock23(), saveat=t)\nRandom.seed!(2528)\nnoise_vec = [σ * randn(2) for _ in eachindex(t)]\nuᵒ = sol.u .+ noise_vec","category":"page"},{"location":"lotka/","page":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","title":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","text":"We now define the likelihood function. ","category":"page"},{"location":"lotka/","page":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","title":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","text":"@inline function loglik_fnc2(θ::AbstractVector{T}, data, integrator) where {T}\n    α, β, a₀, b₀ = θ\n    uᵒ, σ, u₀_cache, αβ_cache, n = data\n    u₀ = get_tmp(u₀_cache, θ)\n    integrator.p[1] = α\n    integrator.p[2] = β\n    u₀[1] = a₀\n    u₀[2] = b₀\n    reinit!(integrator, u₀)\n    solve!(integrator)\n    ℓ = zero(T)\n    for i in 1:n\n        âᵒ = integrator.sol.u[i][1]\n        b̂ᵒ = integrator.sol.u[i][2]\n        aᵒ = uᵒ[i][1]\n        bᵒ = uᵒ[i][2]\n        ℓ = ℓ - 0.5log(2π * σ^2) - 0.5(âᵒ - aᵒ)^2 / σ^2\n        ℓ = ℓ - 0.5log(2π * σ^2) - 0.5(b̂ᵒ - bᵒ)^2 / σ^2\n    end\n    return ℓ\nend","category":"page"},{"location":"lotka/","page":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","title":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","text":"Now we define our problem, constraining the parameters so that 07 leq alpha leq 12, 07 leq beta leq 14, 05 leq a_0 leq 12, and 01 leq b_0 leq 05. We want to use forward differentiation for this. Let us start by showing a method that fails:","category":"page"},{"location":"lotka/","page":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","title":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","text":"using AbbreviatedStackTraces, PreallocationTools, Optimization, OrdinaryDiffEq, OptimizationOptimJL\nlb = [0.7, 0.7, 0.5, 0.1]\nub = [1.2, 1.4, 1.2, 0.5]\nθ₀ = [0.75, 1.23, 0.76, 0.292]\nsyms = [:α, :β, :a₀, :b₀]\nu₀_cache = DiffCache(zeros(2), 12)\nαβ_cache = DiffCache(zeros(2), 12)\nn = findlast(t .≤ 7) # Using t ≤ 7 for estimation\nprob = LikelihoodProblem(\n    loglik_fnc2, θ₀, ode_fnc!, u₀, tspan;\n    syms=syms,\n    data=(uᵒ, σ, u₀_cache, αβ_cache, n),\n    ode_parameters=[1.0, 1.0],\n    ode_kwargs=(verbose=false, saveat=t),\n    f_kwargs=(adtype=Optimization.AutoForwardDiff(),),\n    prob_kwargs=(lb=lb, ub=ub),\n    ode_alg=Rosenbrock23()\n)\nmle(prob, Optim.LBFGS())","category":"page"},{"location":"lotka/","page":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","title":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","text":"julia> sol = mle(prob, Optim.LBFGS())\nERROR: MethodError: no method matching Float64(::ForwardDiff.Dual{ForwardDiff.Tag{Optimization.var\"#89#106\"{OptimizationFunction{true, Optimization.AutoForwardDiff{nothing}, …}, Tuple{Vector{Vector{Float64}}, Float64, …}}, Float64}, Float64, …})\nClosest candidates are:\n  (::Type{T})(::Real, ::RoundingMode) where T<:AbstractFloat at rounding.jl:200\n  (::Type{T})(::T) where T<:Number at boot.jl:772\n  (::Type{T})(::VectorizationBase.Double{T}) where T<:Union{Float16, Float32, Float64, VectorizationBase.Vec{<:Any, <:Union{Float16, Float32, Float64}}, VectorizationBase.VecUnroll{var\"#s36\", var\"#s35\", var\"#s34\", V} where {var\"#s36\", var\"#s35\", var\"#s34\"<:Union{Float16, Float32, Float64}, V<:Union{Bool, Float16, Float32, Float64, Int16, Int32, Int64, Int8, UInt16, UInt32, UInt64, UInt8, SIMDTypes.Bit, VectorizationBase.AbstractSIMD{var\"#s35\", var\"#s34\"}}}} at C:\\Users\\licer\\.julia\\packages\\VectorizationBase\\e4FnQ\\src\\special\\double.jl:100\n  ...\nStacktrace:\n  [1-22] ⋮ internal\n       @ Base, Optimization, ForwardDiff, OptimizationOptimJL, NLSolversBase, Optim, Unknown\n    [23] #mle#39\n       @ c:\\Users\\licer\\.julia\\dev\\ProfileLikelihood\\src\\mle.jl:15 [inlined]\n    [24] mle(::LikelihoodProblem{4, OptimizationProblem{true, OptimizationFunction{true, Optimization.AutoForwardDiff{nothing}, …}, …}, …}, ::LBFGS{Nothing, LineSearches.InitialStatic{Float64}, \n…})\n       @ ProfileLikelihood c:\\Users\\licer\\.julia\\dev\\ProfileLikelihood\\src\\mle.jl:13\n    [25] ⋮ internal\n       @ Unknown\nUse `err` to retrieve the full stack trace.","category":"page"},{"location":"lotka/","page":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","title":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","text":"The error here comes from trying to use dual numbers when we modify integrator.p with the new values for the parameters. We cannot so simply use DiffCache to get around this, we would need to somehow assign the appropriate tags when constructing the integrator (see e.g. here for some more discussion). Note also that this is not just a Optim.jl issue. With NLopt.jl, we do not error, but the optimiser does not go anywhere:","category":"page"},{"location":"lotka/","page":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","title":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","text":"using OptimizationNLopt ","category":"page"},{"location":"lotka/","page":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","title":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","text":"julia> mle(_prob, NLopt.LD_LBFGS())\nLikelihoodSolution. retcode: Failure\nMaximum likelihood: -0.0\nMaximum likelihood estimates: 4-element Vector{Float64}\n     α: 0.75\n     β: 1.23\n     a₀: 0.76\n     b₀: 0.292","category":"page"},{"location":"lotka/","page":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","title":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","text":"To get around this, we can use GeneralLazyBufferCache from PreallocationTools.jl. This is a cache that wraps around a function, creating the cache when the function is called (and reused if the same function method is used). This does make things a bit slower (in fact, automatic differentiation is slower than using finite differences or e.g. Nelder-Mead for this problem – this is just a demonstration) since the dynamic dispatch slows things down. We provide a method for constructing a LikelihoodProblem using this cache. The method requires that we first define a function that maps the arguments that would be used for constructing an integrator into a GeneralLazyBufferCache. For this problem, this function is as follows:","category":"page"},{"location":"lotka/","page":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","title":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","text":"lbc = @inline (f, u, p, tspan, ode_alg; kwargs...) -> GeneralLazyBufferCache(\n    @inline function ((cache, u₀_cache, α, β, a₀, b₀),) # Needs to be a 1-argument function\n        αβ = get_tmp(cache, α)\n        αβ[1] = α\n        αβ[2] = β\n        u₀ = get_tmp(u₀_cache, a₀)\n        u₀[1] = a₀\n        u₀[2] = b₀\n        int = construct_integrator(f, u₀, tspan, αβ, ode_alg; kwargs...)\n        return int\n    end\n)","category":"page"},{"location":"lotka/","page":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","title":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","text":"This cache argument in the inner function is why we need αβ_cache in our likelihood function. The second thing we need is a method that takes (θ, p) into the appropriate set of arguments for our GeneralLazyBufferCache. For this problem, we want to put α and β into the cache, and we should also put αβ_cache into it. This corresponds to forwarding θ[1], θ[2], and p[4] into the function, so we define ","category":"page"},{"location":"lotka/","page":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","title":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","text":"lbc_index = @inline (θ, p) -> (p[4], p[3], θ[1], θ[2], θ[3], θ[4])","category":"page"},{"location":"lotka/","page":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","title":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","text":"With these ingredients, we can now define our LikelihoodProblem. The constructor is the same as usual for an ODE problem, except with these two functions at the end of the arguments:","category":"page"},{"location":"lotka/","page":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","title":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","text":"prob = LikelihoodProblem(\n    loglik_fnc2, θ₀, ode_fnc!, u₀, tspan, lbc, lbc_index;\n    syms=syms,\n    data=(uᵒ, σ, u₀_cache, αβ_cache, n),\n    ode_parameters=[1.0, 1.0],\n    ode_kwargs=(verbose=false, saveat=t),\n    f_kwargs=(adtype=Optimization.AutoForwardDiff(),),\n    prob_kwargs=(lb=lb, ub=ub),\n    ode_alg=Rosenbrock23()\n)","category":"page"},{"location":"lotka/","page":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","title":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","text":"LikelihoodProblem. In-place: true\nθ₀: 4-element Vector{Float64}\n     α: 0.75\n     β: 1.23\n     a₀: 0.76\n     b₀: 0.292","category":"page"},{"location":"lotka/#Parameter-estimation","page":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","title":"Parameter estimation","text":"","category":"section"},{"location":"lotka/","page":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","title":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","text":"Let us now proceed as usual, computing the MLEs and obtaining the profiles. ","category":"page"},{"location":"lotka/","page":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","title":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","text":"julia> @time sol = mle(prob, NLopt.LD_LBFGS())\n  0.157597 seconds (1.34 M allocations: 57.224 MiB)\nLikelihoodSolution. retcode: Failure\nMaximum likelihood: 5.0672221211843596\nMaximum likelihood estimates: 4-element Vector{Float64}\n     α: 0.9732121347334136\n     β: 1.0887773087403383\n     a₀: 0.7775811746213865\n     b₀: 0.34360331260182864","category":"page"},{"location":"lotka/","page":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","title":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","text":"julia> @time prof = profile(prob, sol; parallel=true)\n 30.587001 seconds (413.50 M allocations: 17.053 GiB, 13.68% gc time)\nProfileLikelihoodSolution. MLE retcode: Failure\nConfidence intervals: \n     95.0% CI for α: (0.8559769794708246, 1.08492137675284)\n     95.0% CI for β: (0.9878542591871937, 1.2123035437369885)\n     95.0% CI for a₀: (0.6582428835810638, 0.9116011408658178)\n     95.0% CI for b₀: (0.24674957441638262, 0.45056076118992644)","category":"page"},{"location":"lotka/","page":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","title":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","text":"Now plotting the profiles:","category":"page"},{"location":"lotka/","page":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","title":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","text":"fig = plot_profiles(prof;\n    latex_names=[L\"\\alpha\", L\"\\beta\", L\"a_0\", L\"b_0\"],\n    show_mles=true,\n    shade_ci=true,\n    nrow=2,\n    ncol=2,\n    true_vals=[α, β, a₀, b₀])","category":"page"},{"location":"lotka/","page":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","title":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","text":"(Image: Lotka profiles)","category":"page"},{"location":"lotka/#Bivariate-profiles","page":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","title":"Bivariate profiles","text":"","category":"section"},{"location":"lotka/","page":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","title":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","text":"In all the examples thus far, we have only considered univariate profiles. We also provide a method for computing bivariate profiles through the bivariate_profile function. In this function instead of providing a set of integers for the parameters to profile, we provide tuples of integers (or symbols). Let's compute the bivariate profiles for all pairs. In the code below, resolution=25 means we define 25 layers between the MLE and the bounds for each parameter (see the implementation details section in the sidebar for a definition of a layer). Setting outer_layers=10 means that we go out 10 layers even after finding the complete confidence region.","category":"page"},{"location":"lotka/","page":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","title":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","text":"param_pairs = ((:α, :β), (:α, :a₀), (:α, :b₀),\n    (:β, :a₀), (:β, :b₀),\n    (:a₀, :b₀)) # Same as param_pairs = ((1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (3, 4))\n@time prof_2 = bivariate_profile(prob, sol, param_pairs; parallel=true, resolution=25, outer_layers=10) \n# Multithreading highly recommended for bivariate profiles - even a resolution of 25 is an upper bound of 2,601 optimisation problems for each pair (in general, this number is 4N(N+1) + 1 for a resolution of N).","category":"page"},{"location":"lotka/","page":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","title":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","text":"303.152134 seconds (6.19 G allocations: 248.989 GiB, 19.71% gc time)\nBivariateProfileLikelihoodSolution. MLE retcode: Failure\nProfile info: \n     (β, b₀): 25 layers. Bbox for 95.0% CR: [0.9651436140629203, 1.247290653464279] × [0.22398105279668468, 0.47820878016805074]\n     (α, β): 25 layers. Bbox for 95.0% CR: [0.8255352964428667, 1.1123497609332378] × [0.9652918919016166, 1.247309583537848]\n     (α, a₀): 25 layers. Bbox for 95.0% CR: [0.8257283233039527, 1.1124082018183483] × [0.6310218313830878, 0.9474704551651254]\n     (a₀, b₀): 25 layers. Bbox for 95.0% CR: [0.6309957786415554, 0.9473859257363246] × [0.22405539515039705, 0.47825405211860755]\n     (α, b₀): 25 layers. Bbox for 95.0% CR: [0.82594429486772, 1.1123826052893557] × [0.22424659978529984, 0.47806214015711407]\n     (β, a₀): 23 layers. Bbox for 95.0% CR: [0.965430158032343, 1.2470957980475892] × [0.6310333834567258, 0.9478656372614495]","category":"page"},{"location":"lotka/","page":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","title":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","text":"To plot these profiles, we can use plot_profiles. These plots usually take a bit more work than the univariate case. Let's first show a poor plot. We specify xlims and ylims to match Simpson and Maclaren (2022).","category":"page"},{"location":"lotka/","page":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","title":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","text":"fig_2 = plot_profiles(prof_2, param_pairs; # param_pairs not needed, but this ensures we get the correct order\n    latex_names=[L\"\\alpha\", L\"\\beta\", L\"a_0\", L\"b_0\"],\n    show_mles=true,\n    nrow=3,\n    ncol=2,\n    true_vals=[α, β, a₀, b₀],\n    xlim_tuples=[(0.5, 1.5), (0.5, 1.5), (0.5, 1.5), (0.7, 1.3), (0.7, 1.3), (0.5, 1.1)],\n    ylim_tuples=[(0.5, 1.5), (0.5, 1.05), (0.1, 0.5), (0.5, 1.05), (0.1, 0.5), (0.1, 0.5)],\n    fig_kwargs=(fontsize=24,))","category":"page"},{"location":"lotka/","page":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","title":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","text":"(Image: Poor Lotka bivariate profiles)","category":"page"},{"location":"lotka/","page":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","title":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","text":"In these plots, the red boundaries mark the confidence region's boundary, the red dot shows the MLE, and the black dots are the true values. There are wo issues with these plots:","category":"page"},{"location":"lotka/","page":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","title":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","text":"The plots are quite pixelated due to the low resolution.\nThe plots don't fill out the entire axis.","category":"page"},{"location":"lotka/","page":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","title":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","text":"These two issues can be resolved using the interpolant defined from the original data. Setting interpolant = true resolves these two problems. (If we also had a poor quality confidence region, you could also set smooth_confidence_boundary = true.)","category":"page"},{"location":"lotka/","page":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","title":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","text":"fig_3 = plot_profiles(prof_2, param_pairs;\n    latex_names=[L\"\\alpha\", L\"\\beta\", L\"a_0\", L\"b_0\"],\n    show_mles=true,\n    nrow=3,\n    ncol=2,\n    true_vals=[α, β, a₀, b₀],\n    interpolation=true,\n    xlim_tuples=[(0.5, 1.5), (0.5, 1.5), (0.5, 1.5), (0.7, 1.3), (0.7, 1.3), (0.5, 1.1)],\n    ylim_tuples=[(0.5, 1.5), (0.5, 1.05), (0.1, 0.5), (0.5, 1.05), (0.1, 0.5), (0.1, 0.5)],\n    fig_kwargs=(fontsize=24,))","category":"page"},{"location":"lotka/","page":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","title":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","text":"(Image: Smooth Lotka bivariate profiles)","category":"page"},{"location":"lotka/#Prediction-intervals","page":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","title":"Prediction intervals","text":"","category":"section"},{"location":"lotka/","page":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","title":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","text":"Let's now proceed with finding prediction intervals. We first find the prediction intervals using our univariate results. We use the in-place version of a prediction function:","category":"page"},{"location":"lotka/","page":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","title":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","text":"function prediction_function!(q, θ::AbstractVector{T}, data) where {T}\n    α, β, a₀, b₀ = θ\n    t, a_idx, b_idx = data\n    prob = ODEProblem(ODEFunction(ode_fnc!, syms=(:a, :b)), [a₀, b₀], extrema(t), (α, β))\n    sol = solve(prob, Rosenbrock23(), saveat=t)\n    q[a_idx] .= sol[:a]\n    q[b_idx] .= sol[:b]\n    return nothing\nend\nt_many_pts = LinRange(extrema(t)..., 1000)\na_idx = 1:1000\nb_idx = 1001:2000\npred_data = (t_many_pts, a_idx, b_idx)\nq_prototype = zeros(2000)\nindividual_intervals, union_intervals, q_vals, param_ranges =\n    get_prediction_intervals(prediction_function!, prof, pred_data; parallel=true,\n        q_prototype)","category":"page"},{"location":"lotka/","page":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","title":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","text":"Now we plot these results, plotting the individual intervals as well as the union intervals. As in Example II, we also look at the intervals from the full likelihood.","category":"page"},{"location":"lotka/","page":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","title":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","text":"# Evaluate the exact and MLE solutions\nexact_soln = zeros(2000)\nmle_soln = zeros(2000)\nprediction_function!(exact_soln, [α, β, a₀, b₀], pred_data)\nprediction_function!(mle_soln, get_mle(sol), pred_data)\n\n# Plot the parameter-wise intervals \nfig = Figure(fontsize=38, resolution=(2935.488f0, 1392.64404f0))\nalp = [['a', 'b', 'e', 'f'], ['c', 'd', 'g', 'h']]\nlatex_names = [L\"\\alpha\", L\"\\beta\", L\"a_0\", L\"b_0\"]\nfor (k, idx) in enumerate((a_idx, b_idx))\n    for i in 1:4\n        ax = Axis(fig[i < 3 ? 1 : 2, mod1(i, 2)+(k==2)*2], title=L\"(%$(alp[k][i])): Profile-wise PI for %$(latex_names[i])\",\n            titlealign=:left, width=600, height=300, xlabel=L\"t\", ylabel=k == 1 ? L\"a(t)\" : L\"b(t)\")\n        vlines!(ax, [7.0], color=:purple, linestyle=:dash, linewidth=2)\n        lines!(ax, t_many_pts, exact_soln[idx], color=:red, linewidth=3)\n        lines!(ax, t_many_pts, mle_soln[idx], color=:blue, linestyle=:dash, linewidth=3)\n        lines!(ax, t_many_pts, getindex.(individual_intervals[i], 1)[idx], color=:black, linewidth=3)\n        lines!(ax, t_many_pts, getindex.(individual_intervals[i], 2)[idx], color=:black, linewidth=3)\n        band!(ax, t_many_pts, getindex.(individual_intervals[i], 1)[idx], getindex.(individual_intervals[i], 2)[idx], color=(:grey, 0.35))\n    end\nend\n\n# Plot the union intervals\na_ax = Axis(fig[3, 1:2], title=L\"(i):$ $ Union of all intervals\",\n    titlealign=:left, width=1200, height=300, xlabel=L\"t\", ylabel=L\"a(t)\")\nb_ax = Axis(fig[3, 3:4], title=L\"(j):$ $ Union of all intervals\",\n    titlealign=:left, width=1200, height=300, xlabel=L\"t\", ylabel=L\"b(t)\")\n_ax = (a_ax, b_ax)\nfor (k, idx) in enumerate((a_idx, b_idx))\n    band!(_ax[k], t_many_pts, getindex.(union_intervals, 1)[idx], getindex.(union_intervals, 2)[idx], color=(:grey, 0.35))\n    lines!(_ax[k], t_many_pts, getindex.(union_intervals, 1)[idx], color=:black, linewidth=3)\n    lines!(_ax[k], t_many_pts, getindex.(union_intervals, 2)[idx], color=:black, linewidth=3)\n    lines!(_ax[k], t_many_pts, exact_soln[idx], color=:red, linewidth=3)\n    lines!(_ax[k], t_many_pts, mle_soln[idx], color=:blue, linestyle=:dash, linewidth=3)\n    vlines!(_ax[k], [7.0], color=:purple, linestyle=:dash, linewidth=2)\nend\n\n# Compare to the results obtained from the full likelihood\nlb = get_lower_bounds(prob)\nub = get_upper_bounds(prob)\nN = 1e5\ngrid = [[lb[i] + (ub[i] - lb[i]) * rand() for _ in 1:N] for i in 1:4]\ngrid = permutedims(reduce(hcat, grid), (2, 1))\nig = IrregularGrid(lb, ub, grid)\ngs, lik_vals = grid_search(prob, ig; parallel=Val(true), save_vals=Val(true))\nlik_vals .-= get_maximum(sol) # normalised \nfeasible_idx = findall(lik_vals .> ProfileLikelihood.get_chisq_threshold(0.95)) # values in the confidence region \nparameter_evals = grid[:, feasible_idx]\nfull_q_vals = zeros(2000, size(parameter_evals, 2))\n@views [prediction_function!(full_q_vals[:, j], parameter_evals[:, j], pred_data) for j in axes(parameter_evals, 2)]\nq_lwr = minimum(full_q_vals; dims=2) |> vec\nq_upr = maximum(full_q_vals; dims=2) |> vec\nfor (k, idx) in enumerate((a_idx, b_idx))\n    lines!(_ax[k], t_many_pts, q_lwr[idx], color=:magenta, linewidth=3)\n    lines!(_ax[k], t_many_pts, q_upr[idx], color=:magenta, linewidth=3)\nend","category":"page"},{"location":"lotka/","page":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","title":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","text":"(Image: Lotka univariate predictions)","category":"page"},{"location":"lotka/","page":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","title":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","text":"We see that the uncertainty around our predictions increases significantly for t  7, as expected since we only use data in 0 leq t leq 7 for estmiating the parameters. Moreover, the union intervals are good approximations to the intervals from the full likelihood.","category":"page"},{"location":"lotka/","page":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","title":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","text":"Now let us extend these results, instead computing prediction intervals from our bivariate profiles. The exact same function can be used for this.","category":"page"},{"location":"lotka/","page":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","title":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","text":"# Bivariate prediction intervals \nindividual_intervals, union_intervals, q_vals, param_ranges =\n    get_prediction_intervals(prediction_function!, prof_2, pred_data; parallel=true,\n        q_prototype)\n\n# Plot the intervals \nfig = Figure(fontsize=38, resolution=(2935.488f0, 1854.64404f0))\ninteger_param_pairs = ProfileLikelihood.convert_symbol_tuples(param_pairs, prof_2) # converts to the integer representation\nalp = [['a', 'b', 'e', 'f', 'i', 'j'], ['c', 'd', 'g', 'h', 'k', 'l']]\nfor (k, idx) in enumerate((a_idx, b_idx))\n    for (i, (u, v)) in enumerate(integer_param_pairs)\n        ax = Axis(fig[i < 3 ? 1 : (i < 5 ? 2 : 3), mod1(i, 2)+(k==2)*2], title=L\"(%$(alp[k][i])): Profile-wise PI for (%$(latex_names[u]), %$(latex_names[v]))\",\n            titlealign=:left, width=600, height=300, xlabel=L\"t\", ylabel=k == 1 ? L\"a(t)\" : L\"b(t)\")\n        vlines!(ax, [7.0], color=:purple, linestyle=:dash, linewidth=2)\n        lines!(ax, t_many_pts, exact_soln[idx], color=:red, linewidth=3)\n        lines!(ax, t_many_pts, mle_soln[idx], color=:blue, linestyle=:dash, linewidth=3)\n        lines!(ax, t_many_pts, getindex.(individual_intervals[(u, v)], 1)[idx], color=:black, linewidth=3)\n        lines!(ax, t_many_pts, getindex.(individual_intervals[(u, v)], 2)[idx], color=:black, linewidth=3)\n        band!(ax, t_many_pts, getindex.(individual_intervals[(u, v)], 1)[idx], getindex.(individual_intervals[(u, v)], 2)[idx], color=(:grey, 0.35))\n    end\nend\na_ax = Axis(fig[4, 1:2], title=L\"(i):$ $ Union of all intervals\",\n    titlealign=:left, width=1200, height=300, xlabel=L\"t\", ylabel=L\"a(t)\")\nb_ax = Axis(fig[4, 3:4], title=L\"(j):$ $ Union of all intervals\",\n    titlealign=:left, width=1200, height=300, xlabel=L\"t\", ylabel=L\"b(t)\")\n_ax = (a_ax, b_ax)\nfor (k, idx) in enumerate((a_idx, b_idx))\n    band!(_ax[k], t_many_pts, getindex.(union_intervals, 1)[idx], getindex.(union_intervals, 2)[idx], color=(:grey, 0.35))\n    lines!(_ax[k], t_many_pts, getindex.(union_intervals, 1)[idx], color=:black, linewidth=3)\n    lines!(_ax[k], t_many_pts, getindex.(union_intervals, 2)[idx], color=:black, linewidth=3)\n    lines!(_ax[k], t_many_pts, exact_soln[idx], color=:red, linewidth=3)\n    lines!(_ax[k], t_many_pts, mle_soln[idx], color=:blue, linestyle=:dash, linewidth=3)\n    vlines!(_ax[k], [7.0], color=:purple, linestyle=:dash, linewidth=2)\nend\nfor (k, idx) in enumerate((a_idx, b_idx))\n    lines!(_ax[k], t_many_pts, q_lwr[idx], color=:magenta, linewidth=3)\n    lines!(_ax[k], t_many_pts, q_upr[idx], color=:magenta, linewidth=3)\nend","category":"page"},{"location":"lotka/","page":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","title":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","text":"(Image: Lotka bivariate predictions)","category":"page"},{"location":"lotka/#Just-the-code","page":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","title":"Just the code","text":"","category":"section"},{"location":"lotka/","page":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","title":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","text":"Here is all the code used for obtaining the results in this example, should you want a version that you can directly copy and paste.","category":"page"},{"location":"lotka/","page":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","title":"Example V: Lotka-Volterra ODE, GeneralLazyBufferCache, and computing bivarate profile likelihoods","text":"## Step 1: Generate the data and define the likelihood\nusing OrdinaryDiffEq, Random\nα = 0.9\nβ = 1.1\na₀ = 0.8\nb₀ = 0.3\nσ = 0.2\nt = LinRange(0, 10, 21)\n@inline function ode_fnc!(du, u, p, t) where {T}\n    α, β = p\n    a, b = u\n    du[1] = α * a - a * b\n    du[2] = β * a * b - b\n    return nothing\nend\n# Initial data is obtained by solving the ODE \ntspan = extrema(t)\np = [α, β]\nu₀ = [a₀, b₀]\nprob = ODEProblem(ode_fnc!, u₀, tspan, p)\nsol = solve(prob, Rosenbrock23(), saveat=t)\nRandom.seed!(252800)\nnoise_vec = [σ * randn(2) for _ in eachindex(t)]\nuᵒ = sol.u .+ noise_vec\n@inline function loglik_fnc2(θ::AbstractVector{T}, data, integrator) where {T}\n    α, β, a₀, b₀ = θ\n    uᵒ, σ, u₀_cache, αβ_cache, n = data\n    u₀ = get_tmp(u₀_cache, θ)\n    integrator.p[1] = α\n    integrator.p[2] = β\n    u₀[1] = a₀\n    u₀[2] = b₀\n    reinit!(integrator, u₀)\n    solve!(integrator)\n    ℓ = zero(T)\n    for i in 1:n\n        âᵒ = integrator.sol.u[i][1]\n        b̂ᵒ = integrator.sol.u[i][2]\n        aᵒ = uᵒ[i][1]\n        bᵒ = uᵒ[i][2]\n        ℓ = ℓ - 0.5log(2π * σ^2) - 0.5(âᵒ - aᵒ)^2 / σ^2\n        ℓ = ℓ - 0.5log(2π * σ^2) - 0.5(b̂ᵒ - bᵒ)^2 / σ^2\n    end\n    return ℓ\nend\n\n## Step 2: Define the problem \nusing PreallocationTools, Optimization\nlb = [0.7, 0.7, 0.5, 0.1]\nub = [1.2, 1.4, 1.2, 0.5]\nθ₀ = [0.75, 1.23, 0.76, 0.292]\nsyms = [:α, :β, :a₀, :b₀]\nu₀_cache = DiffCache(zeros(2), 12)\nαβ_cache = DiffCache(zeros(2), 12)\nn = findlast(t .≤ 7) # Using t ≤ 7 for estimation\nlbc = @inline (f, u, p, tspan, ode_alg; kwargs...) -> GeneralLazyBufferCache(\n    @inline function ((cache, α, β),) # Needs to be a 1-argument function\n        αβ = get_tmp(cache, α)\n        αβ[1] = α\n        αβ[2] = β\n        int = construct_integrator(f, u₀, tspan, αβ, ode_alg; kwargs...)\n        return int\n    end\n)\nlbc_index = @inline (θ, p) -> (p[4], θ[1], θ[2])\nprob = LikelihoodProblem(\n    loglik_fnc2, θ₀, ode_fnc!, u₀, tspan, lbc, lbc_index;\n    syms=syms,\n    data=(uᵒ, σ, u₀_cache, αβ_cache, n),\n    ode_parameters=[1.0, 1.0],\n    ode_kwargs=(verbose=false, saveat=t),\n    f_kwargs=(adtype=Optimization.AutoForwardDiff(),),\n    prob_kwargs=(lb=lb, ub=ub),\n    ode_alg=Rosenbrock23()\n)\n\n## Step 3: Compute the MLE \nusing OptimizationNLopt \nsol = mle(prob, NLopt.LD_LBFGS())\n\n## Step 4: Profile\nprof = profile(prob, sol; parallel=true)\n\n## Step 5: Visualise \nusing CairoMakie, LaTeXStrings \nfig = plot_profiles(prof;\n    latex_names=[L\"\\alpha\", L\"\\beta\", L\"a_0\", L\"b_0\"],\n    show_mles=true,\n    shade_ci=true,\n    nrow=2,\n    ncol=2,\n    true_vals=[α, β, a₀, b₀])\n\n## Step 6: Obtain the bivariate profiles \nparam_pairs = ((:α, :β), (:α, :a₀), (:α, :b₀),\n    (:β, :a₀), (:β, :b₀),\n    (:a₀, :b₀))\nprof_2 = bivariate_profile(prob, sol, param_pairs; parallel=true, resolution=25, outer_layers=10)\n\n## Step 7: Visualise \nusing CairoMakie, LaTeXStrings\nfig_3 = plot_profiles(prof_2, param_pairs;\n    latex_names=[L\"\\alpha\", L\"\\beta\", L\"a_0\", L\"b_0\"],\n    show_mles=true,\n    nrow=3,\n    ncol=2,\n    true_vals=[α, β, a₀, b₀],\n    interpolation=true,\n    xlim_tuples=[(0.5, 1.5), (0.5, 1.5), (0.5, 1.5), (0.7, 1.3), (0.7, 1.3), (0.5, 1.1)],\n    ylim_tuples=[(0.5, 1.5), (0.5, 1.05), (0.1, 0.5), (0.5, 1.05), (0.1, 0.5), (0.1, 0.5)],\n    fig_kwargs=(fontsize=24,))\n\n## Step 8: Get the prediction intervals from the univariate profiles \nfunction prediction_function!(q, θ::AbstractVector{T}, data) where {T}\n    α, β, a₀, b₀ = θ\n    t, a_idx, b_idx = data\n    prob = ODEProblem(ODEFunction(ode_fnc!, syms=(:a, :b)), [a₀, b₀], extrema(t), (α, β))\n    sol = solve(prob, Rosenbrock23(), saveat=t)\n    q[a_idx] .= sol[:a]\n    q[b_idx] .= sol[:b]\n    return nothing\nend\nt_many_pts = LinRange(extrema(t)..., 1000)\na_idx = 1:1000\nb_idx = 1001:2000\npred_data = (t_many_pts, a_idx, b_idx)\nq_prototype = zeros(2000)\nindividual_intervals, union_intervals, q_vals, param_ranges =\n    get_prediction_intervals(prediction_function!, prof, pred_data; parallel=true,\n        q_prototype)\n\n## Step 9: Visualise\n# Evaluate the exact and MLE solutions\nexact_soln = zeros(2000)\nmle_soln = zeros(2000)\nprediction_function!(exact_soln, [α, β, a₀, b₀], pred_data)\nprediction_function!(mle_soln, get_mle(sol), pred_data)\n\n# Plot the parameter-wise intervals \nfig = Figure(fontsize=38, resolution=(2935.488f0, 1392.64404f0))\nalp = [['a', 'b', 'e', 'f'], ['c', 'd', 'g', 'h']]\nlatex_names = [L\"\\alpha\", L\"\\beta\", L\"a_0\", L\"b_0\"]\nfor (k, idx) in enumerate((a_idx, b_idx))\n    for i in 1:4\n        ax = Axis(fig[i < 3 ? 1 : 2, mod1(i, 2)+(k==2)*2], title=L\"(%$(alp[k][i])): Profile-wise PI for %$(latex_names[i])\",\n            titlealign=:left, width=600, height=300, xlabel=L\"t\", ylabel=k == 1 ? L\"a(t)\" : L\"b(t)\")\n        vlines!(ax, [7.0], color=:purple, linestyle=:dash, linewidth=2)\n        lines!(ax, t_many_pts, exact_soln[idx], color=:red, linewidth=3)\n        lines!(ax, t_many_pts, mle_soln[idx], color=:blue, linestyle=:dash, linewidth=3)\n        lines!(ax, t_many_pts, getindex.(individual_intervals[i], 1)[idx], color=:black, linewidth=3)\n        lines!(ax, t_many_pts, getindex.(individual_intervals[i], 2)[idx], color=:black, linewidth=3)\n        band!(ax, t_many_pts, getindex.(individual_intervals[i], 1)[idx], getindex.(individual_intervals[i], 2)[idx], color=(:grey, 0.35))\n    end\nend\n\n# Plot the union intervals\na_ax = Axis(fig[3, 1:2], title=L\"(i):$ $ Union of all intervals\",\n    titlealign=:left, width=1200, height=300, xlabel=L\"t\", ylabel=L\"a(t)\")\nb_ax = Axis(fig[3, 3:4], title=L\"(j):$ $ Union of all intervals\",\n    titlealign=:left, width=1200, height=300, xlabel=L\"t\", ylabel=L\"b(t)\")\n_ax = (a_ax, b_ax)\nfor (k, idx) in enumerate((a_idx, b_idx))\n    band!(_ax[k], t_many_pts, getindex.(union_intervals, 1)[idx], getindex.(union_intervals, 2)[idx], color=(:grey, 0.35))\n    lines!(_ax[k], t_many_pts, getindex.(union_intervals, 1)[idx], color=:black, linewidth=3)\n    lines!(_ax[k], t_many_pts, getindex.(union_intervals, 2)[idx], color=:black, linewidth=3)\n    lines!(_ax[k], t_many_pts, exact_soln[idx], color=:red, linewidth=3)\n    lines!(_ax[k], t_many_pts, mle_soln[idx], color=:blue, linestyle=:dash, linewidth=3)\n    vlines!(_ax[k], [7.0], color=:purple, linestyle=:dash, linewidth=2)\nend\n\n# Compare to the results obtained from the full likelihood\nlb = get_lower_bounds(prob)\nub = get_upper_bounds(prob)\nN = 1e5\ngrid = [[lb[i] + (ub[i] - lb[i]) * rand() for _ in 1:N] for i in 1:4]\ngrid = permutedims(reduce(hcat, grid), (2, 1))\nig = IrregularGrid(lb, ub, grid)\ngs, lik_vals = grid_search(prob, ig; parallel=Val(true), save_vals=Val(true))\nlik_vals .-= get_maximum(sol) # normalised \nfeasible_idx = findall(lik_vals .> ProfileLikelihood.get_chisq_threshold(0.95)) # values in the confidence region \nparameter_evals = grid[:, feasible_idx]\nfull_q_vals = zeros(2000, size(parameter_evals, 2))\n@views [prediction_function!(full_q_vals[:, j], parameter_evals[:, j], pred_data) for j in axes(parameter_evals, 2)]\nq_lwr = minimum(full_q_vals; dims=2) |> vec\nq_upr = maximum(full_q_vals; dims=2) |> vec\nfor (k, idx) in enumerate((a_idx, b_idx))\n    lines!(_ax[k], t_many_pts, q_lwr[idx], color=:magenta, linewidth=3)\n    lines!(_ax[k], t_many_pts, q_upr[idx], color=:magenta, linewidth=3)\nend\n\n## Step 10: Get the prediction intervals from the bivariate profiles \nindividual_intervals, union_intervals, q_vals, param_ranges =\n    get_prediction_intervals(prediction_function!, prof_2, pred_data; parallel=true,\n        q_prototype)\n\n## Step 11: Visualise the prediction intervals \nfig = Figure(fontsize=38, resolution=(2935.488f0, 1854.64404f0))\ninteger_param_pairs = ProfileLikelihood.convert_symbol_tuples(param_pairs, prof_2) # converts to the integer representation\nalp = [['a', 'b', 'e', 'f', 'i', 'j'], ['c', 'd', 'g', 'h', 'k', 'l']]\nfor (k, idx) in enumerate((a_idx, b_idx))\n    for (i, (u, v)) in enumerate(integer_param_pairs)\n        ax = Axis(fig[i < 3 ? 1 : (i < 5 ? 2 : 3), mod1(i, 2)+(k==2)*2], title=L\"(%$(alp[k][i])): Profile-wise PI for (%$(latex_names[u]), %$(latex_names[v]))\",\n            titlealign=:left, width=600, height=300, xlabel=L\"t\", ylabel=k == 1 ? L\"a(t)\" : L\"b(t)\")\n        vlines!(ax, [7.0], color=:purple, linestyle=:dash, linewidth=2)\n        lines!(ax, t_many_pts, exact_soln[idx], color=:red, linewidth=3)\n        lines!(ax, t_many_pts, mle_soln[idx], color=:blue, linestyle=:dash, linewidth=3)\n        lines!(ax, t_many_pts, getindex.(individual_intervals[(u, v)], 1)[idx], color=:black, linewidth=3)\n        lines!(ax, t_many_pts, getindex.(individual_intervals[(u, v)], 2)[idx], color=:black, linewidth=3)\n        band!(ax, t_many_pts, getindex.(individual_intervals[(u, v)], 1)[idx], getindex.(individual_intervals[(u, v)], 2)[idx], color=(:grey, 0.35))\n    end\nend\na_ax = Axis(fig[4, 1:2], title=L\"(i):$ $ Union of all intervals\",\n    titlealign=:left, width=1200, height=300, xlabel=L\"t\", ylabel=L\"a(t)\")\nb_ax = Axis(fig[4, 3:4], title=L\"(j):$ $ Union of all intervals\",\n    titlealign=:left, width=1200, height=300, xlabel=L\"t\", ylabel=L\"b(t)\")\n_ax = (a_ax, b_ax)\nfor (k, idx) in enumerate((a_idx, b_idx))\n    band!(_ax[k], t_many_pts, getindex.(union_intervals, 1)[idx], getindex.(union_intervals, 2)[idx], color=(:grey, 0.35))\n    lines!(_ax[k], t_many_pts, getindex.(union_intervals, 1)[idx], color=:black, linewidth=3)\n    lines!(_ax[k], t_many_pts, getindex.(union_intervals, 2)[idx], color=:black, linewidth=3)\n    lines!(_ax[k], t_many_pts, exact_soln[idx], color=:red, linewidth=3)\n    lines!(_ax[k], t_many_pts, mle_soln[idx], color=:blue, linestyle=:dash, linewidth=3)\n    vlines!(_ax[k], [7.0], color=:purple, linestyle=:dash, linewidth=2)\nend\nfor (k, idx) in enumerate((a_idx, b_idx))\n    lines!(_ax[k], t_many_pts, q_lwr[idx], color=:magenta, linewidth=3)\n    lines!(_ax[k], t_many_pts, q_upr[idx], color=:magenta, linewidth=3)\nend","category":"page"},{"location":"regression/#Example-I:-Multiple-linear-regression","page":"Example I: Multiple linear regression","title":"Example I: Multiple linear regression","text":"","category":"section"},{"location":"regression/","page":"Example I: Multiple linear regression","title":"Example I: Multiple linear regression","text":"Let us start with a linear regression example. First, load the packages needed:","category":"page"},{"location":"regression/","page":"Example I: Multiple linear regression","title":"Example I: Multiple linear regression","text":"using ProfileLikelihood\nusing Random \nusing PreallocationTools \nusing Distributions \nusing CairoMakie \nusing LaTeXStrings \nusing LinearAlgebra\nusing Optimization \nusing OptimizationOptimJL","category":"page"},{"location":"regression/","page":"Example I: Multiple linear regression","title":"Example I: Multiple linear regression","text":"We perform a simulation study where we try and estimate the parameters in a regression of the form ","category":"page"},{"location":"regression/","page":"Example I: Multiple linear regression","title":"Example I: Multiple linear regression","text":"y_i = beta_0 + beta_1x_1i + beta_2x_2i + beta_3x_1ix_3i + varepsilon_i quad varepsilon_i sim mathcal N(0 sigma^2) quad i=12ldots n ","category":"page"},{"location":"regression/","page":"Example I: Multiple linear regression","title":"Example I: Multiple linear regression","text":"We also try and estimate sigma. ","category":"page"},{"location":"regression/#Setting-up-the-problem","page":"Example I: Multiple linear regression","title":"Setting up the problem","text":"","category":"section"},{"location":"regression/","page":"Example I: Multiple linear regression","title":"Example I: Multiple linear regression","text":"Let us start by simulating the data:","category":"page"},{"location":"regression/","page":"Example I: Multiple linear regression","title":"Example I: Multiple linear regression","text":"using Random, Distributions \nRandom.seed!(98871) \nn = 600\nβ = [-1.0, 1.0, 0.5, 3.0]\nσ = 0.05\nx₁ = rand(Uniform(-1, 1), n)\nx₂ = rand(Normal(1.0, 0.5), n)\nX = hcat(ones(n), x₁, x₂, x₁ .* x₂)\nε = rand(Normal(0.0, σ), n)\ny = X * β + ε","category":"page"},{"location":"regression/","page":"Example I: Multiple linear regression","title":"Example I: Multiple linear regression","text":"The data y is now our noisy data. The likelihood function in this example is ","category":"page"},{"location":"regression/","page":"Example I: Multiple linear regression","title":"Example I: Multiple linear regression","text":"ell(sigma boldsymbol beta mid boldsymbol y) = -(n2)log(2mathrmpisigma^2) - (12sigma^2)sum_i (y_i - beta_0 - beta_1x_1i - beta_2x_2i - beta_3x_1ix_2i)^2 ","category":"page"},{"location":"regression/","page":"Example I: Multiple linear regression","title":"Example I: Multiple linear regression","text":"We now define our likelihood function. To allow for automatic differentiation, we use PreallocationTools.DiffCache to define our cache vectors.","category":"page"},{"location":"regression/","page":"Example I: Multiple linear regression","title":"Example I: Multiple linear regression","text":"sse = DiffCache(zeros(n))\nβ_cache = DiffCache(similar(β), 10)\ndat = (y, X, sse, n, β_cache)\n@inline function loglik_fnc(θ, data)\n    σ, β₀, β₁, β₂, β₃ = θ\n    y, X, sse, n, β = data\n    _sse = get_tmp(sse, θ)\n    _β = get_tmp(β, θ)\n    _β[1] = β₀\n    _β[2] = β₁\n    _β[3] = β₂\n    _β[4] = β₃\n    ℓℓ = -0.5n * log(2π * σ^2)\n    mul!(_sse, X, _β)\n    for i in eachindex(y)\n        ℓℓ = ℓℓ - 0.5 / σ^2 * (y[i] - _sse[i])^2\n    end\n    return ℓℓ\nend","category":"page"},{"location":"regression/","page":"Example I: Multiple linear regression","title":"Example I: Multiple linear regression","text":"Now having defined our likelihood, we can define the likelihood problem. We let the problem be unconstrained, except for sigma  0. We start at the value 1 for each parameter. To use automatic differentiation, we use Optimization.AutoForwardDiff for the adtype.","category":"page"},{"location":"regression/","page":"Example I: Multiple linear regression","title":"Example I: Multiple linear regression","text":"using Optimization\nθ₀ = ones(5)\nprob = LikelihoodProblem(loglik_fnc, θ₀;\n    data=dat,\n    f_kwargs=(adtype=Optimization.AutoForwardDiff(),),\n    prob_kwargs=(\n        lb=[0.0, -Inf, -Inf, -Inf, -Inf],\n        ub=Inf * ones(5)\n    ),\n    syms=[:σ, :β₀, :β₁, :β₂, :β₃]\n)\nLikelihoodProblem. In-place: true\nθ₀: 5-element Vector{Float64}\n     σ: 1.0\n     β₀: 1.0\n     β₁: 1.0\n     β₂: 1.0\n     β₃: 1.0","category":"page"},{"location":"regression/#Finding-the-MLEs","page":"Example I: Multiple linear regression","title":"Finding the MLEs","text":"","category":"section"},{"location":"regression/","page":"Example I: Multiple linear regression","title":"Example I: Multiple linear regression","text":"Now we can compute the MLEs.","category":"page"},{"location":"regression/","page":"Example I: Multiple linear regression","title":"Example I: Multiple linear regression","text":"using OptimizationOptimJL\nsol = mle(prob, Optim.LBFGS())\nLikelihoodSolution. retcode: Success\nMaximum likelihood: 957.6376683220673\nMaximum likelihood estimates: 5-element Vector{Float64}\n     σ: 0.049045771053511954\n     β₀: -1.0041730424101303\n     β₁: 1.006051999753723\n     β₂: 0.5041343138021581\n     β₃: 2.9922041467801934","category":"page"},{"location":"regression/","page":"Example I: Multiple linear regression","title":"Example I: Multiple linear regression","text":"We can compare these MLEs to the true MLES hatbeta = (boldsymbol X^mathsf Tboldsymbol X)^-1boldsymbol X^mathsf Tboldsymbol y and hatsigma^2 = (1n_d)(boldsymbol y - boldsymbol Xboldsymbol beta)^mathsf T(boldsymbol y - boldsymbol Xboldsymbol beta), where n_d is the degrees of freedom, as follows (note the indexing):","category":"page"},{"location":"regression/","page":"Example I: Multiple linear regression","title":"Example I: Multiple linear regression","text":"using Test, LinearAlgebra\ndf = n - (length(β) + 1)\nresids = y .- X * sol[2:5]\n@test sol[2:5] ≈ inv(X' * X) * X' * y # sol[i] = sol.mle[i] \n@test sol[:σ]^2 ≈ 1 / df * sum(resids .^ 2) atol = 1e-4 # symbol indexing","category":"page"},{"location":"regression/#Profiling","page":"Example I: Multiple linear regression","title":"Profiling","text":"","category":"section"},{"location":"regression/","page":"Example I: Multiple linear regression","title":"Example I: Multiple linear regression","text":"We can now profile the results. In this case, since the problem has no bounds for some parameters we need to manually define the parameter bounds used for profiling. The function construct_profile_ranges is used for this. Note that we use parallel = true below to allow for multithreading, allowing multiple parameters to be profiled at the same time.","category":"page"},{"location":"regression/","page":"Example I: Multiple linear regression","title":"Example I: Multiple linear regression","text":"lb = [1e-12, -5.0, -5.0, -5.0, -5.0]\nub = [15.0, 15.0, 15.0, 15.0, 15.0]\nresolutions = [600, 200, 200, 200, 200] # use many points for σ\nparam_ranges = construct_profile_ranges(sol, lb, ub, resolutions)\nprof = profile(prob, sol; param_ranges, parallel=true)\nProfileLikelihoodSolution. MLE retcode: Success\nConfidence intervals: \n     95.0% CI for σ: (0.04639652142575396, 0.05196200098682017)\n     95.0% CI for β₀: (-1.013328678265197, -0.9950163004240635)\n     95.0% CI for β₁: (0.9906172772152076, 1.0214865014037124)\n     95.0% CI for β₂: (0.4960199617761395, 0.5122490969333844)\n     95.0% CI for β₃: (2.978618197988093, 3.0057902255444136)","category":"page"},{"location":"regression/","page":"Example I: Multiple linear regression","title":"Example I: Multiple linear regression","text":"These confidence intervals can be compared to the true confidence intervals as follows, noting that the variance-covariance matrix for the beta_i coefficients is boldsymbolSigma = sigma^2(boldsymbol X^mathsf Tboldsymbol X)^-1 so that their confidence interval is hatbeta_i pm 196sqrtboldsymbolSigma_ii. Additionally, a confidence interval for sigma is sqrt(boldsymbol y - boldsymbol Xboldsymbol beta)^mathsf T(boldsymbol y - boldsymbol Xboldsymbol beta)(1sqrtchi_0975 n_d 1sqrtchi_0025 n_d).","category":"page"},{"location":"regression/","page":"Example I: Multiple linear regression","title":"Example I: Multiple linear regression","text":"vcov_mat = sol[:σ]^2 * inv(X' * X)\nfor i in 1:4\n    @test prof.confidence_intervals[i+1][1] ≈ sol.mle[i+1] - 1.96sqrt(vcov_mat[i, i]) atol = 1e-3\n    @test prof.confidence_intervals[i+1][2] ≈ sol.mle[i+1] + 1.96sqrt(vcov_mat[i, i]) atol = 1e-3\nend\nrss = sum(resids .^ 2)\nχ²_up = quantile(Chisq(df), 0.975)\nχ²_lo = quantile(Chisq(df), 0.025)\nσ_CI_exact = sqrt.(rss ./ (χ²_up, χ²_lo))\n@test get_confidence_intervals(prof, :σ).lower ≈ σ_CI_exact[1] atol = 1e-3\n@test ProfileLikelihood.get_upper(get_confidence_intervals(prof, :σ)) ≈ σ_CI_exact[2] atol = 1e-3","category":"page"},{"location":"regression/","page":"Example I: Multiple linear regression","title":"Example I: Multiple linear regression","text":"You can use prof to view a single parameter's results, e.g.","category":"page"},{"location":"regression/","page":"Example I: Multiple linear regression","title":"Example I: Multiple linear regression","text":"prof[:β₂]\nProfile likelihood for parameter β₂. MLE retcode: Success\nMLE: 0.5041343138021581\n95.0% CI for β₂: (0.4960199617761438, 0.5122490969333844)","category":"page"},{"location":"regression/","page":"Example I: Multiple linear regression","title":"Example I: Multiple linear regression","text":"You can also evaluate the profile at a point inside its confidence interval. (If you want to evaluate outside the confidence interval, you need to use a non-Throw extrap in the profile function's keyword argument [see also Interpolations.jl].) The following are all the same, evaluating the profile for beta_2 at beta_2=05:","category":"page"},{"location":"regression/","page":"Example I: Multiple linear regression","title":"Example I: Multiple linear regression","text":"prof[:β₂](0.50)\nprof(0.50, :β₂)\nprof(0.50, 4)","category":"page"},{"location":"regression/#Visualisation","page":"Example I: Multiple linear regression","title":"Visualisation","text":"","category":"section"},{"location":"regression/","page":"Example I: Multiple linear regression","title":"Example I: Multiple linear regression","text":"We can now also visualise the results. In the plot below, the red line is at the threshold for the confidence region, so that the parameters between these values define the confidence interval. The red lines are at the MLEs, and the black lines are at the true values. ","category":"page"},{"location":"regression/","page":"Example I: Multiple linear regression","title":"Example I: Multiple linear regression","text":"using CairoMakie, LaTeXStrings\nfig = plot_profiles(prof;\n    latex_names=[L\"\\sigma\", L\"\\beta_0\", L\"\\beta_1\", L\"\\beta_2\", L\"\\beta_3\"], # default names would be of the form θᵢ\n    show_mles=true,\n    shade_ci=true,\n    true_vals=[σ, β...],\n    fig_kwargs=(fontsize=30, resolution=(2134.0f0, 906.0f0)),\n    axis_kwargs=(width=600, height=300))\nxlims!(fig.content[1], 0.045, 0.055) # fix the ranges\nxlims!(fig.content[2], -1.025, -0.975)\nxlims!(fig.content[4], 0.475, 0.525)","category":"page"},{"location":"regression/","page":"Example I: Multiple linear regression","title":"Example I: Multiple linear regression","text":"(Image: Regression profiles)","category":"page"},{"location":"regression/","page":"Example I: Multiple linear regression","title":"Example I: Multiple linear regression","text":"You could also plot individual or specific parameters:","category":"page"},{"location":"regression/","page":"Example I: Multiple linear regression","title":"Example I: Multiple linear regression","text":"plot_profiles(prof, [1, 3]) # plot σ and β₁\nplot_profiles(prof, [:σ, :β₁, :β₃]) # can use symbols \nplot_profiles(prof, 1) # can just provide an integer \nplot_profiles(prof, :β₂) # symbols work","category":"page"},{"location":"regression/#Just-the-code","page":"Example I: Multiple linear regression","title":"Just the code","text":"","category":"section"},{"location":"regression/","page":"Example I: Multiple linear regression","title":"Example I: Multiple linear regression","text":"Here is all the code used for obtaining the results in this example, should you want a version that you can directly copy and paste.","category":"page"},{"location":"regression/","page":"Example I: Multiple linear regression","title":"Example I: Multiple linear regression","text":"using Random, Distributions, PreallocationTools, LinearAlgebra\n## Step 1: Generate some data for the problem and define the likelihood\nRandom.seed!(98871)\nn = 600\nβ = [-1.0, 1.0, 0.5, 3.0]\nσ = 0.05\nx₁ = rand(Uniform(-1, 1), n)\nx₂ = rand(Normal(1.0, 0.5), n)\nX = hcat(ones(n), x₁, x₂, x₁ .* x₂)\nε = rand(Normal(0.0, σ), n)\ny = X * β + ε\nsse = DiffCache(zeros(n))\nβ_cache = DiffCache(similar(β), 10)\ndat = (y, X, sse, n, β_cache)\n@inline function loglik_fnc(θ, data)\n    σ, β₀, β₁, β₂, β₃ = θ\n    y, X, sse, n, β = data\n    _sse = get_tmp(sse, θ)\n    _β = get_tmp(β, θ)\n    _β[1] = β₀\n    _β[2] = β₁\n    _β[3] = β₂\n    _β[4] = β₃\n    ℓℓ = -0.5n * log(2π * σ^2)\n    mul!(_sse, X, _β)\n    for i in eachindex(y)\n        ℓℓ = ℓℓ - 0.5 / σ^2 * (y[i] - _sse[i])^2\n    end\n    return ℓℓ\nend\n\n## Step 2: Define the problem \nusing Optimization\nθ₀ = ones(5)\nprob = LikelihoodProblem(loglik_fnc, θ₀;\n    data=dat,\n    f_kwargs=(adtype=Optimization.AutoForwardDiff(),),\n    prob_kwargs=(\n        lb=[0.0, -Inf, -Inf, -Inf, -Inf],\n        ub=Inf * ones(5)\n    ),\n    syms=[:σ, :β₀, :β₁, :β₂, :β₃]\n)\n\n## Step 3: Compute the MLE\nusing OptimizationOptimJL\nsol = mle(prob, Optim.LBFGS())\n\n## Step 4: Profile \nlb = [1e-12, -5.0, -5.0, -5.0, -5.0]\nub = [15.0, 15.0, 15.0, 15.0, 15.0]\nresolutions = [600, 200, 200, 200, 200] # use many points for σ\nparam_ranges = construct_profile_ranges(sol, lb, ub, resolutions)\nprof = profile(prob, sol; param_ranges, parallel=true)\n\n## Step 5: Visualise \nusing CairoMakie, LaTeXStrings\nfig = plot_profiles(prof;\n    latex_names=[L\"\\sigma\", L\"\\beta_0\", L\"\\beta_1\", L\"\\beta_2\", L\"\\beta_3\"], # default names would be of the form θᵢ\n    show_mles=true,\n    shade_ci=true,\n    true_vals=[σ, β...],\n    fig_kwargs=(fontsize=30, resolution=(2134.0f0, 906.0f0)),\n    axis_kwargs=(width=600, height=300))\nxlims!(fig.content[1], 0.045, 0.055) # fix the ranges\nxlims!(fig.content[2], -1.025, -0.975)\nxlims!(fig.content[4], 0.475, 0.525)","category":"page"},{"location":"heat/#Example-IV:-Diffusion-equation-on-a-square-plate","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"","category":"section"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"Warning: Much of the code in this example takes a very long time, e.g. the MLEs take just under an hour. The total runtime is around six hours on my machine (mostly coming from the mesh for the PDE being very dense). ","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"The packages we use in this example are:","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"using FiniteVolumeMethod \nusing ProfileLikelihood \nusing DelaunayTriangulation\nusing Random \nusing LinearSolve \nusing OrdinaryDiffEq\nusing CairoMakie \nusing LaTeXStrings\nusing StaticArraysCore\nusing Optimization \nusing OptimizationNLopt","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"Let us now consider the problem of estimating parameters defining a diffusion equation on a square plate. In particular, consider ","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"beginequation*\nbeginarrayrcll\ndisplaystyle\nfracpartial u(x y t)partial t = dfrac1kboldsymbolnabla^2 u(x y t)  (x y) in Omegat0 \nu(x y t) =  0  (x y) in partial Omegat0 \nu(x y 0) =  u_0mathbbI(y leq c) (xy)inOmega\nendarray\nendequation*","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"where Omega = 0 2^2. This problem extends the corresponding example given in FiniteVolumeMethod.jl, namely this example, and so not all the code used in defining this PDE will be explained here; refer to the FiniteVolumeMethod.jl documentation. We will take the true values k = 9, c = 1, u_0 = 50, and let the standard deviation of the noise, sigma, in the data be 01. We are interested in recovering (k c u_0); we do not consider estimating sigma here, estimating it leads to identifiability issues that distract from the main point of our example here, i.e. to just show how to setup a problem.","category":"page"},{"location":"heat/#Building-the-FVMProblem","page":"Example IV: Diffusion equation on a square plate","title":"Building the FVMProblem","text":"","category":"section"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"Let us start by defining the PDE problem, and then we will discuss profiling.","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"using FiniteVolumeMethod, DelaunayTriangulation, LinearSolve\na, b, c, d = 0.0, 2.0, 0.0, 2.0\nr = 0.022\nGMSH_PATH = \"./gmsh-4.9.4-Windows64/gmsh.exe\" # set this to whatever your path is\ntri = generate_mesh(x, y, r; gmsh_path=GMSH_PATH)\nmesh = FVMGeometry(tri)\nbc = ((x, y, t, u::T, p) where {T}) -> zero(T)\ntype = :D\nBCs = BoundaryConditions(mesh, bc, type)\nc = 1.0\nu₀ = 50.0\nf = (x, y) -> y ≤ c ? u₀ : 0.0\nD = (x, y, t, u, p) -> p[1]\nflux = (q, x, y, t, α, β, γ, p) -> (q[1] = -α / p[1]; q[2] = -β / p[1])\nR = ((x, y, t, u::T, p) where {T}) -> zero(T)\npoints = get_points(tri)\ninitc = @views f.(points[1, :], points[2, :])\niip_flux = true\nfinal_time = 0.1\nk = [9.0]\nprob = FVMProblem(mesh, BCs; iip_flux,\n    flux_function=flux, reaction_function=R,\n    initial_condition=initc, final_time,\n    flux_parameters=k)","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"Our problem has now been defined. Notice that we wrap k in a vector so that we can easily mutate the flux_parameters field of prob; if k were a scalar, we could not mutate it.","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"Now let's generate some data. We start by solving the PDE.","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"alg = TRBDF2(linsolve=KLUFactorization(; reuse_symbolic=false))\nsol = solve(prob, alg; specialization=SciMLBase.FullSpecialize, saveat=0.01)","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"(We use reuse_symbolic=false due to https://github.com/JuliaSparse/KLU.jl/issues/12 causing issues with multithreading later.) ","category":"page"},{"location":"heat/#Defining-a-summary-statistic","page":"Example IV: Diffusion equation on a square plate","title":"Defining a summary statistic","text":"","category":"section"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"Now, one complication with a PDE compared to the scalar ODE cases that we considered previously is that we have data at (x_i y_j t_k) for many indices (i j k). Rather than defining our objective function in terms of these data points, we will instead use a summary statistic. The summary statistic we use in this example is the average density,","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"tilde M(t) = frac1mathrmArea(Omega)iint_Omega u(x y t)mathrmdA ","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"We need to be able to compute this integral efficiently and accurately. For this, recall that the finite volume method discretises the domain into triangles. If mathcal T is this set of triangles, then ","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"tilde M(t) = frac1mathrmArea(Omega)sum_T_k in mathcal T iint_T_k u(x y t)mathrmdA ","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"Then, recall that u is represented as a linear function alpha_k x + beta_k y + gamma_k inside the triangle T_k, thus ","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"tilde M(t) approx frac1mathrmArea(Omega)sum_T_k in mathcal T leftalpha_k iint_T_k xmathrmdA + beta_k iint_T_k ymathrmdA + gamma_kiint_T_kmathrmdAright ","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"Now factoring out an mathrmArea(T_k) = iint_T_kmathrmdA, ","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"tilde M(t) approx sum_T_k in mathcal T fracmathrmArea(T_k)mathrmArea(Omega)leftalpha_k dfraciint_T_k xmathrmdAiint_T_k mathrmdA + beta_k dfraciint_T_k ymathrmdAiint_T_k mathrmdA + gamma_kright ","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"Notice that the two ratios of integrals shown are simply hat x_k and hat y_k, where (hat x_k hat y_k) is the centroid of T_k. Thus, the term in brackets is alpha_k hat x_k + beta_k hat y_k + gamma_k, which is the approximation to u at the centroid, tilde u(hat x_k hat y_k t). Thus, our approximation to the average density is ","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"tilde M(t) approx sum_T_k in mathcal T w_k tilde u(hat x_k hat y_k t) qquad w_k = fracmathrmArea(T_k)mathrmArea(Omega) ","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"The following function provides a method for computing this mass. ","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"function compute_mass!(M::AbstractVector{T}, αβγ, sol, prob) where {T}\n    mesh_area = prob.mesh.mesh_information.total_area\n    fill!(M, zero(T))\n    for i in eachindex(M)\n        for V in FiniteVolumeMethod.get_elements(prob)\n            element = FiniteVolumeMethod.get_element_information(prob.mesh, V)\n            cx, cy = FiniteVolumeMethod.get_centroid(element)\n            element_area = FiniteVolumeMethod.get_area(element)\n            interpolant_val = eval_interpolant!(αβγ, prob, cx, cy, V, sol.u[i])\n            M[i] += (element_area / mesh_area) * interpolant_val\n        end\n    end\n    return nothing\nend ","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"Let's now compute this mass and add some noise onto it. ","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"using Random \nM = zeros(length(sol.t))\nαβγ = zeros(3)\ncompute_mass!(M, αβγ, sol, prob)\ntrue_M = deepcopy(M)\nRandom.seed!(29922881)\nσ = 0.1\ntrue_M .+= σ * randn(length(M))","category":"page"},{"location":"heat/#Defining-the-LikelihoodProblem","page":"Example IV: Diffusion equation on a square plate","title":"Defining the LikelihoodProblem","text":"","category":"section"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"We now need to define the likelihood problem. We need to use the method for LikelihoodProblem that takes the integrator as an argument explicitly, so we must somehow construct an integrator from an FVMProblem. Here is one way that this can be done. Notice that we use parallel=true so that the PDE is solved with multithreading. For an isolated solution, this seems to solve the PDE twice as fast on my machine (eight threads).","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"function ProfileLikelihood.construct_integrator(prob::FVMProblem, alg; ode_problem_kwargs, kwargs...)\n    ode_problem = ODEProblem(prob; no_saveat=false, ode_problem_kwargs...)\n    return ProfileLikelihood.construct_integrator(ode_problem, alg; kwargs...)\nend\njac = float.(FiniteVolumeMethod.jacobian_sparsity(prob))\nfvm_integrator = construct_integrator(prob, alg; ode_problem_kwargs=(jac_prototype=jac, saveat=0.01, parallel=true))","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"Now we define the likelihood function. ","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"function loglik_fvm(θ::AbstractVector{T}, param, integrator) where {T}\n    _k, _c, _u₀ = θ\n    ## Update and solve\n    (; prob) = param\n    prob.flux_parameters[1] = _k\n    pts = FiniteVolumeMethod.get_points(prob)\n    for i in axes(pts, 2)\n        pt = get_point(pts, i)\n        prob.initial_condition[i] = gety(pt) ≤ _c ? _u₀ : zero(T)\n    end\n    reinit!(integrator, prob.initial_condition)\n    solve!(integrator)\n    if !SciMLBase.successful_retcode(integrator.sol)\n        return typemin(T)\n    end\n    ## Compute the mass\n    (; mass_data, mass_cache, shape_cache, sigma) = param\n    compute_mass!(mass_cache, shape_cache, integrator.sol, prob)\n    if any(isnan, mass_cache)\n        return typemin(T)\n    end\n    ## Done \n    ℓ = @views gaussian_loglikelihood(mass_data, mass_cache, sigma, length(mass_data))\n    return ℓ\nend","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"Finally, here is the LikelihoodProblem.","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"likprob = LikelihoodProblem(\n    loglik_fvm,\n    [8.54, 0.98, 29.83],\n    fvm_integrator;\n    syms=[:k, :c, :u₀],\n    data=(prob=prob, mass_data=true_M, mass_cache=zeros(length(true_M)), shape_cache=zeros(3), sigma=σ),\n    f_kwargs=(adtype=Optimization.AutoFiniteDiff(),),\n    prob_kwargs=(lb=[3.0, 0.0, 0.0],\n        ub=[15.0, 2.0, 250.0])\n)","category":"page"},{"location":"heat/#Parameter-estimation","page":"Example IV: Diffusion equation on a square plate","title":"Parameter estimation","text":"","category":"section"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"Now that we have the problem completely setup, we are in a position for maximum likelihood estimation and profiling. For the maximum likelihood estimates, we first use a global optimiser and then we refine the solution with a local optimiser.","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"mle_sol = mle(likprob, (NLopt.GN_DIRECT_L_RAND(), NLopt.LN_BOBYQA); ftol_abs=1e-8, ftol_rel=1e-8, xtol_abs=1e-8, xtol_rel=1e-8) # global, and then refine with a local algorithm\nLikelihoodSolution. retcode: Failure\nMaximum likelihood: 11.046014040624534\nMaximum likelihood estimates: 3-element Vector{Float64}\n     k: 7.847020395441574\n     c: 1.1944331289720689\n     u₀: 41.667309553688305","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"Next, let us profile. For interest, we show the difference in runtime when we use multithreading for profiling vs. when we do not use multithreading. I am using eight threads.","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"@time prof = profile(likprob, mle_sol; alg=NLopt.LN_BOBYQA,\n    ftol_abs=1e-4, ftol_rel=1e-4, xtol_abs=1e-4, xtol_rel=1e-4,\n    resolution=60)\n5131.960778 seconds (133.61 M allocations: 948.495 GiB, 0.13% gc time, 0.04% compilation time)","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"@time _prof = profile(likprob, mle_sol; alg=NLopt.LN_BOBYQA,\n    ftol_abs=1e-4, ftol_rel=1e-4, xtol_abs=1e-4, xtol_rel=1e-4,\n    resolution=60, parallel=true)\n3324.605865 seconds (131.24 M allocations: 948.598 GiB, 0.40% gc time, 0.01% compilation time)","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"The results are about twice as fast in this example. The reason it's not even faster is because we are also using multithreading in solving the PDE. If we had no used multithreading in solving the PDE, these results would take a significantly longer time. Here are the results from prof (same for _prof):","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"ProfileLikelihoodSolution. MLE retcode: Failure\nConfidence intervals: \n     95.0% CI for k: (7.4088716591304715, 8.574442050142432)\n     95.0% CI for c: (0.6478281377475628, 2.0)\n     95.0% CI for u₀: (33.78499567791489, 79.47955668442242)","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"See that all the true parameter intervals are inside these confidence intervals except for k, although c's upper bound is right at the bounds we gave it in the problem. Let's now view the profile curves.","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"using CairoMakie, LaTeXStrings\nfig = plot_profiles(prof; nrow=1, ncol=3,\n    latex_names=[L\"k\", L\"c\", L\"u_0\"],\n    true_vals=[k[1], c, u₀],\n    fig_kwargs=(fontsize=38, resolution=(2109.644f0, 444.242f0)),\n    axis_kwargs=(width=600, height=300))\nscatter!(fig.content[1], get_parameter_values(prof, :k), get_profile_values(prof, :k), color=:black, markersize=9)\nscatter!(fig.content[2], get_parameter_values(prof, :c), get_profile_values(prof, :c), color=:black, markersize=9)\nscatter!(fig.content[3], get_parameter_values(prof, :u₀), get_profile_values(prof, :u₀), color=:black, markersize=9)\nxlims!(fig.content[1], 7.0, 9.5)","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"(Image: PDE profiles)","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"See that the profile curves for c and u_0 are very flat, and we have not recovered k. This means that the parameters c and u_0 are not identifiable, essentially meaning the data is not enough to recover these parameters. This is most likely because the mass tilde M(t) alone is not enough to uniquely define the solution. We could consider a summary statistic like ","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"mathcal S(t) = wtilde M(t) + (1-w)tilde A(t)","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"for some 0 leq w leq 1, where tilde A(t) is the area of the region below the leading edge of the solution, i.e. the area of the non-zero part of the solution. We do not consider this here. What we do consider is fixing c, keeping the summary statistic tilde M(t), and seeing what we can do with only two parameters k and u_0.","category":"page"},{"location":"heat/#Reducing-to-two-parameters-and-grid-searching","page":"Example IV: Diffusion equation on a square plate","title":"Reducing to two parameters and grid searching","text":"","category":"section"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"Let us now fix c at its true value, c = 1, and consider estimating only k and u_0. Since we have only k and u_0 to estimate, it may be worthwhile to perform a grid search over our likelihood function so that we can (1) visualise the likelihood surface and (2) see reasonable estimates for k and u_0. ","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"First, we redefine the problem.","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"using StaticArraysCore\n@inline function loglik_fvm_2(θ::AbstractVector{T}, param, integrator) where {T}\n    _k, _u₀, = θ\n    (; c) = param\n    new_θ = SVector{3,T}((_k, c, _u₀))\n    return loglik_fvm(new_θ, param, integrator)\n\nend\nlikprob_2 = LikelihoodProblem(\n    loglik_fvm_2,\n    [8.54, 29.83],\n    fvm_integrator;\n    syms=[:k, :u₀],\n    data=(prob=prob, mass_data=true_M, mass_cache=zeros(length(true_M)), shape_cache=zeros(3), sigma=σ, c=c),\n    f_kwargs=(adtype=Optimization.AutoFiniteDiff(),),\n    prob_kwargs=(lb=[3.0, 0.0],\n        ub=[15.0, 250.0])\n)","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"Now let's do our grid search. We show the timing when we use a multithreaded grid search vs. a serial grid search. ","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"grid = RegularGrid(get_lower_bounds(likprob_2), get_upper_bounds(likprob_2), 50)\n@time gs, lik_vals = grid_search(likprob_2, grid; save_vals = Val(true), parallel=Val(true))\n1529.393520 seconds (91.55 M allocations: 606.223 GiB, 2.10% gc time)","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"@time _gs, _lik_vals = grid_search(likprob_2, grid; save_vals = Val(true), parallel=Val(false))\n3454.357503 seconds (86.48 M allocations: 605.468 GiB, 0.14% gc time)","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"Here are the results from the grid search.","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"LikelihoodSolution. retcode: Success\nMaximum likelihood: -24.399451875029165\nMaximum likelihood estimates: 2-element Vector{Float64}\n     k: 7.408163265306122\n     u₀: 51.0204081632653","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"Let us now visualise the likelihood function.","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"fig = Figure(fontsize=38)\nk_grid = get_range(grid, 1)\nu₀_grid = get_range(grid, 2)\nax = Axis(fig[1, 1],\n    xlabel=L\"k\", ylabel=L\"u_0\",\n    xticks=0:3:15,\n    yticks=0:50:250)\nco = heatmap!(ax, k_grid, u₀_grid, lik_vals, colormap=Reverse(:matter))\ncontour!(ax, k_grid, u₀_grid, lik_vals, levels=40, color=:black, linewidth=1 / 4)\nscatter!(ax, [k[1]], [u₀], color=:white, markersize=14)\nscatter!(ax, [gs[:k]], [gs[:u₀]], color=:blue, markersize=14)\nclb = Colorbar(fig[1, 2], co, label=L\"\\ell(k, u_0)\", vertical=true)","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"(Image: Likelihood function for the PDE)","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"The true parameter values are shown at the white marker, while the results from the grid search are shown in blue, and these two markers are reasonably close. We see that the likelihood function is quite flat around these values, so this might be an indicator of further identifiability issues. Let us now use the grid search results to update our initial guess and compute the MLEs, and then we profile.","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"likprob_2 = update_initial_estimate(likprob_2, gs)\nmle_sol = mle(likprob_2, NLopt.LN_BOBYQA; ftol_abs=1e-8, ftol_rel=1e-8, xtol_abs=1e-8, xtol_rel=1e-8)\nLikelihoodSolution. retcode: Failure\nMaximum likelihood: 11.016184577792082\nMaximum likelihood estimates: 2-element Vector{Float64}\n     k: 9.40527352240195\n     u₀: 49.741093700294336","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"@time prof = profile(likprob_2, mle_sol; ftol_abs=1e-4, ftol_rel=1e-4, xtol_abs=1e-4, xtol_rel=1e-4, parallel=true)\n612.723061 seconds (25.45 M allocations: 155.874 GiB, 0.41% gc time)\nProfileLikelihoodSolution. MLE retcode: Failure\nConfidence intervals: \n     95.0% CI for k: (8.788003299163778, 10.094019297587579)\n     95.0% CI for u₀: (49.44377511158833, 50.03883730450469)","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"The confidence intervals contain the true values. We can now visualise.","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"fig = plot_profiles(prof; nrow=1, ncol=3,\n    latex_names=[L\"k\", L\"u_0\"],\n    true_vals=[k[1], u₀],\n    fig_kwargs=(fontsize=38, resolution=(1441.9216f0, 470.17322f0)),\n    axis_kwargs=(width=600, height=300))\nscatter!(fig.content[1], get_parameter_values(prof, :k), get_profile_values(prof, :k), color=:black, markersize=9)\nscatter!(fig.content[2], get_parameter_values(prof, :u₀), get_profile_values(prof, :u₀), color=:black, markersize=9)","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"(Image: Second set of profiles)","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"See that we've recovered the parameters in the confidence intervals, and the profiles are smooth – the identifiability issues are gone. So, it seems like c was the problematic parameter, since our summary statistic does not really give us any information about it. Our idea of using the summary statistic mathcal S(t) from above would likely ameliorate this issue, since it will give information directly relating to c.","category":"page"},{"location":"heat/#Comparing-methods-for-constructing-initial-estimates-when-profiling","page":"Example IV: Diffusion equation on a square plate","title":"Comparing methods for constructing initial estimates when profiling","text":"","category":"section"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"In the mathematical details section at the end of this README, it is mentioned that initial values for boldsymbolomega_j (the parameters to be optimised while an interest parameter is held fixed) can currently be set in two ways:","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"Method 1: Simply starting boldsymbolomega_j at boldsymbolomega_j-1. This is the next_initial_estimate_method = :prev option in profile, and is the default.\nMethod 2: Using linear interpolation, we can use the previous two values and set boldsymbolomega_j = boldsymbolomega_j-2(psi_j-1 - psi_j) + boldsymbolomega_j-1(psi_j - psi_j-2)  (psi_j-1 - psi_j-2) (if boldsymbolomega_j then starts outside of the parameter bounds, we fall back to the first method). This is the next_initial_estimate_method = :interp option in profile.","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"Is there a big difference in these methods? Let's demonstrate if there is any difference by doing some benchmarking. We will also compare multithreading versus no multithreading.","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"bnch_prev_serial = @benchmark profile($likprob_2, $mle_sol; ftol_abs=$1e-4, ftol_rel=$1e-4, xtol_abs=$1e-4, xtol_rel=$1e-4, parallel=$false, next_initial_estimate_method=$:prev)\nbnch_interp_serial = @benchmark profile($likprob_2, $mle_sol; ftol_abs=$1e-4, ftol_rel=$1e-4, xtol_abs=$1e-4, xtol_rel=$1e-4, parallel=$false, next_initial_estimate_method=$:interp)\nbnch_prev_parallel = @benchmark profile($likprob_2, $mle_sol; ftol_abs=$1e-4, ftol_rel=$1e-4, xtol_abs=$1e-4, xtol_rel=$1e-4, parallel=$true, next_initial_estimate_method=$:prev)\nbnch_interp_parallel = @benchmark profile($likprob_2, $mle_sol; ftol_abs=$1e-4, ftol_rel=$1e-4, xtol_abs=$1e-4, xtol_rel=$1e-4, parallel=$true, next_initial_estimate_method=$:interp)","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"Here are the results:","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"julia> bnch_prev_serial\nBenchmarkTools.Trial: 1 sample with 1 evaluation.\n Single result which took 855.578 s (0.23% GC) to evaluate,\n with a memory estimate of 155.70 GiB, over 24670284 allocations.","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"julia> bnch_interp_serial\nBenchmarkTools.Trial: 1 sample with 1 evaluation.\n Single result which took 757.444 s (0.24% GC) to evaluate,\n with a memory estimate of 144.34 GiB, over 22976564 allocations.","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"julia> bnch_prev_parallel\nBenchmarkTools.Trial: 1 sample with 1 evaluation.\n Single result which took 548.814 s (0.34% GC) to evaluate,\n with a memory estimate of 155.87 GiB, over 25443078 allocations.","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"julia> bnch_interp_parallel\nBenchmarkTools.Trial: 1 sample with 1 evaluation.\n Single result which took 498.408 s (0.36% GC) to evaluate,\n with a memory estimate of 144.52 GiB, over 23809418 allocations.","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"We see that linear interpolation is a significant help to the algorithm, saving 100 seconds when we profile without multithreading, and 50 seconds when we profile with multithreading. In summary, profiling with the :interp method was about 12% faster than :prev without multithreading, and about 10% faster with multithreading –- interpolation is certainly a big help. For problems where the likelihood function is much faster to compute, these results may be opposite – it is worth thinking about this for your applications.","category":"page"},{"location":"heat/#Prediction-intervals-for-the-mass","page":"Example IV: Diffusion equation on a square plate","title":"Prediction intervals for the mass","text":"","category":"section"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"Let us now consider propagating the uncertainty in k and u_0 into computing prediction intervals for tilde M(t) at each t. This is done using the get_prediction_intervals function introduced in the second example. First, we must define our prediction function.","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"@inline function compute_mass_function(θ::AbstractVector{T}, data) where {T}\n    k, u₀ = θ\n    (; c, prob, t, alg, jac) = data\n    prob.flux_parameters[1] = k\n    pts = FiniteVolumeMethod.get_points(prob)\n    for i in axes(pts, 2)\n        pt = get_point(pts, i)\n        prob.initial_condition[i] = gety(pt) ≤ c ? u₀ : zero(T)\n    end\n    sol = solve(prob, alg; saveat=t, parallel=true, jac_prototype=jac)\n    shape_cache = zeros(T, 3)\n    mass_cache = zeros(T, length(sol.u))\n    compute_mass!(mass_cache, shape_cache, sol, prob)\n    return mass_cache\nend","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"Now let's get the intervals.","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"t_many_pts = LinRange(prob.initial_time, prob.final_time, 250)\njac = FiniteVolumeMethod.jacobian_sparsity(prob)\nprediction_data = (c=c, prob=prob, t=t_many_pts, alg=alg, jac=jac)\nparameter_wise, union_intervals, all_curves, param_range =\n    get_prediction_intervals(compute_mass_function, prof, prediction_data; parallel=true)","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"Now we can visualise the curves. We will also show the mass curve from the exact parameter values, as well as from the MLE. ","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"exact_soln = compute_mass_function([k[1], u₀], prediction_data)\nmle_soln = compute_mass_function(get_mle(mle_sol), prediction_data)\nfig = Figure(fontsize=38, resolution=(1360.512f0, 848.64404f0))\nalp = join('a':'z')\nlatex_names = [L\"k\", L\"u_0\"]\nfor i in 1:2\n    ax = Axis(fig[1, i], title=L\"(%$(alp[i])): Profile-wise PI for %$(latex_names[i])\",\n        titlealign=:left, width=600, height=300)\n    [lines!(ax, t_many_pts, all_curves[i][:, j], color=:grey) for j in eachindex(param_range[1])]\n    lines!(ax, t_many_pts, exact_soln, color=:red)\n    lines!(ax, t_many_pts, mle_soln, color=:blue, linestyle=:dash)\n    lines!(ax, t_many_pts, getindex.(parameter_wise[i], 1), color=:black)\n    lines!(ax, t_many_pts, getindex.(parameter_wise[i], 2), color=:black)\nend\nax = Axis(fig[2, 1:2], title=L\"(c):$ $ Union of all intervals\",\n    titlealign=:left, width=1200, height=300)\nband!(ax, t_many_pts, getindex.(union_intervals, 1), getindex.(union_intervals, 2), color=:grey)\nlines!(ax, t_many_pts, getindex.(union_intervals, 1), color=:black)\nlines!(ax, t_many_pts, getindex.(union_intervals, 2), color=:black)\nlines!(ax, t_many_pts, exact_soln, color=:red)\nlines!(ax, t_many_pts, mle_soln, color=:blue, linestyle=:dash)","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"Let us also add onto these plots the intervals coming from the full likelihood. (The reason to just not do this everytime in applications is because the code below takes a very long time to compute - a lifetime compared to the profile-wise intervals above.)","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"lb = [8.0, 45.0]\nub = [11.0, 50.0]\nN = 1e4\ngrid = [[lb[i] + (ub[i] - lb[i]) * rand() for _ in 1:N] for i in 1:2]\ngrid = permutedims(reduce(hcat, grid), (2, 1))\nig = IrregularGrid(lb, ub, grid)\ngs, lik_vals = grid_search(likprob_2, ig; parallel=Val(true), save_vals=Val(true))\nlik_vals .-= get_maximum(mle_sol) # normalised \nfeasible_idx = findall(lik_vals .> ProfileLikelihood.get_chisq_threshold(0.95)) # values in the confidence region \nparameter_evals = grid[:, feasible_idx]\nq = [compute_mass_function(θ, prediction_data) for θ in eachcol(parameter_evals)]\nq_mat = reduce(hcat, q)\nq_lwr = minimum(q_mat; dims=2) |> vec\nq_upr = maximum(q_mat; dims=2) |> vec\nlines!(ax, t_many_pts, q_lwr, color=:magenta)\nlines!(ax, t_many_pts, q_upr, color=:magenta)","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"(Image: Prediction intervals for the mass)","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"The exact curve has been recovered by our profile likelihood results, and the uncertainty is extremely small. Moreover, the intervals are indeed close to the interval obtained the full profile likelihood as we would hope.","category":"page"},{"location":"heat/#Just-the-code","page":"Example IV: Diffusion equation on a square plate","title":"Just the code","text":"","category":"section"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"Here is all the code used for obtaining the results in this example, should you want a version that you can directly copy and paste.","category":"page"},{"location":"heat/","page":"Example IV: Diffusion equation on a square plate","title":"Example IV: Diffusion equation on a square plate","text":"## Step 1: Define the problem. See FiniteVolumeMethod.jl\nusing DelaunayTriangulation, FiniteVolumeMethod\na, b, c, d = 0.0, 2.0, 0.0, 2.0\nr = 0.022\nGMSH_PATH = \"./gmsh-4.9.4-Windows64/gmsh.exe\"\ntri = generate_mesh(x, y, r; gmsh_path=GMSH_PATH)\nmesh = FVMGeometry(tri)\nbc = ((x, y, t, u::T, p) where {T}) -> zero(T)\ntype = :D\nBCs = BoundaryConditions(mesh, bc, type)\nc = 1.0\nu₀ = 50.0\nf = (x, y) -> y ≤ c ? u₀ : 0.0\nD = (x, y, t, u, p) -> p[1]\nflux = (q, x, y, t, α, β, γ, p) -> (q[1] = -α / p[1]; q[2] = -β / p[1])\nR = ((x, y, t, u::T, p) where {T}) -> zero(T)\npoints = get_points(tri)\ninitc = @views f.(points[1, :], points[2, :])\niip_flux = true\nfinal_time = 0.1\nk = [9.0]\nprob = FVMProblem(mesh, BCs; iip_flux,\n    flux_function=flux, reaction_function=R,\n    initial_condition=initc, final_time,\n    flux_parameters=deepcopy(k))\n\n## Step 2: Generate some data.\nusing LinearSolve, OrdinaryDiffEq\nalg = TRBDF2(linsolve=KLUFactorization(; reuse_symbolic=false))\nsol = solve(prob, alg; specialization=SciMLBase.FullSpecialize, saveat=0.01)    \n\n## Step 3: Let us compute the mass at each time and then add some noise to it\nusing Random\nfunction compute_mass!(M::AbstractVector{T}, αβγ, sol, prob) where {T}\n    mesh_area = prob.mesh.mesh_information.total_area\n    fill!(M, zero(T))\n    for i in eachindex(M)\n        for V in FiniteVolumeMethod.get_elements(prob)\n            element = FiniteVolumeMethod.get_element_information(prob.mesh, V)\n            cx, cy = FiniteVolumeMethod.get_centroid(element)\n            element_area = FiniteVolumeMethod.get_area(element)\n            interpolant_val = eval_interpolant!(αβγ, prob, cx, cy, V, sol.u[i])\n            M[i] += (element_area / mesh_area) * interpolant_val\n        end\n    end\n    return nothing\nend\nM = zeros(length(sol.t))\nαβγ = zeros(3)\ncompute_mass!(M, αβγ, sol, prob)\ntrue_M = deepcopy(M)\nRandom.seed!(29922881)\nσ = 0.1\ntrue_M .+= σ * randn(length(M))\n\n## Step 4: We need to now construct the integrator. Here's a method for converting an FVMProblem into an integrator. \nfunction ProfileLikelihood.construct_integrator(prob::FVMProblem, alg; ode_problem_kwargs, kwargs...)\n    ode_problem = ODEProblem(prob; no_saveat=false, ode_problem_kwargs...)\n    return ProfileLikelihood.construct_integrator(ode_problem, alg; kwargs...)\nend\njac = float.(FiniteVolumeMethod.jacobian_sparsity(prob))\nfvm_integrator = construct_integrator(prob, alg; ode_problem_kwargs=(jac_prototype=jac, saveat=0.01, parallel=true))\n\n## Step 5: Now define the likelihood problem \nusing Optimization\n@inline function loglik_fvm(θ::AbstractVector{T}, param, integrator) where {T}\n    _k, _c, _u₀ = θ\n    ## Update and solve\n    (; prob) = param\n    prob.flux_parameters[1] = _k\n    pts = FiniteVolumeMethod.get_points(prob)\n    for i in axes(pts, 2)\n        pt = get_point(pts, i)\n        prob.initial_condition[i] = gety(pt) ≤ _c ? _u₀ : zero(T)\n    end\n    reinit!(integrator, prob.initial_condition)\n    solve!(integrator)\n    if !SciMLBase.successful_retcode(integrator.sol)\n        return typemin(T)\n    end\n    ## Compute the mass\n    (; mass_data, mass_cache, shape_cache, sigma) = param\n    compute_mass!(mass_cache, shape_cache, integrator.sol, prob)\n    if any(isnan, mass_cache)\n        return typemin(T)\n    end\n    ## Done \n    ℓ = @views gaussian_loglikelihood(mass_data, mass_cache, sigma, length(mass_data))\n    @show ℓ\n    return ℓ\nend\nlikprob = LikelihoodProblem(\n    loglik_fvm,\n    [8.54, 0.98, 29.83],\n    fvm_integrator;\n    syms=[:k, :c, :u₀],\n    data=(prob=prob, mass_data=true_M, mass_cache=zeros(length(true_M)), shape_cache=zeros(3), sigma=σ),\n    f_kwargs=(adtype=Optimization.AutoFiniteDiff(),),\n    prob_kwargs=(lb=[3.0, 0.0, 0.0],\n        ub=[15.0, 2.0, 250.0])\n)\n\n## Step 6: Find the MLEs \nusing OptimizationNLopt \nmle_sol = mle(likprob, (NLopt.GN_DIRECT_L_RAND(), NLopt.LN_BOBYQA); ftol_abs=1e-8, ftol_rel=1e-8, xtol_abs=1e-8, xtol_rel=1e-8) # global, and then refine with a local algorithm\n\n## Step 7: Profile \nprof = profile(likprob, mle_sol; alg=NLopt.LN_BOBYQA,\n    ftol_abs=1e-4, ftol_rel=1e-4, xtol_abs=1e-4, xtol_rel=1e-4,\n    resolution=60)\n\n## Step 8: Visualise \nusing CairoMakie, LaTeXStrings\nfig = plot_profiles(prof; nrow=1, ncol=3,\n    latex_names=[L\"k\", L\"c\", L\"u_0\"],\n    true_vals=[k[1], c, u₀],\n    fig_kwargs=(fontsize=38, resolution=(2109.644f0, 444.242f0)),\n    axis_kwargs=(width=600, height=300))\nscatter!(fig.content[1], get_parameter_values(prof, :k), get_profile_values(prof, :k), color=:black, markersize=9)\nscatter!(fig.content[2], get_parameter_values(prof, :c), get_profile_values(prof, :c), color=:black, markersize=9)\nscatter!(fig.content[3], get_parameter_values(prof, :u₀), get_profile_values(prof, :u₀), color=:black, markersize=9)\nxlims!(fig.content[1], 7.0, 9.5)\n\n### Now consider profiling only two parameters\n## Step 9: Define the problem \nusing StaticArraysCore\n@inline function loglik_fvm_2(θ::AbstractVector{T}, param, integrator) where {T}\n    _k, _u₀, = θ\n    (; c) = param\n    new_θ = SVector{3,T}((_k, c, _u₀))\n    return loglik_fvm(new_θ, param, integrator)\n\nend\nlikprob_2 = LikelihoodProblem(\n    loglik_fvm_2,\n    [8.54, 29.83],\n    fvm_integrator;\n    syms=[:k, :u₀],\n    data=(prob=prob, mass_data=true_M, mass_cache=zeros(length(true_M)), shape_cache=zeros(3), sigma=σ, c=c),\n    f_kwargs=(adtype=Optimization.AutoFiniteDiff(),),\n    prob_kwargs=(lb=[3.0, 0.0],\n        ub=[15.0, 250.0])\n)\n\n## Step 10: Grid search \ngrid = RegularGrid(get_lower_bounds(likprob_2), get_upper_bounds(likprob_2), 50)\ngs, lik_vals = grid_search(likprob_2, grid; save_vals=Val(true), parallel=Val(true));\n\n## Step 11: Visualise \nfig = Figure(fontsize=38)\nk_grid = get_range(grid, 1)\nu₀_grid = get_range(grid, 2)\nax = Axis(fig[1, 1],\n    xlabel=L\"k\", ylabel=L\"u_0\",\n    xticks=0:3:15,\n    yticks=0:50:250)\nco = heatmap!(ax, k_grid, u₀_grid, lik_vals, colormap=Reverse(:matter))\ncontour!(ax, k_grid, u₀_grid, lik_vals, levels=40, color=:black, linewidth=1 / 4)\nscatter!(ax, [k[1]], [u₀], color=:white, markersize=14)\nscatter!(ax, [gs[:k]], [gs[:u₀]], color=:blue, markersize=14)\nclb = Colorbar(fig[1, 2], co, label=L\"\\ell(k, u_0)\", vertical=true)\n\n## Step 12: Find the MLEs and profile \nlikprob_2 = update_initial_estimate(likprob_2, gs)\nmle_sol = mle(likprob_2, NLopt.LN_BOBYQA; ftol_abs=1e-8, ftol_rel=1e-8, xtol_abs=1e-8, xtol_rel=1e-8)\nprof = profile(likprob_2, mle_sol; ftol_abs=1e-4, ftol_rel=1e-4, xtol_abs=1e-4, xtol_rel=1e-4, parallel=true)\n\n## Step 13: Visualise the profile \nfig = plot_profiles(prof; nrow=1, ncol=3,\n    latex_names=[L\"k\", L\"u_0\"],\n    true_vals=[k[1], u₀],\n    fig_kwargs=(fontsize=38, resolution=(1441.9216f0, 470.17322f0)),\n    axis_kwargs=(width=600, height=300))\nscatter!(fig.content[1], get_parameter_values(prof, :k), get_profile_values(prof, :k), color=:black, markersize=9)\nscatter!(fig.content[2], get_parameter_values(prof, :u₀), get_profile_values(prof, :u₀), color=:black, markersize=9)\n\n## Step 14: Compute prediction intervals for the mass\n@inline function compute_mass_function(θ::AbstractVector{T}, data) where {T}\n    k, u₀ = θ\n    (; c, prob, t, alg, jac) = data\n    prob.flux_parameters[1] = k\n    pts = FiniteVolumeMethod.get_points(prob)\n    for i in axes(pts, 2)\n        pt = get_point(pts, i)\n        prob.initial_condition[i] = gety(pt) ≤ c ? u₀ : zero(T)\n    end\n    sol = solve(prob, alg; saveat=t, parallel=true, jac_prototype=jac)\n    shape_cache = zeros(T, 3)\n    mass_cache = zeros(T, length(sol.u))\n    compute_mass!(mass_cache, shape_cache, sol, prob)\n    return mass_cache\nend\nt_many_pts = LinRange(prob.initial_time, prob.final_time, 250)\njac = FiniteVolumeMethod.jacobian_sparsity(prob)\nprediction_data = (c=c, prob=prob, t=t_many_pts, alg=alg, jac=jac)\nparameter_wise, union_intervals, all_curves, param_range =\n    get_prediction_intervals(compute_mass_function, prof, prediction_data; parallel=true)\n\n## Step 15: Visualise the prediction intervals\nexact_soln = compute_mass_function([k[1], u₀], prediction_data)\nmle_soln = compute_mass_function(get_mle(mle_sol), prediction_data)\n\nfig = Figure(fontsize=38, resolution=(1360.512f0, 848.64404f0))\nalp = join('a':'z')\nlatex_names = [L\"k\", L\"u_0\"]\nfor i in 1:2\n    ax = Axis(fig[1, i], title=L\"(%$(alp[i])): Profile-wise PI for %$(latex_names[i])\",\n        titlealign=:left, width=600, height=300)\n    [lines!(ax, t_many_pts, all_curves[i][:, j], color=:grey) for j in eachindex(param_range[1])]\n    lines!(ax, t_many_pts, exact_soln, color=:red)\n    lines!(ax, t_many_pts, mle_soln, color=:blue, linestyle=:dash)\n    lines!(ax, t_many_pts, getindex.(parameter_wise[i], 1), color=:black)\n    lines!(ax, t_many_pts, getindex.(parameter_wise[i], 2), color=:black)\nend\nax = Axis(fig[2, 1:2], title=L\"(c):$ $ Union of all intervals\",\n    titlealign=:left, width=1200, height=300)\nband!(ax, t_many_pts, getindex.(union_intervals, 1), getindex.(union_intervals, 2), color=:grey)\nlines!(ax, t_many_pts, getindex.(union_intervals, 1), color=:black)\nlines!(ax, t_many_pts, getindex.(union_intervals, 2), color=:black)\nlines!(ax, t_many_pts, exact_soln, color=:red)\nlines!(ax, t_many_pts, mle_soln, color=:blue, linestyle=:dash)\n\nlb = [8.0, 45.0]\nub = [11.0, 50.0]\nN = 1e4\ngrid = [[lb[i] + (ub[i] - lb[i]) * rand() for _ in 1:N] for i in 1:2]\ngrid = permutedims(reduce(hcat, grid), (2, 1))\nig = IrregularGrid(lb, ub, grid)\ngs, lik_vals = grid_search(likprob_2, ig; parallel=Val(true), save_vals=Val(true))\nlik_vals .-= get_maximum(mle_sol) # normalised \nfeasible_idx = findall(lik_vals .> ProfileLikelihood.get_chisq_threshold(0.95)) # values in the confidence region \nparameter_evals = grid[:, feasible_idx]\nq = [compute_mass_function(θ, prediction_data) for θ in eachcol(parameter_evals)]\nq_mat = reduce(hcat, q)\nq_lwr = minimum(q_mat; dims=2) |> vec\nq_upr = maximum(q_mat; dims=2) |> vec\nlines!(ax, t_many_pts, q_lwr, color=:magenta)\nlines!(ax, t_many_pts, q_upr, color=:magenta)","category":"page"},{"location":"interface/#Interface","page":"Interface","title":"Interface","text":"","category":"section"},{"location":"interface/","page":"Interface","title":"Interface","text":"The interface for defining a likelihood problem builds on top of Optimization.jl. Below we list the three main structs that we use, with LikelihoodProblem the most important one and the only one that needs to be directly defined. Examples of how we use these structs are given later, and much extra functionality is given in the tests. Complete docstrings are given in the sidebar.","category":"page"},{"location":"interface/#LikelihoodProblem:-Defining-the-likelihood-problem","page":"Interface","title":"LikelihoodProblem: Defining the likelihood problem","text":"","category":"section"},{"location":"interface/","page":"Interface","title":"Interface","text":"The LikelihoodProblem is the definition of a likelihood function, and provides the following constructor:","category":"page"},{"location":"interface/","page":"Interface","title":"Interface","text":"LikelihoodProblem(loglik::Function, θ₀;\n    syms=eachindex(θ₀), data=SciMLBase.NullParameters(),\n    f_kwargs=nothing, prob_kwargs=nothing)","category":"page"},{"location":"interface/","page":"Interface","title":"Interface","text":"Here, loglik is a function for the log-likelihood, taking the form ℓ(θ, p). The second argument, θ₀, is the initial estimate for the parameter values. You can provide symbolic names for the parameters via syms, so that e.g. prob[:α] (where prob is a LikelihoodProblem with :α ∈ syms) returns the initial estimate for :α. The argument p in the likelihood function can be used to pass data or other parameters into the argument, and the keyword argument data can be used for this. Lastly, f_kwargs and prob_kwargs are additional keyword arguments for the OptimizationFunction and OptimizationProblem, respectively; see the Optimization.jl documentation for more detail here. ","category":"page"},{"location":"interface/","page":"Interface","title":"Interface","text":"We also provide a simple interface for defining a log-likelihood that requires the solution of a differential equation:","category":"page"},{"location":"interface/","page":"Interface","title":"Interface","text":"LikelihoodProblem(loglik::Function, θ₀,\n    ode_function, u₀, tspan;\n    syms=eachindex(θ₀), data=SciMLBase.NullParameters(),\n    ode_parameters=SciMLBase.NullParameters(), ode_alg,\n    ode_kwargs=nothing, f_kwargs=nothing, prob_kwargs=nothing)","category":"page"},{"location":"interface/","page":"Interface","title":"Interface","text":"Importantly, loglik in this case is now a function of the form ℓ(θ, p, integrator), where integrator is the same integrator as in the integrator interface from DifferentialEquations.jl; see the documentation at DifferentialEquations.jl for more detail on using the integrator. Furthermore, ode_function is the function for the ODE, u₀ its initial condition, and tspan its time span. Additionally, the parameters for the ode_function (e.g. the p in ode_function(du, u, p, t) or ode_function(u, p, t)) can be passed using the keyword argument ode_parameters. The algorithm used to solve the differential equation is passed with ode_alg, and lastly any additional keyword arguments for solving the problem are to be passed through ode_kwargs. ","category":"page"},{"location":"interface/","page":"Interface","title":"Interface","text":"There is also a method that makes it easier to use automatic differentiation when considering ODE problems by using a GeneralLazyBufferCache from PreallocationTools.jl - see Example V.","category":"page"},{"location":"interface/","page":"Interface","title":"Interface","text":"The full docstrings for the methods available are given in the sidebar.","category":"page"},{"location":"interface/#LikelihoodSolution:-Obtaining-an-MLE","page":"Interface","title":"LikelihoodSolution: Obtaining an MLE","text":"","category":"section"},{"location":"interface/","page":"Interface","title":"Interface","text":"The MLEs for a given LikelihoodProblem are found using the function mle, e.g. mle(prob, Optim.LBFGS()) will optimise the likelihood function using the LBFGS algorithm from Optim.jl (see also ?mle). This function returns a LikelihoodSolution, defined by:","category":"page"},{"location":"interface/","page":"Interface","title":"Interface","text":"struct LikelihoodSolution{N,Θ,P,M,R,A} <: AbstractLikelihoodSolution{N,P}\n    mle::Θ\n    problem::P\n    optimiser::A\n    maximum::M\n    retcode::R\nend","category":"page"},{"location":"interface/","page":"Interface","title":"Interface","text":"If sol isa LikelihoodSolution, then you can use the syms from your original problem to access a specific MLE, e.g. sol[:α] would return the MLE for the paramter :α.","category":"page"},{"location":"interface/","page":"Interface","title":"Interface","text":"If you want to use multiple optimisers, i.e. a sequence of optimisers (O_1 O_2 ldots), in which O_j's initial estimate starts from the solution from the optimiser O_j-1, you can also provide a Tuple into the algorithm argument, e.g. mle(prob, (Optim.LBFGS(), NLopt.LN_NELDERMEAD)).","category":"page"},{"location":"interface/","page":"Interface","title":"Interface","text":"The full docstring for mle is given in the docstring section in the sidebar, along with the docstring for LikelihoodSolution.","category":"page"},{"location":"interface/#ProfileLikelihoodSolution:-Profiling-the-parameters","page":"Interface","title":"ProfileLikelihoodSolution: Profiling the parameters","text":"","category":"section"},{"location":"interface/","page":"Interface","title":"Interface","text":"The results for a profile likelihood, obtained from profile(prob, sol) (see also ?profile), are stored in a ProfileLikelihoodSolution struct:","category":"page"},{"location":"interface/","page":"Interface","title":"Interface","text":"struct ProfileLikelihoodSolution{I,V,LP,LS,Spl,CT,CF,OM}\n    parameter_values::Dict{I,V}\n    profile_values::Dict{I,V}\n    likelihood_problem::LP\n    likelihood_solution::LS\n    splines::Dict{I,Spl}\n    confidence_intervals::Dict{I,ConfidenceInterval{CT,CF}}\n    other_mles::OM\nend","category":"page"},{"location":"interface/","page":"Interface","title":"Interface","text":"Here, the parameter values used for each parameter are given in parameter_values, with parameter indices (or symbols) mapped to these values. Similarly, the values of the profile log-likelihood are stored in profile_values. We use a spline (see Interpolations.jl) to make the profile log-likelihood a continuous function, and these splines are given by splines. Next, the computed confidence intervals are given in confidence_intervals, with a confidence interval represented by a ConfidenceInterval struct. Lastly, since computing the profile log-likelihood function requires an optimisation problem with one variable fixed and the others free, we obtain for each profile log-likelihood value a set of optimised parameters – these parameters are given in other_mles.","category":"page"},{"location":"interface/","page":"Interface","title":"Interface","text":"If prof is a ProfileLikelihoodSolution, then you can also call it as e.g. prof(0.5, 1) to evaluate the profile log-likelihood function of the first parameter at the point 0.5. Alternatively, prof(0.7, :α) does the same but for the parameter :α at the point 0.7. You can also index prof at a specific index (or symbol) to see the results only for that parameter, e.g. prof[1] or prof[:α]; this returns a ProfileLikelihoodSolutionView.","category":"page"},{"location":"interface/","page":"Interface","title":"Interface","text":"The full docstring for profile and related functions are given in the sidebar.","category":"page"},{"location":"interface/#BivariateProfileLikelihoodSolution:-Computing-bivariate-profiles","page":"Interface","title":"BivariateProfileLikelihoodSolution: Computing bivariate profiles","text":"","category":"section"},{"location":"interface/","page":"Interface","title":"Interface","text":"You can also compute bivariate profiles, obtained from e.g. bivariate_profile(prob, sol, ((1, 2), (3, 1))), which will compute bivariate profiles between the parameters (1, 2) and between (3, 1). The results are stored in a BivariateProfileLikelihoodSolution struct: ","category":"page"},{"location":"interface/","page":"Interface","title":"Interface","text":"struct BivariateProfileLikelihoodSolution{I,G,V,LP,LS,Spl,CT,CF,OM}\n    parameter_values::Dict{I,G}\n    profile_values::Dict{I,V}\n    likelihood_problem::LP\n    likelihood_solution::LS\n    interpolants::Dict{I,Spl}\n    confidence_regions::Dict{I,ConfidenceRegion{CT,CF}}\n    other_mles::OM\nend","category":"page"},{"location":"interface/","page":"Interface","title":"Interface","text":"The definitions are similar to the univariate case, although parameter_values now maps to Tuples of grids, making use of OffsetVectors to define grids relative to an MLE. Similarly, we use OffsetMatrixs to define the grid of profile values, given in profile_values. You can also call the resulting struct, e.g. if prof is a BivariateProfileLikelihoodSolution, then prof(0.3, 0.9, :λ, :K) computes the bivariate profile between lambda and K at (lambdaK)=(0309). See ?ProfileLikelihood.BivariateProfileLikelihoodSolution for more detail, or its docstring in the sidebar. ","category":"page"},{"location":"interface/","page":"Interface","title":"Interface","text":"You can also index such a prof to look at only a specific parameter. For example, prof[1, 2] will return a BivariateProfileLikelihoodSolutionView for the profile between the first and second parameter. Similarly, prof[:λ, :K] is the profile between lambda and K.","category":"page"},{"location":"interface/#Propagating-uncertainty:-Prediction-intervals","page":"Interface","title":"Propagating uncertainty: Prediction intervals","text":"","category":"section"},{"location":"interface/","page":"Interface","title":"Interface","text":"The confidence intervals obtained from profiling can be used to obtain approximate prediction intervals via profile-wise profile likelihoods, as defined e.g. in Simpson and Maclaren (2022), for a prediction function boldsymbol q(boldsymboltheta). These intervals can be based on varying a single parameter, or by taking the union of individual prediction intervals. The main function for this is get_prediction_intervals. Rather than explain in full detail here, please refer to the second example (the logistic ODE example), where we reproduce the first case study of Simpson and Maclaren (2022).","category":"page"},{"location":"interface/","page":"Interface","title":"Interface","text":"The full docstring for get_prediction_intervals is given in the sidebar.","category":"page"},{"location":"interface/#Plotting","page":"Interface","title":"Plotting","text":"","category":"section"},{"location":"interface/","page":"Interface","title":"Interface","text":"We provide a function plot_profiles that can be useful for plotting profile likelihoods. It requires that you have done ","category":"page"},{"location":"interface/","page":"Interface","title":"Interface","text":"using CairoMakie\nusing LaTeXString ","category":"page"},{"location":"interface/","page":"Interface","title":"Interface","text":"(else the function does not exist, thanks to Requires.jl). A full description of this function is given in the corresponding docstring in the sidebar.","category":"page"},{"location":"interface/#GridSearch","page":"Interface","title":"GridSearch","text":"","category":"section"},{"location":"interface/","page":"Interface","title":"Interface","text":"it can sometimes be useful to evaluate the likelihood function over many points prior to optimising it, e.g. to find a good initial estimate or to just obtain data at many points for the purpose of visualisation. We provide functions for this, based on either a RegularGrid or an IrregularGrid.","category":"page"},{"location":"interface/","page":"Interface","title":"Interface","text":"A RegularGrid is a grid in which the grid for each parameter is uniformly spaced, so that the values for all parameter values to try fall on a lattice. An IrregularGrid allows for the parameters to take on whatever values you want, with the requirement that the parameter values to evaluate at are provided as a matrix with each column a different parameter set.","category":"page"},{"location":"interface/","page":"Interface","title":"Interface","text":"The function grid_search, after having defined a grid, can then be used for performing the grid search. The main method of interest is:","category":"page"},{"location":"interface/","page":"Interface","title":"Interface","text":"grid_search(prob::LikelihoodProblem, grid::AbstractGrid; save_vals=Val(false), parallel=Val(false))","category":"page"},{"location":"interface/","page":"Interface","title":"Interface","text":"Here, grid could be either a RegularGrid or an IrregularGrid. You can set save_vals=Val(true) if you want an array with all the likelihood function values, Val(false) otherwise. To enable multithreading, allowing for the evaluation of the function across different points via multiple threads, set parallel=Val(true), otherwise leave it as Val(false). The result of this grid search, if save_vals=Val(true), will be (sol, f_vals), where sol is a likelihood solution giving the parameters that gave to the highest likelihood, and f_res is the array of likelihoods at the corresponding parameters. If save_vals=Val(false), only sol is returned.","category":"page"},{"location":"interface/","page":"Interface","title":"Interface","text":"More example is given in the examples, and complete docstrings are provided in the sidebar.","category":"page"},{"location":"logistic/#Example-II:-Logistic-ordinary-differential-equation","page":"Example II: Logistic ordinary differential equation","title":"Example II: Logistic ordinary differential equation","text":"","category":"section"},{"location":"logistic/","page":"Example II: Logistic ordinary differential equation","title":"Example II: Logistic ordinary differential equation","text":"The following example comes from the first case study of Simpson and Maclaren (2022). First, load the packages we'll be using:","category":"page"},{"location":"logistic/","page":"Example II: Logistic ordinary differential equation","title":"Example II: Logistic ordinary differential equation","text":"using Random \nusing ProfileLikelihood\nusing Optimization \nusing OrdinaryDiffEq\nusing CairoMakie \nusing LaTeXStrings\nusing OptimizationNLopt\nusing Test","category":"page"},{"location":"logistic/","page":"Example II: Logistic ordinary differential equation","title":"Example II: Logistic ordinary differential equation","text":"Let us consider the logistic ordinary differential equation (ODE). For ODEs, our treatment is as follows: Let us have some ODE mathrm dymathrm dt = f(y t boldsymbol theta) for some parameters boldsymboltheta of interest. We will suppose that we have some data y_i^o at time t_i, i=1ldotsn, with initial condition y_0^o at time t_0=0, which we model according to a normal distribution y_i^o mid boldsymbol theta sim mathcal N(y_i(boldsymbol theta) sigma^2), i=01ldotsn, where y_i is a solution of the ODE at time t_i. This defines a likelihood that we can use for estimating the parameters.","category":"page"},{"location":"logistic/","page":"Example II: Logistic ordinary differential equation","title":"Example II: Logistic ordinary differential equation","text":"Let us now proceed with our example. We are considering mathrm dumathrm dt = lambda u(1-uK), u(0)=u_0, and our interest is in estimating (lambda K u_0), we will fix the standard deviation of the noise, sigma, at sigma=10. Note that the exact solution to this ODE is u(t) = Ku_0(K-u_0)mathrme^-lambda t + u_0.","category":"page"},{"location":"logistic/#Data-generation-and-setting-up-the-problem","page":"Example II: Logistic ordinary differential equation","title":"Data generation and setting up the problem","text":"","category":"section"},{"location":"logistic/","page":"Example II: Logistic ordinary differential equation","title":"Example II: Logistic ordinary differential equation","text":"The first step is to generate the data.","category":"page"},{"location":"logistic/","page":"Example II: Logistic ordinary differential equation","title":"Example II: Logistic ordinary differential equation","text":"using OrdinaryDiffEq, Random\nλ = 0.01\nK = 100.0\nu₀ = 10.0\nt = 0:100:1000\nσ = 10.0\n@inline function ode_fnc(u, p, t)\n    λ, K = p\n    du = λ * u * (1 - u / K)\n    return du\nend\n# Initial data is obtained by solving the ODE \ntspan = extrema(t)\np = (λ, K)\nprob = ODEProblem(ode_fnc, u₀, tspan, p)\nsol = solve(prob, Rosenbrock23(), saveat=t)\nRandom.seed!(2828)\nuᵒ = sol.u + σ * randn(length(t))","category":"page"},{"location":"logistic/","page":"Example II: Logistic ordinary differential equation","title":"Example II: Logistic ordinary differential equation","text":"Now having our data, we define the likelihood function.","category":"page"},{"location":"logistic/","page":"Example II: Logistic ordinary differential equation","title":"Example II: Logistic ordinary differential equation","text":"@inline function loglik_fnc2(θ, data, integrator)\n    λ, K, u₀ = θ\n    uᵒ, σ = data\n    integrator.p[1] = λ\n    integrator.p[2] = K\n    reinit!(integrator, u₀)\n    solve!(integrator)\n    return gaussian_loglikelihood(uᵒ, integrator.sol.u, σ, length(uᵒ))\nend","category":"page"},{"location":"logistic/","page":"Example II: Logistic ordinary differential equation","title":"Example II: Logistic ordinary differential equation","text":"Now we can define our problem. We constrain the problem so that 0 leq lambda leq 005, 50 leq K leq 150, and 0 leq u_0 leq 50.","category":"page"},{"location":"logistic/","page":"Example II: Logistic ordinary differential equation","title":"Example II: Logistic ordinary differential equation","text":"lb = [0.0, 50.0, 0.0] # λ, K, u₀\nub = [0.05, 150.0, 50.0]\nθ₀ = [λ, K, u₀]\nsyms = [:λ, :K, :u₀]\nprob = LikelihoodProblem(\n    loglik_fnc2, θ₀, ode_fnc, u₀, maximum(t); # Note that u₀ is just a placeholder IC in this case since we are estimating it\n    syms=syms,\n    data=(uᵒ, σ),\n    ode_parameters=[1.0, 1.0], # temp values for [λ, K]\n    ode_kwargs=(verbose=false, saveat=t),\n    f_kwargs=(adtype=Optimization.AutoFiniteDiff(),),\n    prob_kwargs=(lb=lb, ub=ub),\n    ode_alg=Rosenbrock23()\n)","category":"page"},{"location":"logistic/#Parameter-estimation","page":"Example II: Logistic ordinary differential equation","title":"Parameter estimation","text":"","category":"section"},{"location":"logistic/","page":"Example II: Logistic ordinary differential equation","title":"Example II: Logistic ordinary differential equation","text":"Now we find the MLEs.","category":"page"},{"location":"logistic/","page":"Example II: Logistic ordinary differential equation","title":"Example II: Logistic ordinary differential equation","text":"using OptimizationNLopt\nsol = mle(prob, NLopt.LD_LBFGS)\nLikelihoodSolution. retcode: Failure\nMaximum likelihood: -38.99053694428977\nMaximum likelihood estimates: 3-element Vector{Float64}\n     λ: 0.010438031266786045\n     K: 99.59921873132551\n     u₀: 8.098422110755225","category":"page"},{"location":"logistic/","page":"Example II: Logistic ordinary differential equation","title":"Example II: Logistic ordinary differential equation","text":"We can now profile. ","category":"page"},{"location":"logistic/","page":"Example II: Logistic ordinary differential equation","title":"Example II: Logistic ordinary differential equation","text":"prof = profile(prob, sol; alg=NLopt.LN_NELDERMEAD, parallel=false)\nProfileLikelihoodSolution. MLE retcode: Failure\nConfidence intervals: \n     95.0% CI for λ: (0.006400992274213644, 0.01786032876226762)\n     95.0% CI for K: (90.81154862835605, 109.54214763511888)\n     95.0% CI for u₀: (1.5919805025139593, 19.070831536649305)","category":"page"},{"location":"logistic/","page":"Example II: Logistic ordinary differential equation","title":"Example II: Logistic ordinary differential equation","text":"@test λ ∈ get_confidence_intervals(prof, :λ)\n@test K ∈ prof.confidence_intervals[2]\n@test u₀ ∈ get_confidence_intervals(prof, 3)","category":"page"},{"location":"logistic/","page":"Example II: Logistic ordinary differential equation","title":"Example II: Logistic ordinary differential equation","text":"We can visualise as we did before:","category":"page"},{"location":"logistic/","page":"Example II: Logistic ordinary differential equation","title":"Example II: Logistic ordinary differential equation","text":"using CairoMakie, LaTeXStrings\nfig = plot_profiles(prof;\n    latex_names=[L\"\\lambda\", L\"K\", L\"u_0\"],\n    show_mles=true,\n    shade_ci=true,\n    nrow=1,\n    ncol=3,\n    true_vals=[λ, K, u₀],\n    fig_kwargs=(fontsize=30, resolution=(2109.644f0, 444.242f0)),\n    axis_kwargs=(width=600, height=300))","category":"page"},{"location":"logistic/","page":"Example II: Logistic ordinary differential equation","title":"Example II: Logistic ordinary differential equation","text":"(Image: Logistic profiles)","category":"page"},{"location":"logistic/#Prediction-intervals","page":"Example II: Logistic ordinary differential equation","title":"Prediction intervals","text":"","category":"section"},{"location":"logistic/","page":"Example II: Logistic ordinary differential equation","title":"Example II: Logistic ordinary differential equation","text":"Let us now use these results to compute prediction intervals for u(t). Following Simpson and Maclaren (2022), the idea is to use the profile likelihood to construct another profile likelihood, called the profile-wise profile likelihood, that allows us to obtain prediction intervals for some prediction function q(boldsymbol theta). More detail is given in the mathematical details section.","category":"page"},{"location":"logistic/","page":"Example II: Logistic ordinary differential equation","title":"Example II: Logistic ordinary differential equation","text":"The first step is to define a function q(boldsymboltheta) that comptues our prediction given some parameters boldsymboltheta. The function in this case is simply:","category":"page"},{"location":"logistic/","page":"Example II: Logistic ordinary differential equation","title":"Example II: Logistic ordinary differential equation","text":"function prediction_function(θ, data)\n    λ, K, u₀ = θ\n    t = data\n    prob = ODEProblem(ode_fnc, u₀, extrema(t), (λ, K))\n    sol = solve(prob, Rosenbrock23(), saveat=t)\n    return sol.u\nend","category":"page"},{"location":"logistic/","page":"Example II: Logistic ordinary differential equation","title":"Example II: Logistic ordinary differential equation","text":"Note that the second argument data allows for extra parameters to be passed. To now obtain prediction intervals for sol.u, for each t, we define a large grid for t and use get_prediction_intervals:","category":"page"},{"location":"logistic/","page":"Example II: Logistic ordinary differential equation","title":"Example II: Logistic ordinary differential equation","text":"t_many_pts = LinRange(extrema(t)..., 1000)\nparameter_wise, union_intervals, all_curves, param_range =\n    get_prediction_intervals(prediction_function, prof,\n        t_many_pts; parallel=true)\n# t_many_pts is the `data` argument, it doesn't have to be time for other problems","category":"page"},{"location":"logistic/","page":"Example II: Logistic ordinary differential equation","title":"Example II: Logistic ordinary differential equation","text":"This function get_prediction_intervals has four outputs:","category":"page"},{"location":"logistic/","page":"Example II: Logistic ordinary differential equation","title":"Example II: Logistic ordinary differential equation","text":"parameter_wise: These are prediction intervals for the prediction at each point t, coming from the profile likelihood of each respective parameter:","category":"page"},{"location":"logistic/","page":"Example II: Logistic ordinary differential equation","title":"Example II: Logistic ordinary differential equation","text":"julia> parameter_wise\nDict{Int64, Vector{ConfidenceInterval{Float64, Float64}}} with 3 entries:\n  2 => [ConfidenceInterval{Float64, Float64}(6.45444, 12.0694, 0.95), ConfidenceInterval{Float64, Float64}(6.52931, 12.1545, 0.95), ConfidenceInterval{Float64, Float64}(6.60498, 12.24, 0.95), ConfidenceInterva…  3 => [ConfidenceInterval{Float64, Float64}(1.59389, 19.0709, 0.95), ConfidenceInterval{Float64, Float64}(1.621, 19.1773, 0.95), ConfidenceInterval{Float64, Float64}(1.64856, 19.2842, 0.95), ConfidenceInterva…  1 => [ConfidenceInterval{Float64, Float64}(1.86302, 17.5828, 0.95), ConfidenceInterval{Float64, Float64}(1.89596, 17.6768, 0.95), ConfidenceInterval{Float64, Float64}(1.92948, 17.7712, 0.95), ConfidenceInter…","category":"page"},{"location":"logistic/","page":"Example II: Logistic ordinary differential equation","title":"Example II: Logistic ordinary differential equation","text":"For example, parameter_wise[1] comes from varying lambda, with the parameters K and u_0 coming from optimising the likelihood function with lambda fixed.","category":"page"},{"location":"logistic/","page":"Example II: Logistic ordinary differential equation","title":"Example II: Logistic ordinary differential equation","text":"union_intervals: These are prediction intervals at each point t coming from taking the union of the intervals from the corresponding elements of parameter_wise.\nall_curves: The intervals come from taking extrema over many curves. This is a Dict mapping parameter indices to the curves that were used, with all_curves[i][j] being the set of curves for the ith parameter (e.g. i=1 is for lambda) and the jth parameter.\nparam_range: The curves come from evaluating the prediction function between the bounds of the confidence intervals for each parameter, and this output gives the parameters used, so that e.g. all_curves[i][j] uses param_range[i][j] for the value of the ith parameter.","category":"page"},{"location":"logistic/","page":"Example II: Logistic ordinary differential equation","title":"Example II: Logistic ordinary differential equation","text":"Let us now use these outputs to visualise the prediction intervals. First, let us extract the solution with the true parameter values and with the MLEs.","category":"page"},{"location":"logistic/","page":"Example II: Logistic ordinary differential equation","title":"Example II: Logistic ordinary differential equation","text":"exact_soln = prediction_function([λ, K, u₀], t_many_pts)\nmle_soln = prediction_function(get_mle(sol), t_many_pts)","category":"page"},{"location":"logistic/","page":"Example II: Logistic ordinary differential equation","title":"Example II: Logistic ordinary differential equation","text":"Now let us plot the prediction intervals coming from each parameter, and from the union of all intervals (not shown yet, see below).","category":"page"},{"location":"logistic/","page":"Example II: Logistic ordinary differential equation","title":"Example II: Logistic ordinary differential equation","text":"fig = Figure(fontsize=38, resolution=(1402.7681f0, 848.64404f0))\nalp = join('a':'z')\nlatex_names = [L\"\\lambda\", L\"K\", L\"u_0\"]\nfor i in 1:3\n    ax = Axis(fig[i < 3 ? 1 : 2, i < 3 ? i : 1], title=L\"(%$(alp[i])): Profile-wise PI for %$(latex_names[i])\",\n        titlealign=:left, width=600, height=300)\n    [lines!(ax, t_many_pts, all_curves[i][:, j], color=:grey) for j in eachindex(param_range[1])]\n    lines!(ax, t_many_pts, exact_soln, color=:red)\n    lines!(ax, t_many_pts, mle_soln, color=:blue, linestyle=:dash)\n    lines!(ax, t_many_pts, getindex.(parameter_wise[i], 1), color=:black, linewidth=3)\n    lines!(ax, t_many_pts, getindex.(parameter_wise[i], 2), color=:black, linewidth=3)\nend\nax = Axis(fig[2, 2], title=L\"(d):$ $ Union of all intervals\",\n    titlealign=:left, width=600, height=300)\nband!(ax, t_many_pts, getindex.(union_intervals, 1), getindex.(union_intervals, 2), color=:grey)\nlines!(ax, t_many_pts, getindex.(union_intervals, 1), color=:black, linewidth=3)\nlines!(ax, t_many_pts, getindex.(union_intervals, 2), color=:black, linewidth=3)\nlines!(ax, t_many_pts, exact_soln, color=:red)\nlines!(ax, t_many_pts, mle_soln, color=:blue, linestyle=:dash)","category":"page"},{"location":"logistic/","page":"Example II: Logistic ordinary differential equation","title":"Example II: Logistic ordinary differential equation","text":"To now assess the coverage of these intervals, we want to compare them to the interval coming from the full likelihood. We find this interval by taking a large number of parameters, and finding all of them for which the normalised log-likelihood exceeds the threshold -192. We then take the parameters that give a value exceeding this threshold, compute the prediction function at these values, and then take the extrema. The code below uses the function grid_search that evaluates the function at many points, and we describe this function in more detail in the next example.","category":"page"},{"location":"logistic/","page":"Example II: Logistic ordinary differential equation","title":"Example II: Logistic ordinary differential equation","text":"lb = get_lower_bounds(prob)\nub = get_upper_bounds(prob)\nN = 1e5\ngrid = [[lb[i] + (ub[i] - lb[i]) * rand() for _ in 1:N] for i in 1:3]\ngrid = permutedims(reduce(hcat, grid), (2, 1))\nig = IrregularGrid(lb, ub, grid)\ngs, lik_vals = grid_search(prob, ig; parallel=Val(true), save_vals=Val(true))\nlik_vals .-= get_maximum(sol) # normalised \nfeasible_idx = findall(lik_vals .> ProfileLikelihood.get_chisq_threshold(0.95)) # values in the confidence region \nparameter_evals = grid[:, feasible_idx]\nq = [prediction_function(θ, t_many_pts) for θ in eachcol(parameter_evals)]\nq_mat = reduce(hcat, q)\nq_lwr = minimum(q_mat; dims=2) |> vec\nq_upr = maximum(q_mat; dims=2) |> vec\nlines!(ax, t_many_pts, q_lwr, color=:magenta, linewidth=3)\nlines!(ax, t_many_pts, q_upr, color=:magenta, linewidth=3)","category":"page"},{"location":"logistic/","page":"Example II: Logistic ordinary differential equation","title":"Example II: Logistic ordinary differential equation","text":"(Image: Logistic prediction intervals)","category":"page"},{"location":"logistic/","page":"Example II: Logistic ordinary differential equation","title":"Example II: Logistic ordinary differential equation","text":"The first plot shows that the profile-wise prediction interval for lambda is quite large when t is small, and then small for large time. This makes sense since the large time solution is independent of lambda (the large time solution is u_s(t)=K). For K, we see that the profile-wise interval only becomes large for large time, which again makes sense. For u_0 we see similar behaviour as for lambda. Finally, taking the union over all the intervals, as is done in (d), shows that we fully enclose the solution coming from the MLE, as well as the true curve. The magenta curve shows the results from the full likelihood function, and is reasonably close to the approximate interval obtained from the union.","category":"page"},{"location":"logistic/#Just-the-code","page":"Example II: Logistic ordinary differential equation","title":"Just the code","text":"","category":"section"},{"location":"logistic/","page":"Example II: Logistic ordinary differential equation","title":"Example II: Logistic ordinary differential equation","text":"Here is all the code used for obtaining the results in this example, should you want a version that you can directly copy and paste.","category":"page"},{"location":"logistic/","page":"Example II: Logistic ordinary differential equation","title":"Example II: Logistic ordinary differential equation","text":"## Step 1: Generate the data and define the likelihood\nusing OrdinaryDiffEq, Random\nλ = 0.01\nK = 100.0\nu₀ = 10.0\nt = 0:100:1000\nσ = 10.0\n@inline function ode_fnc(u, p, t)\n    λ, K = p\n    du = λ * u * (1 - u / K)\n    return du\nend\n# Initial data is obtained by solving the ODE \ntspan = extrema(t)\np = (λ, K)\nprob = ODEProblem(ode_fnc, u₀, tspan, p)\nsol = solve(prob, Rosenbrock23(), saveat=t)\nRandom.seed!(2828)\nuᵒ = sol.u + σ * randn(length(t))\n@inline function loglik_fnc2(θ, data, integrator)\n    λ, K, u₀ = θ\n    uᵒ, σ = data\n    integrator.p[1] = λ\n    integrator.p[2] = K\n    reinit!(integrator, u₀)\n    solve!(integrator)\n    return gaussian_loglikelihood(uᵒ, integrator.sol.u, σ, length(uᵒ))\nend\n\n## Step 2: Define the problem \nusing Optimization\nlb = [0.0, 50.0, 0.0] # λ, K, u₀\nub = [0.05, 150.0, 50.0]\nθ₀ = [λ, K, u₀]\nsyms = [:λ, :K, :u₀]\nprob = LikelihoodProblem(\n    loglik_fnc2, θ₀, ode_fnc, u₀, maximum(t); # Note that u₀ is just a placeholder IC in this case since we are estimating it\n    syms=syms,\n    data=(uᵒ, σ),\n    ode_parameters=[1.0, 1.0], # temp values for [λ, K]\n    ode_kwargs=(verbose=false, saveat=t),\n    f_kwargs=(adtype=Optimization.AutoFiniteDiff(),),\n    prob_kwargs=(lb=lb, ub=ub),\n    ode_alg=Rosenbrock23()\n)\n\n## Step 3: Compute the MLE \nusing OptimizationNLopt\nsol = mle(prob, NLopt.LD_LBFGS)\n\n## Step 4: Profile \nprof = profile(prob, sol; alg=NLopt.LN_NELDERMEAD, parallel=false)\n\n## Step 5: Visualise \nusing CairoMakie, LaTeXStrings\nfig = plot_profiles(prof;\n    latex_names=[L\"\\lambda\", L\"K\", L\"u_0\"],\n    show_mles=true,\n    shade_ci=true,\n    nrow=1,\n    ncol=3,\n    true_vals=[λ, K, u₀],\n    fig_kwargs=(fontsize=30, resolution=(2109.644f0, 444.242f0)),\n    axis_kwargs=(width=600, height=300))\n\n## Step 6: Get prediction intervals, compare to evaluating at many points \nfunction prediction_function(θ, data)\n    λ, K, u₀ = θ\n    t = data\n    prob = ODEProblem(ode_fnc, u₀, extrema(t), (λ, K))\n    sol = solve(prob, Rosenbrock23(), saveat=t)\n    return sol.u\nend\nt_many_pts = LinRange(extrema(t)..., 1000)\nparameter_wise, union_intervals, all_curves, param_range =\n    get_prediction_intervals(prediction_function, prof,\n        t_many_pts; parallel=true)\n\n## Step 7: Plot the prediction intervals \n# Get the exact solution and MLE solutions first for comparison \nexact_soln = prediction_function([λ, K, u₀], t_many_pts)\nmle_soln = prediction_function(get_mle(sol), t_many_pts)\n\n# Now plot the prediction intervals\nfig = Figure(fontsize=38, resolution=(1402.7681f0, 848.64404f0))\nalp = join('a':'z')\nlatex_names = [L\"\\lambda\", L\"K\", L\"u_0\"]\nfor i in 1:3\n    ax = Axis(fig[i < 3 ? 1 : 2, i < 3 ? i : 1], title=L\"(%$(alp[i])): Profile-wise PI for %$(latex_names[i])\",\n        titlealign=:left, width=600, height=300)\n    [lines!(ax, t_many_pts, all_curves[i][:, j], color=:grey) for j in eachindex(param_range[1])]\n    lines!(ax, t_many_pts, exact_soln, color=:red)\n    lines!(ax, t_many_pts, mle_soln, color=:blue, linestyle=:dash)\n    lines!(ax, t_many_pts, getindex.(parameter_wise[i], 1), color=:black, linewidth=3)\n    lines!(ax, t_many_pts, getindex.(parameter_wise[i], 2), color=:black, linewidth=3)\nend\nax = Axis(fig[2, 2], title=L\"(d):$ $ Union of all intervals\",\n    titlealign=:left, width=600, height=300)\nband!(ax, t_many_pts, getindex.(union_intervals, 1), getindex.(union_intervals, 2), color=:grey)\nlines!(ax, t_many_pts, getindex.(union_intervals, 1), color=:black, linewidth=3)\nlines!(ax, t_many_pts, getindex.(union_intervals, 2), color=:black, linewidth=3)\nlines!(ax, t_many_pts, exact_soln, color=:red)\nlines!(ax, t_many_pts, mle_soln, color=:blue, linestyle=:dash)\n\n# We can now compare these intervals to the one obtained from the full likelihood. We re-use our grid_search code (see the next example) to evaluate at many points \nlb = get_lower_bounds(prob)\nub = get_upper_bounds(prob)\nN = 1e5\ngrid = [[lb[i] + (ub[i] - lb[i]) * rand() for _ in 1:N] for i in 1:3]\ngrid = permutedims(reduce(hcat, grid), (2, 1))\nig = IrregularGrid(lb, ub, grid)\ngs, lik_vals = grid_search(prob, ig; parallel=Val(true), save_vals=Val(true))\nlik_vals .-= get_maximum(sol) # normalised \nfeasible_idx = findall(lik_vals .> ProfileLikelihood.get_chisq_threshold(0.95)) # values in the confidence region \nparameter_evals = grid[:, feasible_idx]\nq = [prediction_function(θ, t_many_pts) for θ in eachcol(parameter_evals)]\nq_mat = reduce(hcat, q)\nq_lwr = minimum(q_mat; dims=2) |> vec\nq_upr = maximum(q_mat; dims=2) |> vec\nlines!(ax, t_many_pts, q_lwr, color=:magenta, linewidth=3)\nlines!(ax, t_many_pts, q_upr, color=:magenta, linewidth=3)","category":"page"},{"location":"math/#Mathematical-and-Implementation-Details","page":"Mathematical and Implementation Details","title":"Mathematical and Implementation Details","text":"","category":"section"},{"location":"math/","page":"Mathematical and Implementation Details","title":"Mathematical and Implementation Details","text":"We now give some of the mathematical and implementation details used in this package, namely for computing the profile likelihood function and for computing prediction intervals.","category":"page"},{"location":"math/#Computing-the-profile-likelihood-function","page":"Mathematical and Implementation Details","title":"Computing the profile likelihood function","text":"","category":"section"},{"location":"math/","page":"Mathematical and Implementation Details","title":"Mathematical and Implementation Details","text":"Let us start by giving a mathematical description of the method that we use for computing the profile log-likelihood function. Suppose that we have a parameter vector boldsymboltheta that we partition as boldsymbol theta = (boldsymbolpsi boldsymbol omega) - boldsymbolpsi is either a scalar, psi, or a 2-vector, (psi varphi). We suppose that we have a likelihood function mathcal L(boldsymbol theta) equiv mathcal L(boldsymbolpsi boldsymbol omega) so that the normalised profile log-likelihood function for boldsymbolpsi is defined as ","category":"page"},{"location":"math/","page":"Mathematical and Implementation Details","title":"Mathematical and Implementation Details","text":"hatell_p(boldsymbolpsi) = sup_boldsymbol omega in Omega mid boldsymbolpsi leftell(boldsymbolpsi boldsymbolomega) - ell^*right","category":"page"},{"location":"math/","page":"Mathematical and Implementation Details","title":"Mathematical and Implementation Details","text":"where Omega is the parameter space for boldsymbol omega, ell(boldsymbolpsiboldsymbolomega) = log mathcal L(boldsymbolpsi boldsymbol omega), and ell^* = ell(hatboldsymbol theta), where boldsymbol theta are the MLEs for boldsymbol theta. This definition of hatell_p(boldsymbolpsi) induces a function boldsymbolomega^*(boldsymbolpsi) depending on boldsymbolpsi that gives the values of boldsymbol omega leading to the supremum above, i.e. ","category":"page"},{"location":"math/","page":"Mathematical and Implementation Details","title":"Mathematical and Implementation Details","text":"ell(boldsymbolpsi boldsymbolomega^star(psi)) = sup_boldsymbol omega in Omega mid boldsymbolpsi leftell(boldsymbolpsi boldsymbolomega) - ell^starright ","category":"page"},{"location":"math/","page":"Mathematical and Implementation Details","title":"Mathematical and Implementation Details","text":"To compute hatell_p(boldsymbolpsi), then, requires a way to efficiently compute the omega^*(boldsymbolpsi), and requires knowing where to stop computing. Where we stop computing the profile likelihood is simply when hatell_p(psi)  -chi_k1-alpha^22, where alpha is the significance level and k=1 if boldsymbolpsi = psi and k=2 if boldsymbolpsi = (psivarphi). This motivates a iterative algorithm, where we start at the MLE and expand outwards.","category":"page"},{"location":"math/#Univariate-profile-likelihoods","page":"Mathematical and Implementation Details","title":"Univariate profile likelihoods","text":"","category":"section"},{"location":"math/","page":"Mathematical and Implementation Details","title":"Mathematical and Implementation Details","text":"We first describe our implementation for a univariate profile, in which case boldsymbolpsi = psi. The basic summary of this procedure is that we simply step to the left and to the right of the MLE, continuing until we reach the threshold.","category":"page"},{"location":"math/","page":"Mathematical and Implementation Details","title":"Mathematical and Implementation Details","text":"We describe how we evaluate the function to the right of the MLE – the case of going to the left is identical. First, we define psi_1 = hatpsi, where hatpsi is the MLE for psi. This defines boldsymbolomega_1 = boldsymbolomega^star(psi_1), which in this case just gives the MLE hatboldsymboltheta = (hatpsi boldsymbolomega_1) by definition. The value of the normalised profile log-likelihood here is simply hatell_1 = hatell(psi_1) = 0. Then, defining some step size Deltapsi, we define psi_2 = psi_1 + Delta psi, and in general psi_j+1 = psi_j + Delta psi, we need to estimate boldsymbolomega_2 = boldsymbol omega^*(psi_2). We do this by starting an optimiser at the initial estimate boldsymbolomega_2 = boldsymbolomega_1 and then using this initial estimate to produce a refined value of boldsymbolomega_2 that we take as its true value. In particular, each boldsymbolomega_j comes from starting the optimiser at the previous boldsymbolomega_j-1, and the value for hatell_j = hatell(psi_j) comes from the value of the likelihood at (psi_j boldsymbolomega_j). The same holds when going to the left except with psi_j+1 = psi_j - Deltapsi, and then rearranging the indices j when combining the results to the left and to the right.  At each step, we check if hatell_j  -chi_11-alpha^22 and, if so, we terminate. ","category":"page"},{"location":"math/","page":"Mathematical and Implementation Details","title":"Mathematical and Implementation Details","text":"Once we have terminated the algorithm, we need to obtain the confidence intervals. To do this, we fit a spline to the data (psi_j hatell_j), and use a bisection algorithm over the two intervals (min_j psi_j hatpsi) and (hatpsi max_jpsi_j), to find where hatell_j = -chi_1-alpha^22. This leads to two solutions (L U) that we take together to give the confidence interval for psi. ","category":"page"},{"location":"math/","page":"Mathematical and Implementation Details","title":"Mathematical and Implementation Details","text":"This is all done for each parameter.","category":"page"},{"location":"math/","page":"Mathematical and Implementation Details","title":"Mathematical and Implementation Details","text":"Note that a better method for initialising the optimisation for boldsymbolomega_j may be to use e.g. linear interpolation for the previous two values, boldsymbolomega_j-1 and boldsymbolomega_j-2 (with special care for the bounds of the parameters). We provide support for this, letting boldsymbolomega_j = boldsymbolomega_j-2(psi_j-1 - psi_j) + boldsymbolomega_j-1(psi_j - psi_j-2)  (psi_j-1 - psi_j-2). See the next_initial_estimate_method option in ?profile.","category":"page"},{"location":"math/#Bivariate-profile-likelihoods","page":"Mathematical and Implementation Details","title":"Bivariate profile likelihoods","text":"","category":"section"},{"location":"math/","page":"Mathematical and Implementation Details","title":"Mathematical and Implementation Details","text":"Now we describe the implementation for a bivariate profile. In this case, there are many possibilities as instead of only having to think about going to the left and to the right, we could think about how we step away from the MLE in each direction, and how we want to stop iterating. This package currently has a basic implementation that we describe below, where we simply step out from the MLE in layers. In what follows, we let boldsymbolpsi = (psi varphi).","category":"page"},{"location":"math/","page":"Mathematical and Implementation Details","title":"Mathematical and Implementation Details","text":"To start, we suppose that we are on some square grid with integer coordinates (i j)  i j = -N -N+1 ldots 0 ldots N-1 N, and we suppose that (0 0) refers to the MLE. We call the coordinate (0 0) the zeroth layer, denoted L_0 = (0 0). The jth layer, L_j, is defined to wrap around L_j-1. For example, L_1 wraps around (0 0) so that L_1 = (-1-1)(0-1)(1-1)(10)(11)(01)(-11)(-10). Note that L_j = 8j. The idea here is that we can solve the required optimisation problems at each (i j) in L_j, accelerating the solutions by making use of information at L_j-1 to define initial estimates for each optimisation problem. Layers are implemented via ProfileLikelihood.LayerIterator. ","category":"page"},{"location":"math/","page":"Mathematical and Implementation Details","title":"Mathematical and Implementation Details","text":"We also need to prescribe the parameter values that are defined at each coordinate. We suppose that we have bounds psi_L leq psi leq psi_U and varphi_L leq varphi leq varphi_U for psi and varphi, and that we have MLEs hatpsi and hatvarphi for psi and varphi, respectively. We then define Deltapsi_L = (hatpsi - psi_L)N, Deltapsi_R = (psi_U - hatpsi)N, Deltavarphi_L = (hatvarphi-varphi_L)N, and Deltavarphi_R = (varphi_U - hatvarphi)N. With these definitions, we let","category":"page"},{"location":"math/","page":"Mathematical and Implementation Details","title":"Mathematical and Implementation Details","text":"psi_j = begincases psi_0 + jDeltapsi_R  j  0  hatpsi  j = 0  psi_0 - jDeltapsi_L  j  0 endcases qquad varphi_j = begincases varphi_0 + jDeltavarphi_R  j0  hatvarphi  j = 0  varphi_0 - jDeltavarphi_L  j  0 endcases ","category":"page"},{"location":"math/","page":"Mathematical and Implementation Details","title":"Mathematical and Implementation Details","text":"We thus associate the parameter values (psi_i varphi_j) with the coordinate (i j). We will similarly associate hatell_p(psi_i varphi_j) = hatell(psi_i varphi_j boldsymbolomega^*(psi_i varphi_j)) with the coordinate (i j), and lastly boldsymbolomega_ij = boldsymbolomega^*(psi_i varphi_j). ","category":"page"},{"location":"math/","page":"Mathematical and Implementation Details","title":"Mathematical and Implementation Details","text":"So, the procedure is as follows: First, we start at the layer L_1 and compute hatell_p ij equiv hatell_p(psi_i varphi_j) for each (i j) in L_1, starting each initial estimate boldsymbolomega_ij at boldsymbolomega_00, which is just the MLE. For each (i j), the parameter values we use are (psi_i varphi_j). Once we have evaluated at each (i j) in L_1, we can move into L_2, making use of the same procedure. In this case, though, there are a few choices that we could make for choosing an initial value for boldsymbolomega_ij, (i j) in L_2. As defined in set_next_initial_estimate!, we currently have three options: ","category":"page"},{"location":"math/","page":"Mathematical and Implementation Details","title":"Mathematical and Implementation Details","text":":mle: The simplest choice is to simply start boldsymbolomega_ij at boldsymbolomega_00.\n:nearest: An alternative choice is to simply start boldsymbolomega_ij at boldsymbolomega_ij, where (i j) in L_1 is the nearest coordinate to (i j) in L_2. For example, if (i j) = (-2 0) then (i j) = (-1 0), and if (i j) = (2 2) then (i j) = (1 1).\n:interp: An alternative choice, which is currently the slowest (but could be made faster in the future, it just needs some work), is to maintain a linear interpolant across the data from L_0 and L_1 (i.e., all the previous layers, so L_j uses data from L_0 L_1 ldots L_j-1), and use extrapolation to obtain a new value for boldsymbolomega_ij from the linear interpolant evaluated at (psi_i varphi_j). ","category":"page"},{"location":"math/","page":"Mathematical and Implementation Details","title":"Mathematical and Implementation Details","text":"This procedure allows us to easily solve our optimisation problems for any given L_j. To decide how to terminate, we will simply terminate when we find that hatell_p ij  -chi_2 1-alpha^22 for all (i j) in some layer, i.e. we do not terminate if any hatell_p ij  chi_2 1-alpha^22. This choice means that we stop once we have found a box, i.e. a layer, that bounds the confidence region.","category":"page"},{"location":"math/","page":"Mathematical and Implementation Details","title":"Mathematical and Implementation Details","text":"Now having a box that bounds the confidence region (assuming it to be simply connected), we need to find the boundary of the confidence region. Note that we define the confidence region as mathcal C = (psi_i varphi_j)  hatell_p(psi_i varphi_j)  -chi_21-alpha^22, and we are trying to now find the boundary partialmathcal C. We currently provide two methods for this, as defined in get_confidence_regions:","category":"page"},{"location":"math/","page":"Mathematical and Implementation Details","title":"Mathematical and Implementation Details","text":":contour: Here we use Contours.jl, defining a contour at level -chi_21-alpha^22, to find the contour.\n:delaunay: This method uses DelaunayTriangulation.jl, defining a Delaunay triangulation over the entire grid of (psi_i varphi_j) in the bounding box, making use of triangulation contouring to find the contours. In particular, we take a set of triangles mathcal T that triangulate the bounding box, and then we iterate over all edges mathcal E in the triangulation. Assuming that hatell_p is linear over each triangle, we can assume that hatell_p increases linearly over an edge. Thus, if the value of the profile at one vertex of an edge is below -chi_21-alpha^22 and the other is above -chi_21-alpha^22, then there must be a point where hatell_p = -chi_21-alpha^22 on this edge (making use of our linearity assumption). This thus defines a point on the boundary of partialmathcal C. We do this for each edge, giving us a complete boundary.","category":"page"},{"location":"math/","page":"Mathematical and Implementation Details","title":"Mathematical and Implementation Details","text":"These procedures give us the complete solution.","category":"page"},{"location":"math/#Computing-prediction-intervals","page":"Mathematical and Implementation Details","title":"Computing prediction intervals","text":"","category":"section"},{"location":"math/","page":"Mathematical and Implementation Details","title":"Mathematical and Implementation Details","text":"Our method for computing prediction intervals follows Simpson and Maclaren (2022), as does our description that follows. This method is nice as it provides a means for sensitivity analysis, enabling the attribution of components of uncertainty in some prediction function q(boldsymbolpsi boldsymbol omega) (with boldsymbolpsi the interest parameter and boldsymbolomega the nuisance parameters as above) to individual parameters (or pairs, in the case of a bivariate profile). The resulting intervals are called profile-wise intervals, with the predictions themselves called parameter-based, profile-wise predictions or profile-wise predictions.","category":"page"},{"location":"math/","page":"Mathematical and Implementation Details","title":"Mathematical and Implementation Details","text":"The idea is to take a set of profile likelihoods and the confidence intervals obtained from each, and then pushing those into a prediction function that we then use to obtain prediction intervals, making heavy use of the transformation invariance property of MLEs.","category":"page"},{"location":"math/","page":"Mathematical and Implementation Details","title":"Mathematical and Implementation Details","text":"So, let us start with some prediction function q(boldsymbolpsi boldsymbol omega), and recall that the profile likelihood function for boldsymbolpsi induces a function boldsymbolomega^star(boldsymbolpsi). The profile-wise likelihood for q, given the set of values (boldsymbolpsi boldsymbolomega^star(boldsymbolpsi)), is defined by ","category":"page"},{"location":"math/","page":"Mathematical and Implementation Details","title":"Mathematical and Implementation Details","text":"hatell_pleft(qleft(boldsymbolpsi boldsymbolomega^star(boldsymbolpsi)right) = qright) = sup_boldsymbolpsi mid  qleft(boldsymbolpsi boldsymbolomega^star(boldsymbolpsi)right) = q hatell_p(boldsymbolpsi) ","category":"page"},{"location":"math/","page":"Mathematical and Implementation Details","title":"Mathematical and Implementation Details","text":"Note that if q(boldsymbolpsi boldsymbolomega^star(boldsymbolpsi)) is injective, there is only one such boldsymbolpsi such that qleft(boldsymbolpsi boldsymbolomega^star(psi)right) = q for any given q in which case the profile-wise likelihood for q (based on boldsymbolpsi) is simply hatell_p(boldsymbolpsi). This definition is intuitive, recalling that the profile likelihood comes from a definition like the above except with the likelihood function on the right, so profile-wise likelihoods come from profile likelihoods.  Using this definition, and using the transformation invariance property of the MLE, confidence sets for psi directly translate into confidence sets for q, in particular to find a 100(1-alpha) prediction interval for q we need only evaluate q for psi inside its confidence interval.","category":"page"},{"location":"math/","page":"Mathematical and Implementation Details","title":"Mathematical and Implementation Details","text":"Let us now describe the extra details involved in obtaining these prediction intervals, in particular what we are doing in the get_prediction_intervals function. For this, we imagine that q is scalar valued, but the description below can be easily extended to the vector case (just apply the idea to each component – see the logistic ODE example). We also only explain this for a single parameter boldsymbolpsi, but we describe how we use the results for each parameter to obtain a more conservative interval.","category":"page"},{"location":"math/","page":"Mathematical and Implementation Details","title":"Mathematical and Implementation Details","text":"The first step is to evaluate the family of curves. Here we describe the evaluation for a scalar parameter of interest, boldsymbolpsi = psi, but note that the case of a bivariate parameter of interest is similar (just use a confidence region instead of a confidence interval). If we suppose that the confidence interval for psi is (psi_L psi_U), we define psi_j = psi_L + (j-1)(psi_U - psi_L)(n_psi - 1), j=1ldotsn_psi – this is a set of n_psi equally spaced points between the interval limits. For each psi_j we need to then compute boldsymbolomega^star(psi_j). Rather than re-optimise, we use the data from our profile likelihoods, where we have stored values for (psi boldsymbolomega^star(psi)) to define a continuous function boldsymbolomega^star(psi) via linear interpolation. Using this linear interpolant we can thus compute boldsymbolomega^star(psi_j) for each gridpoint psi_j. We can therefore compute boldsymboltheta_j = (psi_j boldsymbolomega^star(psi_j)) so that we can evaluate the prediction function at each psi_j, q_j = q(boldsymboltheta_j). ","category":"page"},{"location":"math/","page":"Mathematical and Implementation Details","title":"Mathematical and Implementation Details","text":"We now have a sample q_1 ldots q_n_psi. If we let q_L = min_j=1^n_psi q_j and q_U = max_j=1^n_psi q_j, then our prediction interval is (q_L q_U). To be more specific, this is the profile-wise interval for q given the basis (boldsymbolpsi boldsymbolomega^star(boldsymbolpsi)).","category":"page"},{"location":"math/","page":"Mathematical and Implementation Details","title":"Mathematical and Implementation Details","text":"We have now described how prediction intervals are obtained based on a single parameter. Suppose we do this for a collection of parameters boldsymbolpsi^1 ldots boldsymbolpsi^d (e.g. if boldsymboltheta = (D lambda K), then we might have computed profiles for psi^1=D, psi^2=lambda, and psi^3=K), giving d different intervals for each boldsymbolpsi^i, say (q_L^i q_U^i)_i=1^d. We can take the union of these intervals to get a more conservative interval for the prediction, giving the new interval (min_i=1^d q_L^i max_i=1^d q_U^i).","category":"page"},{"location":"exponential/#Example-III:-Linear-exponential-ODE-and-grid-searching","page":"Example III: Linear exponential ODE and grid searching","title":"Example III: Linear exponential ODE and grid searching","text":"","category":"section"},{"location":"exponential/","page":"Example III: Linear exponential ODE and grid searching","title":"Example III: Linear exponential ODE and grid searching","text":"Now we consider mathrm dymathrm dt = lambda y, y(0) = y_0. This has solution y(t) = y_0mathrme^lambda t. First, load the packages we'll be using:","category":"page"},{"location":"exponential/","page":"Example III: Linear exponential ODE and grid searching","title":"Example III: Linear exponential ODE and grid searching","text":"using OrdinaryDiffEq\nusing ProfileLikelihood\nusing Optimization \nusing CairoMakie \nusing LaTeXStrings \nusing Random\nusing Distributions\nusing MuladdMacro\nusing LoopVectorization\nusing LatinHypercubeSampling \nusing OptimizationOptimJL\nusing OptimizationNLopt\nusing Test","category":"page"},{"location":"exponential/#Setting-up-the-problem","page":"Example III: Linear exponential ODE and grid searching","title":"Setting up the problem","text":"","category":"section"},{"location":"exponential/","page":"Example III: Linear exponential ODE and grid searching","title":"Example III: Linear exponential ODE and grid searching","text":"Let us start by defining the data and the likelihood problem:","category":"page"},{"location":"exponential/","page":"Example III: Linear exponential ODE and grid searching","title":"Example III: Linear exponential ODE and grid searching","text":"## Step 1: Generate some data for the problem and define the likelihood\nRandom.seed!(2992999)\nλ = -0.5\ny₀ = 15.0\nσ = 0.5\nT = 5.0\nn = 450\nΔt = T / n\nt = [j * Δt for j in 0:n]\ny = y₀ * exp.(λ * t)\nyᵒ = y .+ [0.0, rand(Normal(0, σ), n)...]\n@inline function ode_fnc(u, p, t)\n    local λ\n    λ = p\n    du = λ * u\n    return du\nend\nusing LoopVectorization, MuladdMacro\n@inline function _loglik_fnc(θ::AbstractVector{T}, data, integrator) where {T}\n    local yᵒ, n, λ, σ, u0\n    yᵒ, n = data\n    λ, σ, u0 = θ\n    integrator.p = λ\n    ## Now solve the problem \n    reinit!(integrator, u0)\n    solve!(integrator)\n    if !SciMLBase.successful_retcode(integrator.sol)\n        return typemin(T)\n    end\n    ℓ = -0.5(n + 1) * log(2π * σ^2)\n    s = zero(T)\n    @turbo @muladd for i in eachindex(yᵒ, integrator.sol.u)\n        s = s + (yᵒ[i] - integrator.sol.u[i]) * (yᵒ[i] - integrator.sol.u[i])\n    end\n    ℓ = ℓ - 0.5s / σ^2\nend\n\n## Step 2: Define the problem\nθ₀ = [-1.0, 0.5, 19.73] # will be replaced anyway\nlb = [-10.0, 1e-6, 0.5]\nub = [10.0, 10.0, 25.0]\nsyms = [:λ, :σ, :y₀]\nprob = LikelihoodProblem(\n    loglik_fnc, θ₀, ode_fnc, y₀, (0.0, T);\n    syms=syms,\n    data=(yᵒ, n),\n    ode_parameters=1.0, # temp value for λ\n    ode_kwargs=(verbose=false, saveat=t),\n    f_kwargs=(adtype=Optimization.AutoFiniteDiff(),),\n    prob_kwargs=(lb=lb, ub=ub),\n    ode_alg=Tsit5()\n)","category":"page"},{"location":"exponential/#Grid-searching","page":"Example III: Linear exponential ODE and grid searching","title":"Grid searching","text":"","category":"section"},{"location":"exponential/","page":"Example III: Linear exponential ODE and grid searching","title":"Example III: Linear exponential ODE and grid searching","text":"Let us now give an alternative way of exploring this likelihood function. We have been using mle, but we also provide some capability for using a grid search, which can sometimes be useful for e.g. visualising a likelihood function or obtaining initial estmiates for parameters (although it scales terribly for problems with more than even three parameters). Below we define a RegularGrid, a regular grid for each parameter:","category":"page"},{"location":"exponential/","page":"Example III: Linear exponential ODE and grid searching","title":"Example III: Linear exponential ODE and grid searching","text":"regular_grid = RegularGrid(lb, ub, 50) # resolution can also be given as a vector for each parameter","category":"page"},{"location":"exponential/","page":"Example III: Linear exponential ODE and grid searching","title":"Example III: Linear exponential ODE and grid searching","text":"We can now use this grid to evaluate the likelihood function at each point, and then return the maximum values (use save_vals=Val(true) if you want all the computed values as an array, given as a second argument; also see ?grid_search). (You can also set parallel = Val(true) so that the computation is done with multithreading.)","category":"page"},{"location":"exponential/","page":"Example III: Linear exponential ODE and grid searching","title":"Example III: Linear exponential ODE and grid searching","text":"gs = grid_search(prob, regular_grid)\nLikelihoodSolution. retcode: Success\nMaximum likelihood: -547.9579886200935\nMaximum likelihood estimates: 3-element Vector{Float64}\n     λ: -0.612244897959183\n     σ: 0.816327448979592\n     y₀: 16.5","category":"page"},{"location":"exponential/","page":"Example III: Linear exponential ODE and grid searching","title":"Example III: Linear exponential ODE and grid searching","text":"You could also use an irregular grid, defining some grid as a matrix where each column is a set of parameter values, or a vector of vectors. Here is an example using LatinHypercubeSampling.jl to avoid the dimensionality issue (although in practice we would have to be more careful with choosing the parameter bounds to get good coverage of the parameter space).","category":"page"},{"location":"exponential/","page":"Example III: Linear exponential ODE and grid searching","title":"Example III: Linear exponential ODE and grid searching","text":"using LatinHypercubeSampling\nd = 3\ngens = 1000\nplan, _ = LHCoptim(500, d, gens)\nnew_lb = [-2.0, 0.05, 10.0]\nnew_ub = [2.0, 0.2, 20.0]\nbnds = [(new_lb[i], new_ub[i]) for i in 1:d]\nparameter_vals = Matrix(scaleLHC(plan, bnds)') # transpose so that a column is a parameter set \nirregular_grid = IrregularGrid(lb, ub, parameter_vals)\ngs_ir, loglik_vals_ir = grid_search(prob, irregular_grid; save_vals=Val(true), parallel = Val(true))","category":"page"},{"location":"exponential/","page":"Example III: Linear exponential ODE and grid searching","title":"Example III: Linear exponential ODE and grid searching","text":"LikelihoodSolution. retcode: Success\nMaximum likelihood: -1729.7407123603484\nMaximum likelihood estimates: 3-element Vector{Float64}\n     λ: -0.5090180360721444\n     σ: 0.19368737474949904\n     y₀: 15.791583166332664","category":"page"},{"location":"exponential/","page":"Example III: Linear exponential ODE and grid searching","title":"Example III: Linear exponential ODE and grid searching","text":"max_lik, max_idx = findmax(loglik_vals_ir)\n@test max_lik == PL.get_maximum(gs_ir)\n@test parameter_vals[:, max_idx] ≈ PL.get_mle(gs_ir)","category":"page"},{"location":"exponential/","page":"Example III: Linear exponential ODE and grid searching","title":"Example III: Linear exponential ODE and grid searching","text":"(If you just want to try many points for starting your optimiser, see the optimiser in MultistartOptimization.jl.)","category":"page"},{"location":"exponential/#Parameter-estimation","page":"Example III: Linear exponential ODE and grid searching","title":"Parameter estimation","text":"","category":"section"},{"location":"exponential/","page":"Example III: Linear exponential ODE and grid searching","title":"Example III: Linear exponential ODE and grid searching","text":"Now let's use mle. We will restart the initial guess to use the estimates from our grid search.","category":"page"},{"location":"exponential/","page":"Example III: Linear exponential ODE and grid searching","title":"Example III: Linear exponential ODE and grid searching","text":"prob = update_initial_estimate(prob, gs)\nsol = mle(prob, Optim.LBFGS())","category":"page"},{"location":"exponential/","page":"Example III: Linear exponential ODE and grid searching","title":"Example III: Linear exponential ODE and grid searching","text":"Now we profile.","category":"page"},{"location":"exponential/","page":"Example III: Linear exponential ODE and grid searching","title":"Example III: Linear exponential ODE and grid searching","text":"prof = profile(prob, sol; alg=NLopt.LN_NELDERMEAD, parallel = true)","category":"page"},{"location":"exponential/","page":"Example III: Linear exponential ODE and grid searching","title":"Example III: Linear exponential ODE and grid searching","text":"ProfileLikelihoodSolution. MLE retcode: Success\nConfidence intervals: \n     95.0% CI for λ: (-0.51091362373969, -0.49491369219060505)\n     95.0% CI for σ: (0.49607205632240814, 0.5652591835193789)\n     95.0% CI for y₀: (14.98587355568687, 15.305179849533756)","category":"page"},{"location":"exponential/","page":"Example III: Linear exponential ODE and grid searching","title":"Example III: Linear exponential ODE and grid searching","text":"@test λ ∈ get_confidence_intervals(prof, :λ)\n@test σ ∈ get_confidence_intervals(prof[:σ])\n@test y₀ ∈ get_confidence_intervals(prof, 3)","category":"page"},{"location":"exponential/#Visualisation","page":"Example III: Linear exponential ODE and grid searching","title":"Visualisation","text":"","category":"section"},{"location":"exponential/","page":"Example III: Linear exponential ODE and grid searching","title":"Example III: Linear exponential ODE and grid searching","text":"Finally, we can visualise the profiles:","category":"page"},{"location":"exponential/","page":"Example III: Linear exponential ODE and grid searching","title":"Example III: Linear exponential ODE and grid searching","text":"fig = plot_profiles(prof; nrow=1, ncol=3,\n    latex_names=[L\"\\lambda\", L\"\\sigma\", L\"y_0\"],\n    true_vals=[λ, σ, y₀],\n    fig_kwargs=(fontsize=30, resolution=(2109.644f0, 444.242f0)),\n    axis_kwargs=(width=600, height=300))","category":"page"},{"location":"exponential/","page":"Example III: Linear exponential ODE and grid searching","title":"Example III: Linear exponential ODE and grid searching","text":"(Image: Linear exponential profiles)","category":"page"},{"location":"exponential/#Just-the-code","page":"Example III: Linear exponential ODE and grid searching","title":"Just the code","text":"","category":"section"},{"location":"exponential/","page":"Example III: Linear exponential ODE and grid searching","title":"Example III: Linear exponential ODE and grid searching","text":"Here is all the code used for obtaining the results in this example, should you want a version that you can directly copy and paste.","category":"page"},{"location":"exponential/","page":"Example III: Linear exponential ODE and grid searching","title":"Example III: Linear exponential ODE and grid searching","text":"## Step 1: Generate some data for the problem and define the likelihood\nusing OrdinaryDiffEq, Random, Distributions, LoopVectorization, MuladdMacro\nRandom.seed!(2992999)\nλ = -0.5\ny₀ = 15.0\nσ = 0.5\nT = 5.0\nn = 450\nΔt = T / n\nt = [j * Δt for j in 0:n]\ny = y₀ * exp.(λ * t)\nyᵒ = y .+ [0.0, rand(Normal(0, σ), n)...]\n@inline function ode_fnc(u, p, t)\n    local λ\n    λ = p\n    du = λ * u\n    return du\nend\n@inline function _loglik_fnc(θ::AbstractVector{T}, data, integrator) where {T}\n    local yᵒ, n, λ, σ, u0\n    yᵒ, n = data\n    λ, σ, u0 = θ\n    integrator.p = λ\n    ## Now solve the problem \n    reinit!(integrator, u0)\n    solve!(integrator)\n    if !SciMLBase.successful_retcode(integrator.sol)\n        return typemin(T)\n    end\n    ℓ = -0.5(n + 1) * log(2π * σ^2)\n    s = zero(T)\n    @turbo @muladd for i in eachindex(yᵒ, integrator.sol.u)\n        s = s + (yᵒ[i] - integrator.sol.u[i]) * (yᵒ[i] - integrator.sol.u[i])\n    end\n    ℓ = ℓ - 0.5s / σ^2\nend\n\n## Step 2: Define the problem\nusing Optimization\nθ₀ = [-1.0, 0.5, 19.73] # will be replaced anyway\nlb = [-10.0, 1e-6, 0.5]\nub = [10.0, 10.0, 25.0]\nsyms = [:λ, :σ, :y₀]\nprob = LikelihoodProblem(\n    _loglik_fnc, θ₀, ode_fnc, y₀, (0.0, T);\n    syms=syms,\n    data=(yᵒ, n),\n    ode_parameters=1.0, # temp value for λ\n    ode_kwargs=(verbose=false, saveat=t),\n    f_kwargs=(adtype=Optimization.AutoFiniteDiff(),),\n    prob_kwargs=(lb=lb, ub=ub),\n    ode_alg=Tsit5()\n)\n\n## Step 3: Grid search\nregular_grid = RegularGrid(lb, ub, 50) # resolution can also be given as a vector for each parameter\ngs = grid_search(prob, regular_grid)\n\n## Step 4: Compute the MLE, starting at the grid search solution \nusing OptimizationOptimJL\nprob = ProfileLikelihood.update_initial_estimate(prob, gs)\nsol = mle(prob, Optim.LBFGS())\n\n## Step 5: Profile \nusing OptimizationNLopt\nprof = profile(prob, sol; alg=NLopt.LN_NELDERMEAD, parallel=true)\n\n\n## Step 6: Visualise \nusing CairoMakie, LaTeXStrings\nfig = plot_profiles(prof; nrow=1, ncol=3,\n    latex_names=[L\"\\lambda\", L\"\\sigma\", L\"y_0\"],\n    true_vals=[λ, σ, y₀],\n    fig_kwargs=(fontsize=30, resolution=(2109.644f0, 444.242f0)),\n    axis_kwargs=(width=600, height=300))","category":"page"},{"location":"#ProfileLikelihood","page":"Home","title":"ProfileLikelihood","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"(Image: DOI)","category":"page"},{"location":"","page":"Home","title":"Home","text":"This package defines the routines required for computing maximum likelihood estimates and profile likelihoods. The optimisation routines are built around the Optimization.jl interface, allowing us to e.g. easily switch between algorithms, between finite differences and automatic differentiation, and it allows for constraints to be defined with ease. Below we list the definitions we are using for likelihoods and profile likelihoods. This code only works for scalar or bivariate parameters of interest (i.e. out of a vector boldsymbol theta, you can profile a single scalar parameter theta_i in boldsymboltheta or a pair (theta_itheta_j) in boldsymboltheta) for now. We use the following definitions:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Definition: Likelihood function (see Casella & Berger, 2002): Let f(boldsymbol x mid boldsymbol theta) denote the joint probability density function (PDF) of the sample boldsymbol X = (X_1ldotsX_n)^mathsf T, where boldsymbol theta in Theta is some set of parameters and Theta is the parameter space. We define the likelihood function mathcal L colon Theta to 0 infty) by mathcal L(boldsymbol theta mid boldsymbol x) = f(boldsymbol x mid boldsymbol theta) for some realisation boldsymbol x = (x_1ldotsx_n)^mathsf T of boldsymbol X. The log-likelihood function ellcolonThetatomathbb R is defined by ell(boldsymbol theta mid boldsymbol x) =  logmathcal L(boldsymboltheta mid boldsymbol x).The maximum likelihood estimate (MLE) hatboldsymboltheta is the parameter boldsymboltheta that maximises the likelihood function, hatboldsymboltheta = argmax_boldsymboltheta in Theta mathcalL(boldsymboltheta mid boldsymbol x) = argmax_boldsymboltheta in Theta ell(boldsymboltheta mid boldsymbol x).","category":"page"},{"location":"","page":"Home","title":"Home","text":"Definition: Profile likelihood function (see Pawitan, 2001): Suppose we have some parameters of interest, boldsymbol theta in Theta, and some nuisance parameters, boldsymbol phi in Phi, and some data boldsymbol x = (x_1ldotsx_n)^mathsf T, giving some joint likelihood mathcal L colon Theta cup Phi to 0 infty) defined by mathcal L(boldsymboltheta boldsymbolphi mid boldsymbol x). We define the profile likelihood mathcal L_p colon Theta to 0 infty) of boldsymboltheta by mathcal L_p(boldsymboltheta mid boldsymbol x) = sup_boldsymbol phi in Phi mid boldsymbol theta mathcal L(boldsymbol theta boldsymbol phi mid boldsymbol x). The profile log-likelihood ell_p colon Theta to mathbb R of boldsymboltheta is defined by ell_p(boldsymbol theta mid boldsymbol x) = log mathcal L_p(boldsymboltheta mid boldsymbol x). The normalised profile likelihood is defined by hatmathcal L_p(boldsymboltheta mid boldsymbol x) = mathcal L_p(boldsymbol theta mid boldsymbol x) - mathcal L_p(hatboldsymboltheta mid boldsymbol x), where hatboldsymboltheta is the MLE of boldsymboltheta, and similarly for the normalised profile log-likelihood.","category":"page"},{"location":"","page":"Home","title":"Home","text":"From Wilk's theorem, we know that 2hatell_p(boldsymboltheta mid boldsymbol x) geq -chi_p 1-alpha^2 is an approximate 100(1-alpha) confidence region for boldsymbol theta, and this enables us to obtain confidence intervals for parameters by considering only their profile likelihood, where chi_p1-alpha^2 is the 1-alpha quantile of the chi_p^2 distribution and p is the length of boldsymboltheta. For the case of a scalar parameter of interest, -chi_1 095^22 approx -192, and for a bivariate parameter of interest we have -chi_2095^22 approx -3.","category":"page"},{"location":"","page":"Home","title":"Home","text":"We compute the profile log-likelihood in this package by starting at the MLE, and stepping left/right until we reach a given threshold. The code is iterative to not waste time in so much of the parameter space. In the bivariate case, we start at the MLE and expand outwards in layers. This implementation is described in the documentation.","category":"page"},{"location":"","page":"Home","title":"Home","text":"More detail about the methods we use in this package is given in the sections in the sidebar, with extra detail in the tests.","category":"page"},{"location":"docstrings/#Docstrings","page":"Docstrings","title":"Docstrings","text":"","category":"section"},{"location":"docstrings/","page":"Docstrings","title":"Docstrings","text":"Here we give some of the main docstrings. ","category":"page"},{"location":"docstrings/#LikelihoodProblem","page":"Docstrings","title":"LikelihoodProblem","text":"","category":"section"},{"location":"docstrings/","page":"Docstrings","title":"Docstrings","text":"ProfileLikelihood.AbstractLikelihoodProblem \nLikelihoodProblem","category":"page"},{"location":"docstrings/#ProfileLikelihood.AbstractLikelihoodProblem","page":"Docstrings","title":"ProfileLikelihood.AbstractLikelihoodProblem","text":"abstract type AbstractLikelihoodProblem{N, L}\n\nAbstract type of a likelihood problem, where N is the number of parameters and  L is the type of the likelihood function.\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#ProfileLikelihood.LikelihoodProblem","page":"Docstrings","title":"ProfileLikelihood.LikelihoodProblem","text":"LikelihoodProblem{N,P,D,L,Θ,S} <: AbstractLikelihoodProblem\n\nStruct representing a likelihood problem. \n\nFields\n\nproblem::P\n\nThe associated OptimizationProblem.\n\ndata::D\n\nThe argument p used in the log-likelihood function. \n\nlog_likelihood_function::L\n\nThe log-likelihood function, taking the form ℓ(θ, p).\n\nθ₀::Θ\n\nInitial estimates for the MLE θ.\n\nsyms::S\n\nVariable names for the parameters.\n\nThe extra parameter N is the number of parameters.\n\nConstructors\n\nStandard\n\nLikelihoodProblem(loglik::Function, θ₀;\n    syms=eachindex(θ₀), data=SciMLBase.NullParameters(),\n    f_kwargs=nothing, prob_kwargs=nothing)\n\nConstructor for the LikelihoodProblem.\n\nArguments\n\nloglik::Function: The log-likelihood function, taking the form ℓ(θ, p).\nθ₀: The estimates estimates for the MLEs.\n\nKeyword Arguments\n\nsyms=eachindex(θ₀): Names for each parameter. \ndata=SciMLBase.NullParameters(): The parameter p in the log-likelihood function. \nf_kwargs=nothing: Keyword arguments, passed as a NamedTuple, for the OptimizationFunction.\nprob_kwargs=nothing: Keyword arguments, passed as a NamedTuple, for the OptimizationProblem.\n\nOutputs\n\nReturns the LikelihoodProblem problem object.\n\nWith arguments for a differential equation problem\n\nLikelihoodProblem(loglik::Function, θ₀,\n    ode_function, u₀, tspan;\n    syms=eachindex(θ₀), data=SciMLBase.NullParameters(),\n    ode_parameters=SciMLBase.NullParameters(), ode_alg,\n    ode_kwargs=nothing, f_kwargs=nothing, prob_kwargs=nothing)\n\nConstructor for the LikelihoodProblem for a differential equation problem.\n\nArguments\n\nloglik::Function: The log-likelihood function, taking the form ℓ(θ, p, integrator).\nθ₀: The estimates estimates for the MLEs.\node_function: The function f(du, u, p, t) or f(u, p, t) for the differential equation.\nu₀: The initial condition for the differential equation. \ntspan: The time-span to solve the differential equation over. \n\nKeyword Arguments\n\nsyms=eachindex(θ₀): Names for each parameter. \ndata=SciMLBase.NullParameters(): The parameter p in the log-likelihood function. \node_parameters=SciMLBase.NullParameters(): The parameter p in ode_function.\node_alg: The algorithm used for solving the differential equatios.\node_kwargs=nothing: Extra keyword arguments, passed as a NamedTuple, to pass into the integrator; see construct_integrator.\nf_kwargs=nothing: Keyword arguments, passed as a NamedTuple, for the OptimizationFunction.\nprob_kwargs=nothing: Keyword arguments, passed as a NamedTuple, for the OptimizationProblem.\n\nOutputs\n\nReturns the LikelihoodProblem problem object.\n\nWith an integrator\n\nLikelihoodProblem(loglik::Function, θ₀, integrator;\n    syms=eachindex(θ₀), data=SciMLBase.NullParameters(),\n    f_kwargs=nothing, prob_kwargs=nothing)\n\nConstructor for the LikelihoodProblem for a differential equation problem  with associated integrator.\n\nArguments\n\nloglik::Function: The log-likelihood function, taking the form ℓ(θ, p, integrator).\nθ₀: The estimates estimates for the MLEs.\nintegrator: The integrator for the differential equation problem. See also construct_integrator.\n\nKeyword Arguments\n\nsyms=eachindex(θ₀): Names for each parameter. \ndata=SciMLBase.NullParameters(): The parameter p in the log-likelihood function. \nf_kwargs=nothing: Keyword arguments, passed as a NamedTuple, for the OptimizationFunction.\nprob_kwargs=nothing: Keyword arguments, passed as a NamedTuple, for the OptimizationProblem.\n\nOutputs\n\nReturns the LikelihoodProblem problem object.\n\nUsing a GeneralLazyBufferCache\n\nLikelihoodProblem(loglik::Function, θ₀,\n    ode_function, u₀, tspan, lbc_fnc::F, lbc_index::G;\n    syms=eachindex(θ₀), data=SciMLBase.NullParameters(),\n    ode_parameters=SciMLBase.NullParameters(), ode_alg,\n    ode_kwargs=nothing, f_kwargs=nothing, prob_kwargs=nothing) where {F,G}\n\nConstructor for the LikelihoodProblem for a differential equation problem, using a  GeneralLazyBufferCache from PreallocationTools.jl to assist with automatic differentiation  support. See the Lotka-Volterra example for a demonstration.\n\nArguments\n\nloglik::Function: The log-likelihood function, taking the form ℓ(θ, p, integrator).\nθ₀: The estimates estimates for the MLEs.\node_function: The function f(du, u, p, t) or f(u, p, t) for the differential equation.\nu₀: The initial condition for the differential equation. \ntspan: The time-span to solve the differential equation over. \nlbc_fnc::F: This is a function of the form (f, u, p, tspan, ode_alg; kwargs...), with f the ode_function, u is u₀, p are the ode_parameters (below), ode_alg is the algorithm to use in the integrator (see below), and kwargs... are the ode_kwargs (below). The function should return a GeneralLazyBufferCache that itself defines a function that returns an integrator for your differentiation equation. \nlbc_index::G: This is a function of the form (θ, p), where θ and p are the parameters being optimised and p is the data for the likelihood problem. This argument is needed to define how the integrator from lbc_fnc is constructed inside the GeneralLazyBufferCache from (θ, p).\n\nKeyword Arguments\n\nsyms=eachindex(θ₀): Names for each parameter. \ndata=SciMLBase.NullParameters(): The parameter p in the log-likelihood function. \node_parameters=SciMLBase.NullParameters(): The parameter p in ode_function.\node_alg: The algorithm used for solving the differential equatios.\node_kwargs=nothing: Extra keyword arguments, passed as a NamedTuple, to pass into the integrator; see construct_integrator.\nf_kwargs=nothing: Keyword arguments, passed as a NamedTuple, for the OptimizationFunction.\nprob_kwargs=nothing: Keyword arguments, passed as a NamedTuple, for the OptimizationProblem.\n\nOutputs\n\nReturns the LikelihoodProblem problem object.\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#LikelihoodSolution","page":"Docstrings","title":"LikelihoodSolution","text":"","category":"section"},{"location":"docstrings/","page":"Docstrings","title":"Docstrings","text":"ProfileLikelihood.AbstractLikelihoodSolution\nProfileLikelihood.LikelihoodSolution \nmle","category":"page"},{"location":"docstrings/#ProfileLikelihood.AbstractLikelihoodSolution","page":"Docstrings","title":"ProfileLikelihood.AbstractLikelihoodSolution","text":"abstract type AbstractLikelihoodSolution{N, P}\n\nType representing the solution to a likelihood problem, where N is the  number of parameters and P is the type of the likelihood problem.\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#ProfileLikelihood.LikelihoodSolution","page":"Docstrings","title":"ProfileLikelihood.LikelihoodSolution","text":"struct LikelihoodSolution{Θ,P,M,R,A} <: AbstractLikelihoodSolution\n\nStruct for a solution to a LikelihoodProblem.\n\nFields\n\nmle::Θ: The MLEs.\nproblem::P: The LikelihoodProblem.\noptimiser::A: The algorithm used for solving the optimisation problem. \nmaximum::M: The maximum likelihood. \nretcode::R: The SciMLBase.ReturnCode.\n\nConstructors\n\nLikelihoodSolution(sol::SciMLBase.OptimizationSolution, prob::AbstractLikelihoodProblem; alg=sol.alg)\n\nConstructs the likelihood solution from a solution to an OptimizationProblem with a given LikelihoodProblem.\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#ProfileLikelihood.mle","page":"Docstrings","title":"ProfileLikelihood.mle","text":"mle(prob::LikelihoodProblem, alg, args...; kwargs...)\nmle(prob::LikelihoodProblem, alg::Tuple, args...; kwargs...)\n\nGiven the likelihood problem prob and an optimiser alg, finds the MLEs and returns a  LikelihoodSolution object. Extra arguments and keyword arguments for solve can be passed  through args... and kwargs....\n\nIf alg is a Tuple, then the problem is re-optimised after each algorithm with the next element in alg,  starting from alg[1], with initial estimate coming from the solution with the  previous algorithm (starting with get_initial_estimate(prob)).\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#ProfileLikelihoodSolution","page":"Docstrings","title":"ProfileLikelihoodSolution","text":"","category":"section"},{"location":"docstrings/","page":"Docstrings","title":"Docstrings","text":"ProfileLikelihood.ProfileLikelihoodSolution \nProfileLikelihood.ConfidenceInterval\nprofile \nreplace_profile!\nrefine_profile!\nProfileLikelihood.set_next_initial_estimate!(::Any, ::Any, ::Any, ::Any, ::Any)\nProfileLikelihood.get_confidence_intervals!\nProfileLikelihood.reach_min_steps!","category":"page"},{"location":"docstrings/#ProfileLikelihood.ProfileLikelihoodSolution","page":"Docstrings","title":"ProfileLikelihood.ProfileLikelihoodSolution","text":"ProfileLikelihoodSolution{I,V,LP,LS,Spl,CT,CF,OM}\n\nStruct for the normalised profile log-likelihood. See profile for a constructor.\n\nFields\n\nparameter_values::Dict{I, V}\n\nThis is a dictionary such that parameter_values[i] gives the parameter values used for the normalised profile log-likelihood of the ith variable.\n\nprofile_values::Dict{I, V}\n\nThis is a dictionary such that profile_values[i] gives the values of the normalised profile log-likelihood function at the corresponding values in θ[i].\n\nlikelihood_problem::LP\n\nThe original LikelihoodProblem.\n\nlikelihood_solution::LS\n\nThe solution to the full problem.\n\nsplines::Dict{I, Spl}\n\nThis is a dictionary such that splines[i] is a spline through the data (parameter_values[i], profile_values[i]). This spline can be evaluated at a point ψ for the ith variable by calling an instance of the struct with arguments (ψ, i). See also spline_profile.\n\nconfidence_intervals::Dict{I,ConfidenceInterval{CT,CF}}\n\nThis is a dictonary such that confidence_intervals[i] is a confidence interval for the ith parameter.\n\nother_mles::OM\n\nThis is a dictionary such that other_mles[i] gives the vector for the MLEs of the other parameters not being profiled, for each datum.\n\nSpline evaluation\n\nThis struct is callable. We define the method \n\n(prof::ProfileLikelihoodSolution)(θ, i)\n\nthat evaluates the spline through the ith profile at the point θ.\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#ProfileLikelihood.ConfidenceInterval","page":"Docstrings","title":"ProfileLikelihood.ConfidenceInterval","text":"struct ConfidenceInterval{T, F}\n\nStruct representing a confidence interval. \n\nFields\n\nlower::T\n\nThe lower bound of the confidence interval. \n\nupper::T\n\nThe upper bound of the confidence interval. \n\nlevel::F\n\nThe level of the confidence interval.\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#ProfileLikelihood.profile","page":"Docstrings","title":"ProfileLikelihood.profile","text":"profile(prob::LikelihoodProblem, sol::LikelihoodSolution, n=1:number_of_parameters(prob);\n    alg=get_optimiser(sol),\n    conf_level::F=0.95,\n    confidence_interval_method=:spline,\n    threshold=get_chisq_threshold(conf_level),\n    resolution=200,\n    param_ranges=construct_profile_ranges(sol, get_lower_bounds(prob), get_upper_bounds(prob), resolution),\n    min_steps=10,\n    normalise::Bool=true,\n    spline_alg=FritschCarlsonMonotonicInterpolation,\n    extrap=Line,\n    parallel=false,\n    next_initial_estimate_method = :prev,\n    kwargs...)\n\nComputes profile likelihoods for the parameters from a likelihood problem prob with MLEs sol.\n\nSee also replace_profile! which allows you to re-profile a parameter in case you are not satisfied with  the results. For plotting, see the plot_profiles function (requires that you have loaded CairoMakie.jl and  LaTeXStrings.jl to access the function).\n\nArguments\n\nprob::LikelihoodProblem: The LikelihoodProblem.\nsol::LikelihoodSolution: The LikelihoodSolution. See also mle.\nn=1:number_of_parameters(prob): The parameter indices to compute the profile likelihoods for.\n\nKeyword Arguments\n\nalg=get_optimiser(sol): The optimiser to use for solving each optimisation problem. \nconf_level::F=0.95: The level to use for the ConfidenceIntervals.\nconfidence_interval_method=:spline: The method to use for computing the confidence intervals. See also get_confidence_intervals!. The default :spline uses rootfinding on the spline through the data, defining a continuous function, while the alternative :extrema simply takes the extrema of the values that exceed the threshold.\nthreshold=get_chisq_threshold(conf_level): The threshold to use for defining the confidence intervals. \nresolution=200: The number of points to use for evaluating the profile likelihood in each direction starting from the MLE (giving a total of 2resolution points). - resolution=200: The number of points to use for defining grids below, giving the number of points to the left and right of each interest parameter. This can also be a vector, e.g. resolution = [20, 50, 60] will use 20 points for the first parameter, 50 for the second, and 60 for the third. \nparam_ranges=construct_profile_ranges(sol, get_lower_bounds(prob), get_upper_bounds(prob), resolution): The ranges to use for each parameter.\nmin_steps=10: The minimum number of steps to allow for the profile in each direction. If fewer than this number of steps are used before reaching the threshold, then the algorithm restarts and computes the profile likelihood a number min_steps of points in that direction. See also min_steps_fallback.\nmin_steps_fallback=:replace: Method to use for updating the profile when it does not reach the minimum number of steps, min_steps. See also reach_min_steps!. If :replace, then the profile is completely replaced and we use min_steps equally spaced points to replace it. If :refine, we just fill in some of the space in the grid so that a min_steps number of points are reached. Note that this latter option will mean that the spacing is no longer constant between parameter values. You can use :refine_parallel to apply :refine in parallel.\nnormalise::Bool=true: Whether to optimise the normalised profile log-likelihood or not. \nspline_alg=FritschCarlsonMonotonicInterpolation: The interpolation algorithm to use for computing a spline from the profile data. See Interpolations.jl. \nextrap=Line: The extrapolation algorithm to use for computing a spline from the profile data. See Interpolations.jl.\nparallel=false: Whether to use multithreading. If true, will use multithreading so that multiple parameters are profiled at once, and the steps to the left and right are done at the same time. \nnext_initial_estimate_method = :prev: Method for selecting the next initial estimate when stepping forward when profiling. :prev simply uses the previous solution, but you can also use :interp to use linear interpolation. See also set_next_initial_estimate!.\nkwargs...: Extra keyword arguments to pass into solve for solving the OptimizationProblem. See also the docs from Optimization.jl.\n\nOutput\n\nReturns a ProfileLikelihoodSolution.\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#ProfileLikelihood.replace_profile!","page":"Docstrings","title":"ProfileLikelihood.replace_profile!","text":"replace_profile!(prof::ProfileLikelihoodSolution, n);\n    alg=get_optimiser(prof.likelihood_solution),\n    conf_level::F=0.95,\n    confidence_interval_method=:spline,\n    threshold=get_chisq_threshold(conf_level),\n    resolution=200,\n    param_ranges=construct_profile_ranges(prof.likelihood_solution, get_lower_bounds(prof.likelihood_problem), get_upper_bounds(prof.likelihood_problem), resolution),\n    min_steps=10,\n    min_steps_fallback=:replace,\n    normalise::Bool=true,\n    spline_alg=FritschCarlsonMonotonicInterpolation,\n    extrap=Line,\n    parallel=false,\n    next_initial_estimate_method=:prev,\n    kwargs...) where {F}\n\nGiven an existing prof::ProfileLikelihoodSolution, replaces the profile results for the parameters in n by re-profiling. The keyword  arguments are the same as for profile.\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#ProfileLikelihood.refine_profile!","page":"Docstrings","title":"ProfileLikelihood.refine_profile!","text":"refine_profile!(prof::ProfileLikelihoodSolution, n;\n    alg=get_optimiser(prof.likelihood_solution),\n    conf_level::F=0.95,\n    confidence_interval_method=:spline,\n    threshold=get_chisq_threshold(conf_level),\n    target_number=10,\n    normalise::Bool=true,\n    spline_alg=FritschCarlsonMonotonicInterpolation,\n    extrap=Line,\n    parallel=false,\n    kwargs...) where {F}\n\nGiven an existing prof::ProfileLikelihoodSolution, refines the profile results for the parameters in n by adding more points. The keyword  arguments are the same as for profile. target_number is the total number of points that should be included in the end (not how many more  are added).\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#ProfileLikelihood.set_next_initial_estimate!-NTuple{5, Any}","page":"Docstrings","title":"ProfileLikelihood.set_next_initial_estimate!","text":"set_next_initial_estimate!(sub_cache, param_vals, other_mles, prob, θₙ; next_initial_estimate_method=Val(:prev))\n\nMethod for selecting the next initial estimate for the optimisers. sub_cache is the cache vector for placing  the initial estimate into, param_vals is the current list of parameter values for the interest parameter,  and other_mles is the corresponding list of previous optimisers. prob is the OptimizationProblem. The value  θₙ is the next value of the interest parameter.\n\nThe available methods are: \n\nnext_initial_estimate_method = Val(:prev): If this is selected, simply use other_mles[end], i.e. the previous optimiser. \nnext_initial_estimate_method = Val(:interp): If this is selected, the next optimiser is determined via linear interpolation using the data (param_vals[end-1], other_mles[end-1]), (param_vals[end], other_mles[end]). If the new approximation is outside of the parameter bounds, falls back to next_initial_estimate_method = :prev.\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#ProfileLikelihood.get_confidence_intervals!","page":"Docstrings","title":"ProfileLikelihood.get_confidence_intervals!","text":"get_confidence_intervals!(confidence_intervals, method, n, param_vals, profile_vals, threshold, spline_alg, extrap, mles, conf_level)\n\nMethod for computing the confidence intervals.\n\nArguments\n\nconfidence_intervals: The dictionary storing the confidence intervals. \nmethod: The method to use for computing the confidence interval. The available methods are:\nmethod = Val(:spline): Fits a spline to (param_vals, profile_vals) and finds where the continuous spline equals threshold.\nmethod = Val(:extrema): Takes the first and last values in param_vals whose corresponding value in profile_vals exceeds threshold.\nn: The parameter being profiled. \nparam_vals: The parameter values. \nprofile_vals: The profile values. \nthreshold: The threshold for the confidence interval. \nspline_alg: The algorithm to use for fitting a spline. \nextrap: The extrapolation algorithm used for the spline.\nmles: The MLEs. \nconf_level: The confidence level for the confidence interval.\n\nOutputs\n\nThere are no outputs - confidence_intervals[n] gets the ConfidenceInterval put into it.\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#ProfileLikelihood.reach_min_steps!","page":"Docstrings","title":"ProfileLikelihood.reach_min_steps!","text":"reach_min_steps!(param_vals, profile_vals, other_mles, param_range,\n    restricted_prob, n, cache, alg, sub_cache, ℓmax, normalise,\n    threshold, min_steps, mles; min_steps_fallback=Val(:replace), next_initial_estimate_method=Val(:interp), kwargs...)\n\nUpdates the results from the side of a profile likelihood (e.g. left or right side, see find_endpoint!) to meet the minimum number of  steps min_steps.\n\nArguments\n\nparam_vals: The parameter values. \nprofile_vals: The profile values. \nother_mles: The other MLEs, i.e. the optimised parameters for the corresponding fixed parameter values in param_vals.\nparam_range: The vector of parameter values.\nrestricted_prob: The optimisation problem, restricted to the nth parameter. \nn: The parameter being profiled.\ncache: A cache for the complete parameter vector. \nalg: The algorithm used for optimising. \nsub_cache: A cache for the parameter vector excluding the nth parameter.\nℓmax: The maximum likelihood. \nnormalise: Whether the optimisation problem is normalised.\nthreshold: The threshold for the confidence interval. \nmin_steps: The minimum number of steps to reach. \nmles: The MLEs.\n\nKeyword Arguments\n\nmin_steps_fallback=Val(:interp): The method used for reaching the minimum number of steps. The available methods are:\nmin_steps_fallback = Val(:replace): This method completely replaces the profile, defining a grid from the MLE to the computed endpoint with min_steps points. No information is re-used.\nmin_steps_fallback = Val(:refine): This method just adds more points to the profile, filling in enough points so that the total number of points is min_steps. The initial estimates in this case come from a spline from other_mles.\nmin_steps_fallback = Val(:parallel_refine): This applies the method above, except in parallel. \nnext_initial_estimate_method=Val(:replace): The method used for obtaining initial estimates. See also set_next_initial_estimate!.\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#BivariateProfileLikelihoodSolution","page":"Docstrings","title":"BivariateProfileLikelihoodSolution","text":"","category":"section"},{"location":"docstrings/","page":"Docstrings","title":"Docstrings","text":"ProfileLikelihood.BivariateProfileLikelihoodSolution \nProfileLikelihood.ConfidenceRegion \nbivariate_profile\nProfileLikelihood.set_next_initial_estimate!(::Any, ::Any, ::CartesianIndex, ::Any, ::Any, ::Any, ::Any, ::Val{M}) where M","category":"page"},{"location":"docstrings/#ProfileLikelihood.BivariateProfileLikelihoodSolution","page":"Docstrings","title":"ProfileLikelihood.BivariateProfileLikelihoodSolution","text":"BivariateProfileLikelihoodSolution{I,V,LP,LS,Spl,CT,CF,OM}\n\nStruct for the normalised bivariate profile log-likelihood. See bivariate_profile for a constructor. \n\nArguments\n\nparameter_values::Dict{I, G}\n\nMaps the tuple (i, j) to the grid values used for this parameter pair. The result is a Tuple, with the first element  the grid for the ith parameter, and the second element the grid for the jth parameter. The grids are given as  OffsetVectors, with the 0th index the MLE, negative indices to the left of the MLE, and positive indices to the  right of the MLE.\n\nprofile_values::Dict{I, V}\n\nMaps the tuple (i, j) to the matrix used for this parameter pair. The result is a OffsetMatrix, with the (k, ℓ) entry  the profile at (parameter_values[(i, j)][1][k], parameter_values[(i, j)][2][k]), and particularly the (0, 0) entry is the  profile at the MLEs.\n\nlikelihood_problem::LP\n\nThe original likelihood problem. \n\nlikelihood_solution::LS\n\nThe original likelihood solution. \n\ninterpolants::Dict{I,Spl}\n\nMaps the tuple (i, j) to the interpolant for that parameter pair's profile. This interpolant also uses linear extrapolation. \n\nconfidence_regions::Dict{I,ConfidenceRegion{CT,CF}}\n\nMaps the tuple (i, j) to the confidence region for that parameter pair's confidence region. See also ConfidenceRegion.\n\nother_mles::OM\n\nMaps the tuple (i, j) to an OffsetMatrix storing the solutions for the nuisance parameters at the corresponding grid values. \n\nInterpolant evaluation\n\nThis struct is callable. We define the method \n\n(prof::BivariateProfileLikelihoodSolution)(θ, ψ, i, j)\n\nthat evaluates the interpolant through the (i, j)th profile at the point (θ, ψ).\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#ProfileLikelihood.ConfidenceRegion","page":"Docstrings","title":"ProfileLikelihood.ConfidenceRegion","text":"struct ConfidenceRegion{T, F}\n\nStruct representing a confidence region. \n\nFields\n\nx::T\n\nThe x-coordinates for the region's boundary.\n\ny::T\n\nThe y-coordinates for the region's boundary.\n\nlevel::F\n\nThe level of the confidence region.\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#ProfileLikelihood.bivariate_profile","page":"Docstrings","title":"ProfileLikelihood.bivariate_profile","text":"bivariate_profile(prob::LikelihoodProblem, sol::LikelihoodSolution, n::NTuple{M,NTuple{2,Int64}};\n    alg=get_optimiser(sol),\n    conf_level::F=0.95,\n    confidence_region_method=Val(:contour),\n    threshold=get_chisq_threshold(conf_level, 2),\n    resolution=200,\n    grids=construct_profile_grids(n, sol, get_lower_bounds(prob), get_upper_bounds(prob), resolution),\n    min_layers=10,\n    outer_layers=0,\n    normalise=Val(true),\n    parallel=Val(false),\n    next_initial_estimate_method=Val(:nearest),\n    kwargs...) where {M,F}\n\nComputes bivariates profile likelihoods for the parameters from a likelihood problem prob with MLEs sol. You can also call  this function using Symbols, e.g. if get_syms(prob) = [:λ, :K, :u₀], then calling bivariate_profile(prob, sol, ((:λ, :K), (:K, u₀))) is the same as calling bivariate_profile(prob, sol, ((1, 2), (2, 3))) (the integer coordinate representation is still used in the solution, though).\n\nFor plotting, see the plot_profiles function (requires that you have loaded CairoMakie.jl and  LaTeXStrings.jl to access the function).\n\nArguments\n\nprob::LikelihoodProblem: The LikelihoodProblem.\nsol::LikelihoodSolution: The LikelihoodSolution. See also mle.\nn::NTuple{M,NTuple{2,Int64}}: The parameter indices to compute the profile likelihoods for. These should be tuples of indices, e.g. n = ((1, 2),) will compute the bivariate profile between the parameters 1 and 2.\n\nKeyword Arguments\n\nalg=get_optimiser(sol): The optimiser to use for solving each optimisation problem. \nconf_level::F=0.95: The level to use for the ConfidenceRegions.\nconfidence_region_method=:contour: The method to use for computing the confidence regions. See also get_confidence_regions!. The default, :contour, using Contour.jl to compute the boundary of the confidence region. An alternative option is :delaunay, which uses DelaunayTriangulation.jl and triangulation contouring to find the boundary. This latter option is only available if you have already done using DelaunayTriangulation.\nthreshold=get_chisq_threshold(conf_level, 2): The threshold to use for defining the confidence regions. \nresolution=200: The number of points to use for defining grids below, giving the number of points to the left and right of each interest parameter. This can also be a vector, e.g. resolution = [20, 50, 60] will use 20 points for the first parameter, 50 for the second, and 60 for the third. When defining the grid between pairs of values, the maximum of the two resolutions is used (thus defining a square grid).\ngrids=construct_profile_grids(n, sol, get_lower_bounds(prob), get_upper_bounds(prob), resolution): The grids to use for each parameter pair.\nmin_layers=10: The minimum number of layers to allow for the profile away from the MLE. If fewer than this number of layers are used before reaching the threshold, then the algorithm restarts and computes the profile likelihood a number min_steps of points in that direction. \nouter_layers=0: The number of layers to go out away from the bounding box of the confidence region.\nnormalise=true: Whether to optimise the normalised profile log-likelihood or not. \nparallel=false: Whether to use multithreading. If true, will use multithreading so that multiple parameters are profiled at once, and the work done evaluating the solution at each node in a layer is distributed across each thread.\nnext_initial_estimate_method = :nearest: Method for selecting the next initial estimate when stepping onto the next layer when profiling. :nearest simply uses the solution at the nearest node from the previous layer, but you can also use :mle to reuse the MLE or :interp to use linear interpolation. See also set_next_initial_estimate!.\nkwargs...: Extra keyword arguments to pass into solve for solving the OptimizationProblem. See also the docs from Optimization.jl.\n\nOutput\n\nReturns a BivariateProfileLikelihoodSolution.\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#ProfileLikelihood.set_next_initial_estimate!-Union{Tuple{M}, Tuple{Any, Any, CartesianIndex, Any, Any, Any, Any, Val{M}}} where M","page":"Docstrings","title":"ProfileLikelihood.set_next_initial_estimate!","text":"set_next_initial_estimate!(sub_cache, other_mles, I::CartesianIndex, fixed_vals, grid, layer, prob, next_initial_estimate_method::Val{M}) where {M}\n\nMethod for selecting the next initial estimate for the optimisers. \n\nArguments\n\nsub_cache: Cache for the next initial estimate. \nother_mles: Solutions to the optimisation problems found so far. \nI::CartesianIndex: The coordinate of the node currently being considered. \nfixed_vals: The current values for the parameters of interest. \ngrid: The grid for the parameters of interest. \nlayer: The current layer. \nprob: The restricted optimisation problem. \nnext_initial_estimate_method::Val{M}: The method to use.\n\nThe methods currently available for next_initial_estimate_method are: \n\nnext_initial_estimate_method = Val(:mle): Simply sets sub_cache to be the MLE. \nnext_initial_estimate_method = Val(:nearest): Sets sub_cache to be other_mles[J], where J is the nearest node to I in the previous layer. \nnext_initial_estimate_method = Val(:interp): Uses linear interpolation from all the previous layers to extrapolate and compute a new sub_cache.\n\nOutputs\n\nThere are no outputs.\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#Prediction-intervals","page":"Docstrings","title":"Prediction intervals","text":"","category":"section"},{"location":"docstrings/","page":"Docstrings","title":"Docstrings","text":"get_prediction_intervals ","category":"page"},{"location":"docstrings/#ProfileLikelihood.get_prediction_intervals","page":"Docstrings","title":"ProfileLikelihood.get_prediction_intervals","text":"get_prediction_intervals(q, prof::(Bivariate)ProfileLikelihoodSolution, data;\n    q_prototype=isinplace(q, 3) ? nothing : build_q_prototype(q, prof, data), resolution=250)\n\nObtain prediction intervals for the output of the prediction function q, assuming q returns (or operates in-place on) a vector.\n\nArguments\n\nq: The prediction function, taking either the form (θ, data) or (cache, θ, data). The former version is an out-of-place version, returning the full vector, while the latter version is an in-place version, with the output being placed into cache. The argument θ is the same as the parameters used in the likelihood problem (from prof), and the data argument is the same data as in this function. \nprof::(Bivariate)ProfileLikelihoodSolution: The profile likelihood results. \ndata: The argument data in q.\n\nKeyword Arguments\n\nq_prototype=isinplace(q, 3) ? nothing : build_q_prototype(q, prof, data): A prototype for the result of q. If you are using the q(θ, data) version of q, this can be inferred from build_q_prototype, but if you are using the in-place version then a build_q_prototype is needed. For example, if q returns a vector with eltype Float64 and has length 27, q_prototype could be zeros(27).\nresolution::Integer=250: The amount of curves to evaluate for each parameter. This will be the same for each parameter. If prof isa BivariateProfileLikelihoodSolution, then resolution^2 points are defined inside a bounding box for the confidence region, and then we throw away all points outside of the actual confidence region.\nparallel=false: Whether to use multithreading. Multithreading is used when building q_vals below.\n\nOutputs\n\nFour values are returned. In order: \n\nindividual_intervals: Prediction intervals for the output of q, relative to each parameter. \nunion_intervals: The union of the individual prediction intervals from individual_intervals.\nq_vals: Values of q at each parameter considered. The output is a Dict, where the parameter index is mapped to a matrix where each column is an output from q, with the jth column corresponding to the parameter value at param_ranges[j].\nparam_ranges: Parameter values used for each prediction interval. \n\n\n\n\n\n","category":"function"},{"location":"docstrings/#Plotting","page":"Docstrings","title":"Plotting","text":"","category":"section"},{"location":"docstrings/","page":"Docstrings","title":"Docstrings","text":"plot_profiles ","category":"page"},{"location":"docstrings/#GridSearch","page":"Docstrings","title":"GridSearch","text":"","category":"section"},{"location":"docstrings/#Grid-definitions","page":"Docstrings","title":"Grid definitions","text":"","category":"section"},{"location":"docstrings/","page":"Docstrings","title":"Docstrings","text":"ProfileLikelihood.AbstractGrid \nProfileLikelihood.RegularGrid \nProfileLikelihood.FusedRegularGrid\nProfileLikelihood.IrregularGrid ","category":"page"},{"location":"docstrings/#ProfileLikelihood.AbstractGrid","page":"Docstrings","title":"ProfileLikelihood.AbstractGrid","text":"abstract type AbstractGrid{N,B,T}\n\nType representing a grid, where N is the number of parameters, B is the type for the  bounds, and T is the number type.\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#ProfileLikelihood.RegularGrid","page":"Docstrings","title":"ProfileLikelihood.RegularGrid","text":"struct RegularGrid{N,B,R,S,T} <: AbstractGrid{N,B,T}\n\nStruct for a grid in which each parameter is regularly spaced. \n\nFields\n\nlower_bounds::B: Lower bounds for each parameter. \nupper_bounds::B: Upper bounds for each parameter. \nresolution::R: Number of grid points for each parameter. If R <: Number, then the same number of grid points is used for each parameter. \nstep_sizes::S: Grid spacing for each parameter. \n\nConstructor\n\nYou can construct a RegularGrid using RegularGrid(lower_bounds, upper_bounds, resolution).\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#ProfileLikelihood.FusedRegularGrid","page":"Docstrings","title":"ProfileLikelihood.FusedRegularGrid","text":"struct FusedRegularGrid{N,B,R,S,T,C,OR} <: AbstractGrid{N,B,T}\n\nStruct representing the fusing of two grids.\n\nFields\n\npositive_grid::RegularGrid{N,B,R,S,T}\n\nThis is the first part of the grid, indexed into by positive integers. \n\nnegative_grid::RegularGrid{N,B,R,S,T}\n\nThis is the second part of the grid, indexed into by negative integers.\n\ncentre::C\n\nThe two grids meet at a common centre, and this is that centre. \n\nresolutions::R\n\nThis is the vector of resolutions provided (e.g. if store_original_resolutions=true in the constructor below), or the transformed  version from get_resolution_tuples.\n\nConstructor\n\nYou can construct a FusedRegularGrid using the method\n\nFusedRegularGrid(lower_bounds::B, upper_bounds::B, centre::C, resolutions::R; store_original_resolutions=false) where {B,R,C}\n\nFor example, the following code creates fused as the fusion of grid_1 and grid_2:\n\nlb = [2.0, 3.0, 1.0, 5.0]\nub = [15.0, 13.0, 27.0, 10.0]\ncentre = [7.3, 8.3, 2.3, 7.5]\ngrid_1 = RegularGrid(centre .+ (ub .- centre) / 173, ub, 173)\ngrid_2 = RegularGrid(centre .- (centre .- lb) / 173, lb, 173)\nfused = ProfileLikelihood.FusedRegularGrid(lb, ub, centre, 173)\n\nThere are 173 points to the left and right of centre in this case. To use a varying  number of points, use e.g.\n\nlb = [2.0, 3.0, 1.0, 5.0, 4.0]\nub = [15.0, 13.0, 27.0, 10.0, 13.0]\ncentre = [7.3, 8.3, 2.3, 7.5, 10.0]\nres = [(2, 11), (23, 25), (19, 21), (50, 51), (17, 99)]\ngrid_1 = RegularGrid(centre .+ (ub .- centre) ./ [2, 23, 19, 50, 17], ub, [2, 23, 19, 50, 17])\ngrid_2 = RegularGrid(centre .- (centre .- lb) ./ [11, 25, 21, 51, 99], lb, [11, 25, 21, 51, 99])\nfused = ProfileLikelihood.FusedRegularGrid(lb, ub, centre, res) # fused grid_1 and grid_2\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#ProfileLikelihood.IrregularGrid","page":"Docstrings","title":"ProfileLikelihood.IrregularGrid","text":"struct IrregularGrid{N,B,R,S,T} <: AbstractGrid{N,B,T}\n\nStruct for an irregular grid of parameters.\n\nFields\n\nlower_bounds::B: Lower bounds for each parameter. \nupper_bounds::B: Upper bounds for each parameter. \ngrid::G: The set of parameter values, e.g. a matrix where each column is the parameter vector.\n\nConstructor\n\nYou can construct a IrregularGrid using IrregularGrid(lower_bounds, upper_bounds, grid).\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#Performing-a-grid-search","page":"Docstrings","title":"Performing a grid search","text":"","category":"section"},{"location":"docstrings/","page":"Docstrings","title":"Docstrings","text":"ProfileLikelihood.GridSearch\ngrid_search ","category":"page"},{"location":"docstrings/#ProfileLikelihood.GridSearch","page":"Docstrings","title":"ProfileLikelihood.GridSearch","text":"struct GridSearch{F,G}\n\nStruct for a GridSearch.\n\nFields\n\nf::F: The function to optimise, of the form f(x, p).\np::P: The arguments p in the function f.\ngrid::G: The grid, where G<:AbstractGrid. See also grid_search.\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#ProfileLikelihood.grid_search","page":"Docstrings","title":"ProfileLikelihood.grid_search","text":"grid_search(prob; save_vals=Val(false), minimise:=Val(false), parallel=Val(false))\n\nPerforms a grid search for the given grid search problem prob.\n\nArguments\n\nprob::GridSearch{F, G}: The grid search problem.\n\nKeyword Arguments\n\nsave_vals:=Val(false): Whether to return a array with the function values. \nminimise:=Val(false): Whether to minimise or to maximise the function.\nparallel:=Val(false): Whether to run the grid search with multithreading.\n\nOutputs\n\nf_opt: The optimal objective value. \nx_argopt: The parameter that gave f_opt.\nf_res: If save_vals==Val(true), then this is the array of function values.\n\n\n\n\n\ngrid_search(f, grid::AbstractGrid; save_vals=Val(false), minimise=Val(false), parallel=Val(false))\n\nFor a given grid and function f, performs a grid search. \n\nArguments\n\nf: The function to optimise. \ngrid::AbstractGrid: The grid to use for optimising. \n\nKeyword Arguments\n\nsave_vals=Val(false): Whether to return a array with the function values. \nminimise=Val(false): Whether to minimise or to maximise the function.\nparallel=Val(false): Whether to run the grid search with multithreading.\n\n\n\n\n\ngrid_search(prob::LikelihoodProblem, grid::AbstractGrid, parallel=Val(false); save_vals=Val(false))\n\nGiven a grid and a likelihood problem prob, maximises it over the grid using a grid search. If  save_vals==Val(true), then the likelihood function values at each gridpoint are returned. Set  parallel=Val(true) if you want multithreading.\n\n\n\n\n\n","category":"function"}]
}
